[
  {
    "question": "What specific failures does the 'AfroBench' paper identify in current LLMs?",
    "answer": "The 'AfroBench' paper identifies that there are large gaps in performance between high-resource languages, such as English, and African languages across most tasks. This performance disparity is attributed to the availability of monolingual data resources, indicating that current LLMs struggle with African languages due to insufficient data and resources.",
    "citations_readable": [
      "AfroBench_ How Good are Large Language Models on African Languages?.pdf"
    ],
    "citations_raw": [
      "source_33"
    ],
    "retrieved_chunks": [
      {
        "source_id": "source_09",
        "citation": "The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks.pdf",
        "text_snippet": "engagement in multilingual benchmark creation.\n5. PRESENT: What is the Current Status of Multilingual Evaluation?\nUnderstanding the current landscape of multilingual evaluation is essential for developing effective\nbenchmarks and assessment methodologies. In this section, we examine the present state of multilin-\ngual evaluation from two critical perspectives: the actual interests and needs of multilingual users\n(Section 5.1), and the alignment between existing multilingual benchmarks and human judgments\n(Section 5.2). By investigating these dimensions, we aim to identify gaps between user expectations\nand evaluation practices, and to establish a foundation for more representative and meaningful\nmultilingual benchmarks.\n5.1. What Are the Multilingual Users Interested in?\nSetup To understand the interests of multilingual users, we analyze the distribution of user instruc-\ntions in Chatbot Arena [Chiang et al., 2024] and WildChat [Zhao et al., 2024b]. We analyze six"
      },
      {
        "source_id": "source_33",
        "citation": "AfroBench_ How Good are Large Language Models on African Languages?.pdf",
        "text_snippet": "(2) Evaluation of LLMs needs to be continuous\nsince many new LLMs have been released with im-\nproved multilingual abilities, but a comprehensive\nevaluation is not available for African languages.\nIn this paper, we address the challenges of pre-\nvious large-scale LLM evaluation by introduc-\ning a new carefully curated benchmark known as\nAFROBENCH which comprises 15 tasks, 22 eval-\nuation data, and 64 indigeneous African lan-\nguages. AFROBENCH consists of nine natural\nlanguage understanding tasks, six text generation\ntasks, six knowledge and question answering tasks,\nand one mathematical reasoning task. Finally, we\ncreated a new evaluation datasets, AFRI ADR for\ndiacritic restoration of tonal marks and accents on\nAfrican language texts. Leveraging AFROBENCH ,\nwe conduct an extensive analysis of the perfor-\nmance of LLMs for African languages from differ-\nent language families and geographical locations.\nFor our evaluation, we compute the aver-"
      },
      {
        "source_id": "source_09",
        "citation": "The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks.pdf",
        "text_snippet": "The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks\n4. PAST: What Benchmarks Do We Have?\nIn this section, we present a comprehensive analysis of the current landscape of multilingual bench-\nmarks based on our paper collection. We examine the distribution of languages, the evolution of task\ntypes, translation methods, and more across benchmarks collected from 2021 to 2024. Understanding\nthe existing benchmark ecosystem is crucial for identifying gaps in language coverage, tracking shifts\nin evaluation focus, and recognizing opportunities for more inclusive and representative benchmark\ndevelopment.\nLanguages Figure 2 illustrates the distribution of the top 50 languages across our collected bench-\nmarks. Notably, even though we deliberately exclude English benchmarks during the collection\nprocess, English still tops the chart, peaking near 1000 occurrences. Similarly, other high-resource\nlanguages (HRLs) such as Chinese (zh), Spanish (es), French (fr), and German (de) occupy the"
      },
      {
        "source_id": "source_33",
        "citation": "AfroBench_ How Good are Large Language Models on African Languages?.pdf",
        "text_snippet": "and tasks. To address these challenges, we\nintroduce AFROBENCH \u2014a multi-task bench-\nmark for evaluating the performance of LLMs\nacross 64 African languages, 15 tasks and 22\ndatasets. AFROBENCH consists of nine natural\nlanguage understanding datasets, six text gen-\neration datasets, six knowledge and question\nanswering tasks, and one mathematical reason-\ning task. We present results comparing the\nperformance of prompting LLMs to fine-tuned\nbaselines based on BERT and T5-style models.\nOur results suggest large gaps in performance\nbetween high-resource languages, such as En-\nglish, and African languages across most tasks;\nbut performance also varies based on the avail-\nability of monolingual data resources. Our find-\nings confirm that performance on African lan-\nguages continues to remain a hurdle for current\nLLMs, underscoring the need for additional\nefforts to close this gap.1\n1 Introduction\nLarge language models (LLMs) have risen to the"
      },
      {
        "source_id": "source_32",
        "citation": "Neural Machine Translation for Low-Resource Languages.pdf",
        "text_snippet": "official languages. Therefore, in recent years, there has been a noticeable increase in NMT research\n(both by academia and industry) that specifically focused on LRL pairs.\nDespite this emphasis, we are not aware of any literature review that systematically examines\nthe NMT techniques tailored for LRL pairs. Although there exists some work that discusses the\nchallenges of using NMT in the context of LRL pairs [187] and the application of specific techniques\nfor LRL pairs [34], none of them gives a comprehensive view of the available NMT techniques for\nLRL pairs. This makes it difficult for new researchers in the field to identify the best NMT technique\nfor a given dataset specification. In addition, none of these surveys presents a holistic view of the\nNMT landscape for LRL pairs to derive insights on research efforts and current practices.\nThis survey aims to address the above shortcomings in the NMT research landscape for LRLs."
      },
      {
        "source_id": "source_33",
        "citation": "AfroBench_ How Good are Large Language Models on African Languages?.pdf",
        "text_snippet": "still beneficial since the capabilities of LLMs lags\nbehind. We provide task and per-language results\nin Appendix A and I.\n5.2 AfroBench-Lite Evaluation\nIn the AFROBENCH -LITE evaluation, we restrict\nthe evaluation to seven LLMs, and seven tasks, and\ncompare performance gap to English.\nLarge gap in performance when compared to\nEnglish One striking observation is that open\nmodels such as LlaMa 3.1 70B and Gemma 2 27B\n78-shot samples is the standard setting for MGSM datasets\n6"
      },
      {
        "source_id": "source_19",
        "citation": "Bridging Gaps in Natural Language Processing for Yoru\u0300ba\u0301_ A  Systematic Review of a Decade of Progress and Prospects.pdf",
        "text_snippet": "Subsequently, potential papers are searched through identified databases and screened based\non the selection criteria. A quality assessment is performed on the included paper to ensure\nstandard review and that the relevant data are extracted thereafter. Furthermore, the available\ndata are analysed to obtain meaningful results reported comprehensively afterward. The next\nsections explained the processes in more detail.\n3.2 Research Questions\nResearch questions were developed to define a precise template for the broad study objectives.\nFour questions have been carefully defined to cater to all possible domains intertwined with the\nresearch objectives by forming a structured set of interrogative statements whose answers are\nmeant to provide insight into the research goal. The questions are motivated by the need to\nknow the current status of Yor` ub\u00b4 a language involvement in NLP by investigating the specific\n3https://www.covidence.org/\n4https://www.zotero.org/\n6"
      },
      {
        "source_id": "source_33",
        "citation": "AfroBench_ How Good are Large Language Models on African Languages?.pdf",
        "text_snippet": "languages, evaluating capabilities across 15 dis-\ntinct tasks. This expanded scope allows for a more\nnuanced and thorough understanding of LLM ca-\npabilities in African language contexts.\n3 AfroBench\nAFROBENCH is a comprehensive LLM evaluation\nbenchmark designed to assess both proprietary and\nopen LLMs across diverse Natural Language Pro-\ncessing (NLP) tasks in African languages. As\nshown in Figure 2, the benchmark encompasses\n15 distinct tasks, spanning Natural Language Gen-\neration (NLG) and Natural Language Understand-\ning (NLU), incorporating 22 curated datasets in\n64 African languages. These evaluation tasks ex-\ntend beyond traditional NLP benchmarks, such as\ntext classification and named entity recognition,\nto include more challenging benchmarks such as\nmathematical reasoning and knowledge QA.\nEach task within AFROBENCH has been care-\nfully selected to assess different aspects of lan-\nguage model capabilities, from basic linguistic\ncompetency to more complex reasoning abilities."
      },
      {
        "source_id": "source_31",
        "citation": "A Survey on Multilingual Large Language Models_ Corpora, Alignment, and  Bias.pdf",
        "text_snippet": "bias is naturally present in cross-cultural datasets\nand the design of MLLMs\u2019 modeling processes\n[12]. Previous studies have explored bias in vari-\nous NLP tasks and demographic groups \uff0cbut are\nlargely specific to English-based models [13, 14],\nwhich cannot be generalized to other languages.\nWhat are the types of bias in existing MLLMs?\nWhat are the main de-biasing techniques available\nfor MLLMs? Does the removal of these biases af-\nfect LLMs\u2019 performance? What are the existing\nbias evaluation datasets for MLLMs? These are\nvery worthwhile research questions. This survey\ntries to answer these questions and o ffers valuable\ninsights for bias on MLLMs.\nThe contributions of this survey are as follows:\n\u2022 We present an overview of MLLMs and\nanalyze the language imbalance challenge\nwithin MLLMs, their capacity to support\nlow-resource languages and their potential\nfor cross-lingual transfer learning.\n\u2022 We provide an overview of the multilin-\ngual datasets and corpora utilized by existing"
      },
      {
        "source_id": "source_16",
        "citation": "Where Are We? Evaluating LLM Performance on African Languages.pdf",
        "text_snippet": "2024b) evaluated LLMs on African languages, re-\nvealing stark performance gaps between high- and\nlow-resource languages. Adebara et al. (2022) de-\nveloped AfroLID, a neural language identification\nmodel covering 517 African languages, showing"
      }
    ],
    "time_taken": 2.29
  },
  {
    "question": "According to Conneau (2020), how does XLM-R compare to mBERT?",
    "answer": "According to Conneau (2020), XLM-R outperforms mBERT on cross-lingual transfer tasks. Specifically, XLM-R achieves 80.9% accuracy on XNLI, outperforming mBERT by 14.6% average accuracy. On low-resource languages like Swahili and Urdu, XLM-R outperforms mBERT by 23.5% and 15.8%, respectively.",
    "citations_readable": [
      "Unsupervised Cross-Lingual Representation.pdf"
    ],
    "citations_raw": [
      "source_03"
    ],
    "retrieved_chunks": [
      {
        "source_id": "source_10",
        "citation": "Targeted Multilingual Adaptation for Low-resource Language Families.pdf",
        "text_snippet": "data can be found in Table 8 of Appendix B, along\nwith more details on our evaluation methodology.\nBaselines Our simplest baseline is \u201coff-the-shelf\u201d\nXLM-R \u2014 the pre-trained model from Conneau\net al. (2020a) with no modifications. We also test\nXLM-R adapted with LAPT, but without vocabu-\nlary specialization. LAPT alone is a strong baseline.\nHowever, as Downey et al. (2023) note, keeping\na large \u201ccross-lingual\u201d vocabulary during LAPT\nincurs considerable extra computational cost com-\npared to training a smaller, specialized vocabulary.\nGiven the observation that cross-lingual tokeniz-\ners are inefficient and ineffective for low-resource\nlanguages (\u00c1cs, 2019; Rust et al., 2021), we hy-\npothesize a specialized vocabulary will show a per-\nformance advantage in addition to the reduction in\ncomputational cost.\nWe also compare our multilingual models to\nbaselines adapted to single languages. While mul-\ntilingualism is known to help low-resource lan-"
      },
      {
        "source_id": "source_03",
        "citation": "Unsupervised Cross-Lingual Representation.pdf",
        "text_snippet": "sive study to assess this claim on the XNLI bench-\nmark. We extend our comparison between multilin-\ngual XLM models and monolingual BERT models\non 7 languages and compare performance in Ta-\nble5. We train 14 monolingual BERT models on\nWikipedia and CommonCrawl (capped at 60 GiB),\nand two XLM-7 models. We increase the vocab-\nulary size of the multilingual model for a better\ncomparison. We found thatmultilingual models\ncan outperform their monolingual BERT counter-\nparts. Speci\ufb01cally, in Table5, we show that for\ncross-lingual transfer, monolingual baselines out-\nperform XLM-7 for both Wikipedia and CC by\n1.6% and 1.3% average accuracy. However, by\nmaking use of multilingual training (translate-train-\nall) and leveraging training sets coming from mul-\ntiple languages, XLM-7 can outperform the BERT\nmodels: our XLM-7 trained on CC obtains 80.0%\naverage accuracy on the 7 languages, while the\naverage performance of BERT models trained on\nCC is 77.5%. This is a surprising result that shows"
      },
      {
        "source_id": "source_07",
        "citation": "LLMs Are Few-Shot In-Context Low-Resource Language Learners.pdf",
        "text_snippet": "across 3 text classification tasks using 44 different\ncross-lingual pairs. Their findings emphasize the\nlimitations of random exemplar selection and pro-\npose the use of semantic-based exemplar retrieval\nand label alignment 1 for superior X-ICL perfor-\nmance. Notably, their findings diverge from our\nresults (\u00a75.1), which contend that label alignment\ndoes not provide benefits for X-ICL.\n2.2 LLMs on Low-Resource Languages\nRigorous evaluations have been proposed to in-\nvestigate how LLMs perform on low-resource lan-\nguages. According to (Cahyawijaya et al., 2023a),\nwhile multilingual LLMs typically exhibit pos-\nitive transfer learning among related languages,\nthese models perform notably better for mid- and\nhigh-resource (e.g., Indonesian and English) com-\npared to low-resource languages (e.g., other 18\nIndonesian indigenous languages). This implies a\nchallenge in the generalization capability of exist-\ning multilingual LLMs to low-resource languages."
      },
      {
        "source_id": "source_21",
        "citation": "NaijaSenti_ A Nigerian Twitter Sentiment Corpus for Multilingual Sentiment Analysis.pdf",
        "text_snippet": "Face Transformer (Wolf et al., 2020). Appendix B pro-\nvides the details of the hyper-parameters used for train-\ning.\nmBERT is a multilingual variant of BERT pre-\ntrained on 104 languages, including one Nige-\nrian language\u2014Yor\u00f9b\u00e1. mBERT was pre-trained\nusing masked language modeling (MLM) and\nnext-sentence prediction task. We \ufb01ne-tune the\nmBERT-base-cased model with 172M model\nparameters by adding a linear classi\ufb01cation layer on\ntop of the pre-trained transformer model.\nXLM-R Similar to mBERT, XLM-R (Conneau et al.,\n2020) is a multilingual variant of RoBERTa pre-trained\non 100 languages, including Hausa as the only Nigerian\nlanguage. Unlike mBERT, XLM-R only uses MLM\nduring pre-training. We use XLR-base with 270M"
      },
      {
        "source_id": "source_13",
        "citation": "No Language Left Behind_ Scaling Human-Centered Machine Translation.pdf",
        "text_snippet": "between 50% and 90%. That hard threshold was chosen upon manual inspection, noticing\nthat many classification errors were found within that range. Human annotators were were\ntasked with inspecting our random sentences and assessing whether each was indeed in the\npredicted language.\nBased on these annotations, we built a challenge set for language identification to\nbenchmark our final LID model. As shown on Table 6, we achieve lower performance than\non the Flores-200 dataset, hinting at a non-negligible domain mismatch. We also compare\nperformance against CLD3. As suggested in Caswell et al. (2020), we report False Positive\nRates (FPR) on top of F1 scores, to get a better picture of how well our model would fare\nLanguage Ours F1 CLD3 F1 Ours FPR CLD3 FPR\nMicro 79.14 64.41 0.79 1.12\nMacro 74.16 60.13 0.77 1.12\nTable 6: Comparison of CLD3 and Our Model on a Challenge Set built from\nHuman Annotations. Only the average performance of languages supported by both"
      },
      {
        "source_id": "source_17",
        "citation": "AfriMTE and AfriCOMET_ Enhancing COMET to Embrace Under-resourced African Languages.pdf",
        "text_snippet": "respectively in Appendix A.1. Both AfroXLM-R-L\nbased systems (STL and MTL) tend to outperform\nN-gram matching based metrics, BERTScore and\nCOMET22, and show comparable or superior re-\nsults to GPT-4.\nParticularly, our results reveal improvements for\neng-ibo and eng-yor (FLORES, News, and TED\ntalks) when we utilize AfroXLM-R-L instead of\nXLM-R-L as encoder, aligning with their language\ncoverage in Table 9 in Appendix A.1. Addition-\nally, languages initially supported by XLM-R-L,\nsuch as eng-hau, eng-som and eng-xho, also experi-\nence enhancements with the adoption of AfroXLM-\nR-L. Interestingly, eng-kik and eng-luo transla-\ntion evaluations show marked improvements even\nthough Kikuyu and Luo are not explicitly cov-\nered by AfroXLM-R-L. Further analysis of correla-\ntions across four eng-yor domain-specific datasets\nshow that models trained based on AfroXLM-R-L\nhave the potential to surpass the performance of\nCOMET22, indicating its generalization for dif-"
      },
      {
        "source_id": "source_17",
        "citation": "AfriMTE and AfriCOMET_ Enhancing COMET to Embrace Under-resourced African Languages.pdf",
        "text_snippet": "Adequacy Annotation GuidelineYou are asked to compare the meaning of a source segment and its translation. You will be presented with one pair of segments at a time, where a segment maycontain one or more sentences. For each pair, you are asked to read the text closely and do the following:1. Highlight the text spans that convey different meaning in the compared segments. After highlighting a span in the text, you will be asked to select thecategory that best describes the meaning difference using the following categories:Source Text:Omission:The highlighted span in the source text corresponds to information that does not exist in the translated text.Mistranslation:The highlighted span in the source does not have the exact same meaning as the highlighted span in the translated text.Translation Text:Addition:The highlighted span in the translation corresponds to information that does not exist in the source text.Mistranslation:The highlighted span in the translation does not have the"
      },
      {
        "source_id": "source_03",
        "citation": "Unsupervised Cross-Lingual Representation.pdf",
        "text_snippet": "argue that in real applications, a small amount of\nsupervised data is often available for validation in\neach language.\nXLM-Rsets a new state of the art on XNLI. On\ncross-lingual transfer,XLM-Robtains 80.9% accu-\nracy, outperforming the XLM-100 and mBERT\nopen-source models by 10.2% and 14.6% aver-\nage accuracy. On the Swahili and Urdu low-\nresource languages,XLM-Routperforms XLM-100\nby 15.7% and 11.4%, and mBERT by 23.5% and\n15.8%. WhileXLM-Rhandles 100 languages, we\nalso show that it outperforms the former state of\nthe art Unicoder (Huang et al.,2019) and XLM\n(MLM+TLM), which handle only 15 languages, by\n5.5% and 5.8% average accuracy respectively. Us-\ning the multilingual training of translate-train-all,\nXLM-Rfurther improves performance and reaches\n83.6% accuracy, a new overall state of the art for\nXNLI, outperforming Unicoder by 5.1%. Multi-\nlingual training is similar to practical applications\nwhere training sets are available in various lan-"
      },
      {
        "source_id": "source_23",
        "citation": "Jawaher_ A Multidialectal Dataset of Arabic Proverbs for LLM  Benchmarking.pdf",
        "text_snippet": "\\lamwithalefisolated\\aleffinal\\sadmedial\\khahinitial\\aleffinal\\meeminitial\nDescribes how people\noften inherit their parent\u2019s\ntraits, good or bad.\nTUN\n\\lamfinal\\meemmedial\\jeemmedial\\laminitial\\alefisolated\\sheenisolated\\alefisolated\\rehfinal\\yehinitial\\aleffinal\\meeminitial\\alefisolated\\rehfinal\\yehinitial \\hehfinal\\tehmedial\\behinitial\\rehfinal\\kafinitial\\wawisolated\\rehfinal\\yehmedial\\ghaininitial \\tehmarbutafinal\\behinitial\\rehfinal\\kafinitial\nOne does not see one\u2019s\nown defects.\n\\sadfinal\\khahmedial\\sheenmedial\\laminitial\\alefisolated \\noonisolated\\alefwithhamzaaboveisolated \\aleffinal\\hehmedial\\behinitial \\dalfinal\\sadmedial\\qafmedial\\yehinitial\\alefmaksuraisolated\\rehfinal\\yehinitial \\noonfinal\\kafmedial\\laminitial \\hehfinal\\behinitial\\wawfinal\\yehmedial\\aininitial \\alefmaksuraisolated\\rehfinal\\yehinitial \\lamwithalefisolated\\hehisolated\\rehfinal\\yehmedial\\ghaininitial \\behisolated\\wawfinal\\yehmedial\\aininitial\nPeople who are quick to\npoint out others\u2019 faults"
      },
      {
        "source_id": "source_03",
        "citation": "Unsupervised Cross-Lingual Representation.pdf",
        "text_snippet": "5.3 Multilingual versus Monolingual\nIn this section, we present results of multilingual\nXLM models against monolingual BERT models.\nGLUE:XLM-Rversus RoBERTa.Our goal is\nto obtain a multilingual model with strong perfor-\nmance on both, cross-lingual understanding tasks\nas well as natural language understanding tasks\nfor each language. To that end, we evaluateXLM-\nRon the GLUE benchmark. We show in Table4,\nthatXLM-Robtains better average dev performance\nthan BERTLargeby 1.6% and reaches performance\non par with XLNetLarge. The RoBERTa model out-\nperformsXLM-Rby only 1.0% on average. We\nbelieve future work can reduce this gap even fur-\nther by alleviating the curse of multilinguality and"
      }
    ],
    "time_taken": 2.06
  },
  {
    "question": "What are the three main challenges in preserving cultural identity according to Anik (2025)?",
    "answer": "The three main challenges in preserving cultural identity according to Anik (2025) are: 1) AI-driven translation models often fail to capture cultural nuances, idiomatic expressions, and historical significance, 2) globalization and the dominance of major languages place many languages at risk of extinction, and 3) existing translation models prioritize efficiency over cultural authenticity, leading to translations that marginalize linguistic diversity.",
    "citations_readable": [
      "Preserving Cultural Identity with Context-Aware Translation Through  Multi-Agent AI Systems.pdf"
    ],
    "citations_raw": [
      "source_30"
    ],
    "retrieved_chunks": [
      {
        "source_id": "source_06",
        "citation": "CultureVLM_ Characterizing and Improving Cultural Understanding of Vision-Language Models for over 100 Countries.pdf",
        "text_snippet": "5.2 Main Results\nThe main results are in Figure 3 and detailed results are in Appendix B.2. Our main findings are as follows.\nTask Difficulty: Cultural Scene Reasoning Outperforms Recognition. From the task perspective, we observe that\nimage recognition and cultural knowledge questions pose challenges comparable to VLMs. Image recognition tests\nVLMs\u2019 ability to identify culturally specific objects or concepts, which relies heavily on diverse and relevant image\ndata. For instance, recognizing traditional foods like kimchi from Korea, or regional attire such as a sari from India,\nrequires the model to have encountered similar image-text pairs in its training data. In contrast, cultural knowledge\nquestions assess the model\u2019s understanding of broader cultural elements based on text-based training. For example,\nasking about the significance of a festival like Diwali or the symbolism of a red envelope during Lunar New Year taps"
      },
      {
        "source_id": "source_18",
        "citation": "Generative AI and Large Language Models in Language Preservation_  Opportunities and Challenges.pdf",
        "text_snippet": "content, and support for linguistic research.\n3. Identify Challenges Data scarcity and quality for a less-resourced language, technical complexity in cap-\nturing linguistic nuances, ensuring cultural authenticity and avoiding misrepresentation,\nupholding community ownership and data sovereignty, and addressing resource demands\n(computational, financial, personnel).\nOutputs\nOpportunities (O(Li, TAI)) Summarized as: Creation of accessible digital archives, engaging educational resources,\nincreased language visibility, and advanced research capabilities.\nChallenges (C(Li, TAI)) Summarized as: Need to overcome data limitations, technical hurdles, cultural/ethical\nrisks, and resource constraints.\nStrategies (S(Li)) Community-led development and governance, capacity building within the M \u00afaori\ncommunity, adoption of ethical AI guidelines prioritizing cultural protocols, iterative\ndevelopment with human oversight, and focus on low-resource AI techniques."
      },
      {
        "source_id": "source_31",
        "citation": "A Survey on Multilingual Large Language Models_ Corpora, Alignment, and  Bias.pdf",
        "text_snippet": "bias is naturally present in cross-cultural datasets\nand the design of MLLMs\u2019 modeling processes\n[12]. Previous studies have explored bias in vari-\nous NLP tasks and demographic groups \uff0cbut are\nlargely specific to English-based models [13, 14],\nwhich cannot be generalized to other languages.\nWhat are the types of bias in existing MLLMs?\nWhat are the main de-biasing techniques available\nfor MLLMs? Does the removal of these biases af-\nfect LLMs\u2019 performance? What are the existing\nbias evaluation datasets for MLLMs? These are\nvery worthwhile research questions. This survey\ntries to answer these questions and o ffers valuable\ninsights for bias on MLLMs.\nThe contributions of this survey are as follows:\n\u2022 We present an overview of MLLMs and\nanalyze the language imbalance challenge\nwithin MLLMs, their capacity to support\nlow-resource languages and their potential\nfor cross-lingual transfer learning.\n\u2022 We provide an overview of the multilin-\ngual datasets and corpora utilized by existing"
      },
      {
        "source_id": "source_30",
        "citation": "Preserving Cultural Identity with Context-Aware Translation Through  Multi-Agent AI Systems.pdf",
        "text_snippet": "sion, and marginalizes minority identities. Despite\ngrowing awareness, traditional preservation meth-\nods remain inadequate; documentation efforts fail\nto capture cultural complexity, while machine trans-\nlation distorts contextual meaning (Hutson et al.,\n2024). The digital linguistic divide further excludes\nunderrepresented languages, limiting their digital\npresence and corpus availability (Bella et al., 2023).\nAdditionally, economic pressures favor dominant\nglobal languages, leading younger generations to\nabandon their native tongues. While technologi-\ncal advancements offer potential solutions, current\napproaches often focus on efficiency rather than\ncultural authenticity, overlooking the need for lin-\nguistic preservation beyond translation (Mufwene,\n2005). As AI-driven methods become central to\nlanguage processing, it is essential to rethink how\narXiv:2503.04827v1  [cs.CL]  5 Mar 2025"
      },
      {
        "source_id": "source_20",
        "citation": "Prompting Towards Alleviating Code-Switched Data Scarcity in  Under-Resourced Languages with GPT as a Pivot.pdf",
        "text_snippet": "Multilingual communities, exemplified well by var-\nious African countries, often engage in code-\nswitching, where two or more languages are used\nwithin a single discourse (Poplack, 2001a). This\nlanguage practice highlights the need to develop\nmore advanced natural language processing (NLP)\ntechnologies that can smoothly process and pro-\nduce code-switched sentences. This will move\nthe needle towards equitable representation of the\nworld\u2019s under-resourced languages, ensuring that\neveryone has equal access to these technologies\n(Solorio, 2021).\nThere are numerous challenges in code-\nswitching research. The main three are highlighted\nby Do\u011fru\u00f6z et al. (2021) as follows: i) data, which\nis related to quantity, quality and availability; ii) eval-\nuation, which refers to benchmarks and metrics;\nand iii) challenges related to end-to-end applica-\ntions,particularlytheabilitytoprocessandproduce\ncode-switched data.\nThe focus of this paper is on the first challenge"
      },
      {
        "source_id": "source_12",
        "citation": "A Survey on Large Language Models with Multilingualism_ Recent Advances and New Frontiers.pdf",
        "text_snippet": "that preserve cultural context.\n\u2022 Exploration of techniques for LLMs to effectively integrate and leverage language-specific\nin-domain knowledge.\nBy addressing these two challenges, researchers can pave the way for the development of truly robust\nand equitable LLMs that serve a global audience in multilingual scenarios.\n23"
      },
      {
        "source_id": "source_06",
        "citation": "CultureVLM_ Characterizing and Improving Cultural Understanding of Vision-Language Models for over 100 Countries.pdf",
        "text_snippet": "Scene Reasoning Questions are designed to assess the model\u2019s ability to interpret, interact, and respond within\nculturally specific contexts, rather than simply recalling factual information as in the previous two categories. We curate\nscenarios with cultural elements or characteristics depicted in the images, providing context cues that challenge the\nVLMs to make contextually appropriate choices. Using the detailed introductions of the previous steps, we prompt\nGPT-4o to generate scenario-based reasoning questions. These questions require the model to not only recognize\ncultural concepts but also apply contextual reasoning based on the associated cultural knowledge.\n3.3 Quality Assurance\nTo ensure the integrity of the dataset, we conduct a comprehensive manual quality check on each cultural concept,\nalong with its corresponding images and questions. Specifically, we employ human annotators (details are shown in\nAppendix C) to inspect three main components:"
      },
      {
        "source_id": "source_30",
        "citation": "Preserving Cultural Identity with Context-Aware Translation Through  Multi-Agent AI Systems.pdf",
        "text_snippet": "2024. Preserving linguistic diversity in the digital\nage: a scalable model for cultural heritage continuity.\nJournal of Contemporary Language Research, 3(1).\nGraham M Jones, Shai Satran, and Arvind Satyanarayan.\n2025. Toward cultural interpretability: A linguistic\nanthropological framework for describing and eval-\nuating large language models. Big Data & Society,\n12(1):20539517241303118.\nAnne Kandler and Roman Unger. 2023. Modeling lan-\nguage shift. In Diffusive spreading in nature, tech-\nnology and society, pages 365\u2013387. Springer.\nNir Kshetri. 2024. Linguistic challenges in generative\nartificial intelligence: Implications for low-resource\nlanguages in the developing world.\nHaley Lepp and Parth Sarin. 2024. A global ai com-\nmunity requires language-diverse publishing. arXiv\npreprint arXiv:2408.14772.\nJiangfeng Liu, Ziyi Wang, Jing Xie, and Lei Pei. 2024.\nFrom chatgpt, dall-e 3 to sora: How has generative\nai changed digital humanities research and services?"
      },
      {
        "source_id": "source_19",
        "citation": "Bridging Gaps in Natural Language Processing for Yoru\u0300ba\u0301_ A  Systematic Review of a Decade of Progress and Prospects.pdf",
        "text_snippet": "tasks, techniques, resources, and challenges. The research questions are presented sequentially\nin Table 1 to cater to structure flow and a quick overview.\nTable 1: Research questions\nS/N Research Questions\nRQ1 What NLP tasks have been addressed for Yor` ub\u00b4 a language?\nRQ2 What techniques have been employed for Yor` ub\u00b4 a NLP?\nRQ3 What language resources are available for Yor` ub\u00b4 a language?\nRQ4 What challenges are associated with NLP development involving Yor` ub\u00b4 a\nlanguage?\n3.3 Search Strategy\nThis section describes the gathering of the vital primary studies and the \u201csystematic\u201d steps\ntoward achieving the goal. It is a crucial phase planned to eliminate potential bias and in-\ncorporate randomisation in the studies\u2019 selection and sample size determination. Reputable\ndatabases related to the subject matter were painstakingly explored to obtain all relevant stud-\nies for the systematic review using a well-defined search strategy. Generally, the central goal"
      },
      {
        "source_id": "source_30",
        "citation": "Preserving Cultural Identity with Context-Aware Translation Through  Multi-Agent AI Systems.pdf",
        "text_snippet": "Preserving Cultural Identity with Context-Aware Translation Through\nMulti-Agent AI Systems\nMahfuz Ahmed Anik1 Abdur Rahman1, Azmine Toushik Wasi1\u2020, Md Manjurul Ahsan2\n1Shahjalal University of Science and Technology, Sylhet, Bangladesh\n2University of Oklahoma, Norman, OK 73019, USA\n\u2020Correspondence: azmine32@student.sust.edu\nAbstract\nLanguage is a cornerstone of cultural identity,\nyet globalization and the dominance of major\nlanguages have placed nearly 3,000 languages\nat risk of extinction. Existing AI-driven trans-\nlation models prioritize efficiency but often fail\nto capture cultural nuances, idiomatic expres-\nsions, and historical significance, leading to\ntranslations that marginalize linguistic diver-\nsity. To address these challenges, we propose\na multi-agent AI framework designed for cul-\nturally adaptive translation in underserved lan-\nguage communities. Our approach leverages\nspecialized agents for translation, interpreta-\ntion, content synthesis, and bias evaluation,"
      }
    ],
    "time_taken": 1.94
  },
  {
    "question": "How does the 'Cheetah' paper propose to handle 517 African languages?",
    "answer": "The 'Cheetah' paper proposes to handle 517 African languages by developing a massively multilingual natural language generation (NLG) language model specifically designed for these languages. Cheetah is trained using a 42G multi-domain corpus across 517 African languages and language varieties, covering languages spoken in 50 of 54 African countries and written with five scripts. The model leverages a vast corpus of text data collected from diverse sources, including news articles, health documents, religious texts, legal documents, and social media feeds, to learn intricate linguistic information that characterizes each African language. This approach addresses the scarcity of NLG resources for African languages and aims to foster linguistic diversity, improve access to important information, and support language preservation.",
    "citations_readable": [
      "Cheetah_ Natural Language Generation for 517 African Languages.pdf"
    ],
    "citations_raw": [
      "source_04"
    ],
    "retrieved_chunks": [
      {
        "source_id": "source_04",
        "citation": "Cheetah_ Natural Language Generation for 517 African Languages.pdf",
        "text_snippet": "Cheetah\n : Natural Language Generation for 517 African Languages\nIfe Adebara1,\u22c6 AbdelRahim Elmadany1,\u22c6 Muhammad Abdul-Mageed1,2\n1Deep Learning & Natural Language Processing Group, The University of British Columbia\n2Department of Natural Language Processing & Department of Machine Learning, MBZUAI\n{ife.adebara@,a.elmadany@,muhammad.mageed@}ubc.ca\nAbstract\nLow-resource African languages pose unique\nchallenges for natural language processing\n(NLP) tasks, including natural language\ngeneration (NLG). In this paper, we develop\nCheetah, a massively multilingual NLG\nlanguage model for African languages.\nCheetah supports 517 African languages and\nlanguage varieties, allowing us to address\nthe scarcity of NLG resources and provide\na solution to foster linguistic diversity. We\ndemonstrate the effectiveness of Chee-\ntah through comprehensive evaluations across\nsix generation downstream tasks. In five of the\nsix tasks, Cheetah significantly outperforms\nother models, showcasing its remarkable"
      },
      {
        "source_id": "source_04",
        "citation": "Cheetah_ Natural Language Generation for 517 African Languages.pdf",
        "text_snippet": "against native speakers of those languages.\nMore precisely, it does so by focusing on\nAfrica. To the best of our knowledge, Chee-\ntah is the first massively multilingual PLM de-\nveloped for African languages and language\nvarieties. A model with knowledge of 517\nAfrican languages, is by far the largest to date\nfor African NLP.\n2. Cheetah enables improved access of import-\nant information to the African community in\nIndigenous African languages. This is espe-\ncially beneficial for people who may not be\nfluent in other languages. This will potentially\nconnect more people globally.\n3. Cheetah affords opportunities for language\npreservation for many African languages. To\nthe best of our knowledge, Cheetah consists\nof languages that have not been used for any\nNLP task until now. We believe that it can\nhelp encourage continued use of these lan-\nguages in several domains, as well as trigger\nfuture development of language technologies\nfor many of these languages."
      },
      {
        "source_id": "source_08",
        "citation": "IrokoBench_ A New Benchmark for African Languages in the Age of Large  Language Models.pdf",
        "text_snippet": "Twitter sentiment analysis benchmark for African\nlanguages. In Proceedings of the 2023 Conference\non Empirical Methods in Natural Language Process-\ning, pages 13968\u201313981, Singapore. Association for\nComputational Linguistics.\nYixin Nie, Adina Williams, Emily Dinan, Mohit Bansal,\nJason Weston, and Douwe Kiela. 2020. Adversarial\nNLI: A new benchmark for natural language under-\nstanding. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 4885\u20134901, Online. Association for Computa-\ntional Linguistics.\nJessica Ojo, Kelechi Ogueji, Pontus Stenetorp, and\nDavid I Adelani. 2023. How good are large lan-\nguage models on african languages? arXiv preprint\narXiv:2311.07978.\nOpenAI. 2024. Introducing ChatGPT. https://\nopenai.com/index/chatgpt/. [Accessed 01-06-\n2024].\nOpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal,\nLama Ahmad, Ilge Akkaya, Florencia Leoni Ale-\nman, Diogo Almeida, Janko Altenschmidt, Sam Alt-"
      },
      {
        "source_id": "source_04",
        "citation": "Cheetah_ Natural Language Generation for 517 African Languages.pdf",
        "text_snippet": "and syntactic structures of the languages they are\ntrained on, making them valuable starting points\nfor supporting NLG in low-resource settings.\nIn this paper, we present a pioneering work on\nNLG in African languages by introducing Chee-\ntah: a novel language model (LM) specifically de-\nsigned to support 517 African languages and lan-\nguage varieties. To the best of our knowledge,\nCheetah supports the largest number of African\nlanguages and language varieties. Leveraging a\nvast corpus of text data collected from diverse\nsources, Cheetah learns some intricate linguistic in-\nformation that characterize each African language.\nThe contributions of this research are three fold.\nFirst, we address the scarcity of NLG resources\nfor African languages by providing a comprehens-\nive language model that covers a wide range of\nlanguages spoken on the continent. Second, we\ndemonstrate the efficacy of our approach through\nextensive evaluations across six downstream task"
      },
      {
        "source_id": "source_04",
        "citation": "Cheetah_ Natural Language Generation for 517 African Languages.pdf",
        "text_snippet": "tah through comprehensive evaluations across\nsix generation downstream tasks. In five of the\nsix tasks, Cheetah significantly outperforms\nother models, showcasing its remarkable\nperformance for generating coherent and\ncontextually appropriate text in a wide range\nof African languages. We additionally conduct\na detailed human evaluation to delve deeper\ninto the linguistic capabilities of Cheetah.\nThe introduction of Cheetah has far-reaching\nbenefits for linguistic diversity. By leveraging\npretrained models and adapting them to\nspecific languages, our approach facilitates the\ndevelopment of practical NLG applications\nfor African communities. The findings of\nthis study contribute to advancing NLP\nresearch in low-resource settings, enabling\ngreater accessibility and inclusion for African\nlanguages in a rapidly expanding digital\nlandscape. We will publicly release our models\nfor research. 1\n1 Introduction\nThe linguistic diversity present in African lan-"
      },
      {
        "source_id": "source_16",
        "citation": "Where Are We? Evaluating LLM Performance on African Languages.pdf",
        "text_snippet": "Ife Adebara and Muhammad Abdul-Mageed. 2022. To-\nwards afrocentric NLP for African languages: Where\nwe are and where we can go. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n3814\u20133841, Dublin, Ireland. Association for Compu-\ntational Linguistics.\nIfe Adebara, AbdelRahim Elmadany, and Muhammad\nAbdul-Mageed. 2024a. Cheetah: Natural language\ngeneration for 517 African languages. In Proceed-\nings of the 62nd Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 12798\u201312823, Bangkok, Thailand and\nvirtual meeting. Association for Computational Lin-\nguistics.\nIfe Adebara, AbdelRahim Elmadany, and Muhammad\nAbdul-Mageed. 2024b. Cheetah: Natural language\ngeneration for 517 african languages.\nIfe Adebara, AbdelRahim Elmadany, Muhammad\nAbdul-Mageed, and Alcides Alcoba Inciarte. 2023.\nSERENGETI: Massively multilingual language mod-"
      },
      {
        "source_id": "source_04",
        "citation": "Cheetah_ Natural Language Generation for 517 African Languages.pdf",
        "text_snippet": "data. To address this issue, we undertook the task\nof manually curating a diverse corpus spanning\nmultiple domains, including news articles, health\ndocuments, religious texts, legal documents, and\nsocial media feeds. This manual curation approach\nwas necessary because there were no existing data-\nsets available for the majority of the languages we\naimed to support, and we wanted to ensure the\nutilization of reliable and high-quality data.\nCoverage. In all, we train Cheetah using a 42G\nmulti-domain corpus across 517 African languages\nand language varieties. The languages are spoken\nin 50 of 54 African countries and they are written\nwith five scripts. This provides support to at least\n500M Africans.\nLinguistic Diversity. The inclusion of languages\nfrom various domains, geographical regions, and\nlinguistic typologies, along with the utilization of\nreliable data sources, contributes to enhancing the\nrobustness and quality of Cheetah. Our data con-"
      },
      {
        "source_id": "source_04",
        "citation": "Cheetah_ Natural Language Generation for 517 African Languages.pdf",
        "text_snippet": "Category LM Lang/Total African Languages Families\nMultilingual\nMBART 3/50 afr, swh, yor. 2\nMT0 14 /101 afr, amh, hau, ibo, lin, mlg, nyj, orm, sot, 4\nsna, som, swh, xho, yor, and zul\nMT5 12 /101 afr, amh, nya, hau, ibo, mlg, sna, som, swh, xho, yor, and zul 3\nAfrican\nAfriVeTa 10 /10 gaz, amh, Gahuza, hau, ibo, pcm, som, swa, tir, and yor. 3\nAfriMT5 17 /17 bam, bbj, ewe, fon, hau, ibo, lug, luo, pcm, mos, swa, tsn, twi, wol, yor, zul. 3\nAfriByT5 17 /17 bam, bbj, ewe, fon, hau, ibo, lug, luo, pcm, mos, swa, tsn, twi, wol, yor, zul. 3\nAfriMBART 17 /17 afr, amh, nya, hau, orm, som, swh, xho. 3\nCheetah\n 517 /517 Includes 517 African languages. 14\nTable 1: Comparing with available encoder-decoder models with African languages represented. Lang/Total.\ndescribe the number of African languages comparing with the covered languages in the pretrained language models.\nFamilies. describes the number of covered language families.\nCategory Benchmark Reference Task Lang/Total Datasets Tasks"
      }
    ],
    "time_taken": 2.85
  },
  {
    "question": "What metrics were used to evaluate the 'NaijaSenti' corpus?",
    "answer": "The 'NaijaSenti' corpus was evaluated using benchmark experiments on Twitter sentiment dataset with state-of-the-art pre-trained language models and transfer learning. The results indicated that language-specific models and language-adaptive fine-tuning performed the best on average.",
    "citations_readable": [
      "NaijaSenti_ A Nigerian Twitter Sentiment Corpus for Multilingual Sentiment Analysis.pdf"
    ],
    "citations_raw": [
      "source_21"
    ],
    "retrieved_chunks": [
      {
        "source_id": "source_19",
        "citation": "Bridging Gaps in Natural Language Processing for Yoru\u0300ba\u0301_ A  Systematic Review of a Decade of Progress and Prospects.pdf",
        "text_snippet": "tasks, techniques, resources, and challenges. The research questions are presented sequentially\nin Table 1 to cater to structure flow and a quick overview.\nTable 1: Research questions\nS/N Research Questions\nRQ1 What NLP tasks have been addressed for Yor` ub\u00b4 a language?\nRQ2 What techniques have been employed for Yor` ub\u00b4 a NLP?\nRQ3 What language resources are available for Yor` ub\u00b4 a language?\nRQ4 What challenges are associated with NLP development involving Yor` ub\u00b4 a\nlanguage?\n3.3 Search Strategy\nThis section describes the gathering of the vital primary studies and the \u201csystematic\u201d steps\ntoward achieving the goal. It is a crucial phase planned to eliminate potential bias and in-\ncorporate randomisation in the studies\u2019 selection and sample size determination. Reputable\ndatabases related to the subject matter were painstakingly explored to obtain all relevant stud-\nies for the systematic review using a well-defined search strategy. Generally, the central goal"
      },
      {
        "source_id": "source_21",
        "citation": "NaijaSenti_ A Nigerian Twitter Sentiment Corpus for Multilingual Sentiment Analysis.pdf",
        "text_snippet": "models and transfer learning. The results indicate that\nlanguage-speci\ufb01c models and language-adaptive \ufb01ne-\ntuning perform the best on average. NaijaSenti has\nthe potential to spark interest in sentiment analysis\nand other downstream NLP tasks in the languages in-\nvolved. As future work, we plan to create benchmark\nexperiments with our sentiment lexicon, and extend our\ndataset (NaijaSenti) to include other African languages\n(AfriSenti).\n8. Acknowledgements\nWe thank Daan van Esch for feedback on a draft of\nthis article. This work was carried out with support\nfrom Lacuna Fund, an initiative co-founded by The\nRockefeller Foundation, Google.org, and Canada\u2019s In-\nternational Development Research Centre. The views\nexpressed herein do not necessarily represent those\nof Lacuna Fund, its Steering Committee, its fun-\nders, or Meridian Institute. This work is also par-\ntially funded by the National Funds through the Por-\ntuguese funding agency, FCT - Funda\u00e7\u00e3o para a Ci\u00ean-"
      },
      {
        "source_id": "source_06",
        "citation": "CultureVLM_ Characterizing and Improving Cultural Understanding of Vision-Language Models for over 100 Countries.pdf",
        "text_snippet": "cultural diversity.\nImage Retrieval. Using these concepts and their corresponding countries, we scrape images from Google Images for\neach cultural concept1, obtaining five images for each concept. The first image was reserved for the test set and for\nhuman quality assessment, while the remaining four images were used for the training set. Images larger than 10MB\nwere compressed to ensure compatibility with typical input requirements of VLMs.\n3.2 Question-Answer Generation\nWe designed three levels of VQA tasks to assess and improve the multicultural knowledge of VLMs:\nImage Recognition Questions evaluate models\u2019 ability to identify cultural concepts in images. Accurate identification\nof such concepts is fundamental to retrieving relevant cultural knowledge. Given an image and a cultural concept,\nmodels answer questions like \u201cWhat dish is in the image?\u201d\nCultural Knowledge Questions further evaluate model\u2019s deeper understanding of the cultural background associated"
      },
      {
        "source_id": "source_21",
        "citation": "NaijaSenti_ A Nigerian Twitter Sentiment Corpus for Multilingual Sentiment Analysis.pdf",
        "text_snippet": "10 100 500 1k 2.5k 5k 10k 15k\n0.4\n0.5\n0.6\n0.7\n0.8\nNumber of Sentences\nF1-score\nhau F1 ibo F1\nyor F1 Zero-shot hau\nZero-shot ibo Zero-shot yor\nFigure 2: Sample Ef\ufb01ciency on hau, ibo, and yor\nusing the AfriBERTa model.\ntransfer settings.\n7. Conclusions and Future Work\nIn this paper, we present NaijaSenti\u2014the \ufb01rst publicly\navailable large-scale and manually annotated Twitter\nsentiment dataset for the four main Nigerian languages\n(Hausa, Igbo, Nigerian-pidgin, and Yor\u00f9b\u00e1). We pro-\npose methods to enable the collection, \ufb01ltering, and\nannotation of such low-resource language data. Addi-\ntionally, we introduce a manually annotated sentiment\nlexicon in three languages (Hausa, Igbo, and Yor\u00f9b\u00e1).\nWe present benchmark experiments on Twitter senti-\nment dataset using state-of-the-art pre-trained language\nmodels and transfer learning. The results indicate that\nlanguage-speci\ufb01c models and language-adaptive \ufb01ne-\ntuning perform the best on average. NaijaSenti has"
      },
      {
        "source_id": "source_08",
        "citation": "IrokoBench_ A New Benchmark for African Languages in the Age of Large  Language Models.pdf",
        "text_snippet": "scores for those three languages since they are not\ncovered in the pre-training of the original AfroX-\nLMR encoder (Alabi et al., 2022) used to build\nthe AfriCOMET QE metric. Similar findings were\nreported in the original AfriCOMET QE paper that\nTwi had worse correlation with human judgement\n(i.e., 0.279 for Pearson, and 0.060 for Spearman)\n(Wang et al., 2024). We provide further analysis of\nthe COMET scores in Appendix A.3.\n3.4 LLMs used for evaluation\nOpen LLMs We evaluate on two encoder-\ndecoder open LLM: mT0-XXL-MT (Muennighoff\n2We recruited a logistic company in Kenya that managed\nall recruitment and payments\u2014each country has different\nrates. For example, we paid $549.78 for the translation of\n1020 XNLI samples in South Africa, $355.86 in Nigeria.\n4"
      },
      {
        "source_id": "source_19",
        "citation": "Bridging Gaps in Natural Language Processing for Yoru\u0300ba\u0301_ A  Systematic Review of a Decade of Progress and Prospects.pdf",
        "text_snippet": "the performance of state-of-the-art models on the dataset were carried out, and this mainly\ninvolved automatic translation, which entails translating questions or retrieved documents in\npreparation for processing, and multilingual retrieval, involving carrying out direct retrieval\nthrough the aid of multilingual embeddings.\nAfriSenti: AfriSenti [4] focuses on providing a high-quality, large-scale dataset to address the\nlack of resources for sentiment analysis in underrepresented African languages. This benchmark\ncontains over 110 , 000 posts involving 14 African languages across four language families\u2014\nNiger-Congo, Afro-Asiatic, English Creole and Indo-European\u2014in which Yor` ub\u00b4 a is a core part.\nThe languages featured in the dataset are namely: Amharic, Algerian Arabic, Hausa, Igbo,\nKinyarwanda, Moroccan Arabic, Mozambican Portuguese, Nigerian Pidgin, Oromo, Swahili,\nTigrinya, Twi, Xitsonga, and Yor` ub\u00b4 a. The sentiment categories captured in the corpus include"
      },
      {
        "source_id": "source_26",
        "citation": "Global MMLU_ Understanding and Addressing Cultural and Linguistic Biases  in Multilingual Evaluation.pdf",
        "text_snippet": "Prehistory What is the name of the lithic tech-\nnology seen in the Arctic and con-\nsisting of wedge-shaped cores, micro-\nblades, bifacial knives, and burins?\n(A) Clovis Complex\n(B) Denali Complex\n(C) Folsom Complex\n(D) Nenana Complex\nUS F oreign Policy What was the key difference between\nUS expansion pre- and post- 1865?\n(A) US expansion was based on territory\nrather than markets post-1865\n(B) USexpansionwasbasedonmarketsrather\nthan territory post-1865\n(C) US expansion was limited to Latin Amer-\nica post-1865\n(D) US expansion ended after 1865\nEconometrics Which of the following statements\nwill be true if the number of repli-\ncations used in a Monte Carlo study\nis small? i) The statistic of inter-\nest may be estimated imprecisely ii)\nThe results may be affected by un-\nrepresentative combinations of ran-\ndom draws iii) The standard errors\non the estimated quantities may be\nunacceptably large iv) Variance re-\nduction techniques can be used to re-\nduce the standard errors"
      },
      {
        "source_id": "source_21",
        "citation": "NaijaSenti_ A Nigerian Twitter Sentiment Corpus for Multilingual Sentiment Analysis.pdf",
        "text_snippet": "using Interval Type-2 Fuzzy Logic.\nYor\u00f9b\u00e1 Orimaye et al. (2012) built a Yor\u00f9b\u00e1 corpus\nfrom YouTube and applied a translated SentiWordNet\nfor the sentiment analysis task. Iyanda and Abegunde\n(2019) created a multi-domain corpus (health, business,\neducation, politics) and used different classic ML clas-\nsi\ufb01ers such as SVM to predict sentiment in text.\nPidgin Oyewusi et al. (2020) built a Pidgin tweet\ncorpus and used a translated V ADER English lexicon\nfor sentiment analysis.\nTable 1 summarizes the existing datasets for Nigerian\nlanguages; only two datasets are freely available, in-\ndicating that more work is needed to make indigenous\ndatasets accessible and to stimulate research in these\nlanguages. To the best of our knowledge, ours is the\n\ufb01rst publicly available large-scale manually annotated\ndataset for sentiment analysis research in the following\nNigerian languages: Hausa, Igbo, Yor\u00f9b\u00e1 and Nigerian\nPidgin (see Appendix A for the language description\nand characteristics.)"
      },
      {
        "source_id": "source_23",
        "citation": "Jawaher_ A Multidialectal Dataset of Arabic Proverbs for LLM  Benchmarking.pdf",
        "text_snippet": "bic proverbs. To achieve this, we create a universal\nprompt template in English: (1) We set therole of the\nmodel as a language expert with deep knowledge of\nArabic proverbs, cultural history, and literary mean-\nings. (2) We deliver the test input to the model to\nproduce the output. (3) We provide the name of the\ntask that needs to be performed. (4) We specify the\ncontext, asking to include any relevant background\nstories or cultural context that could be helpful for\nexplanations on the tasks. (5) Finally, we define what\nshould be the expected outcome of the model. The\nprompt used to evaluate mLLMs can be found in\nFigure 5.\n4.1 Models\nOpen Source. For open- source mLLMs, we\nevaluate two models from the Llama 3 fam-\nily (Dubey et al., 2024): Llama-3.1-8B-Instruct,\nLlama-3.2-3B-Instruct, and Google\u2019s\nGemma-2-9B-it (Team et al., 2024). These\nmodels were chosen for their varying sizes and\nmultilingual capabilities, including support for\nArabic."
      },
      {
        "source_id": "source_21",
        "citation": "NaijaSenti_ A Nigerian Twitter Sentiment Corpus for Multilingual Sentiment Analysis.pdf",
        "text_snippet": "dataset for sentiment analysis research in the following\nNigerian languages: Hausa, Igbo, Yor\u00f9b\u00e1 and Nigerian\nPidgin (see Appendix A for the language description\nand characteristics.)\n3. Data Collection and Cleaning\n3.1. Data Collection\nTwitter provides easy access to a large amount\nof domain-independent and topic-independent public\nopinionated user-generated data. We collected tweets\nusing the Twitter Academic API 3, which provides\nreal-time and historical tweet data. The Twitter API\nsupports retrieving tweets in 70 languages (including\nAmharic as the only African language) using language\nparameters. This makes it easy to extract a tweet in\n3https://developer.twitter.com/en/\nproducts/twitter-api/academic-research"
      }
    ],
    "time_taken": 1.43
  },
  {
    "question": "Does the 'Localising SA official languages' paper recommend manual or automated collection?",
    "answer": "Insufficient Evidence",
    "citations_readable": [],
    "citations_raw": [],
    "retrieved_chunks": [
      {
        "source_id": "source_09",
        "citation": "The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks.pdf",
        "text_snippet": "relationships and ensure that the generated labels remain meaningful and contextually relevant, we\ndeliberately exclude training datasets, unlabeled datasets, machine translation datasets, language\nidentification datasets, and multi-modal datasets from our study. Furthermore, we also exclude the\nprogramming languages from our study.\nCollection In this work, we collect papers under thecs.CL category of arXiv from January 1, 2021,\nto December 31, 2024 using the arXiv API.\u2217 The arXiv API provides programmatic access to metadata\nand abstracts of papers, enabling efficient data collection. From this process, we initially retrieved a\ntotal of 370K papers. To refine the dataset, we utilizeQ w e n 2 . 5 - 7 B - I n s t ruc tto analyze the\nabstracts of each paper and filter out those irrelevant to our study. Following this automated step,\nwe conduct a manual review to ensure the suitability of each paper for inclusion in our study. This"
      },
      {
        "source_id": "source_25",
        "citation": "WAXAL_ A Large-Scale Multilingual African Language Speech Corpus.pdf",
        "text_snippet": "Saharan African languages collected in partnership with four\nAfrican academic and community partners and supported by\nGoogle\u2019s technical mentorship. The collection features both\nASR and TTS data for languages such as Acholi, Kiswahili,\nLuganda, Akan, and Ewe; ASR data for Shona, Fula, and\nLingala; and TTS data for Hausa, Igbo, and Yoruba, among\nothers. Our primary contributions are:\n\u2022 A Large-Scale ASR Corpus: We release approximately\n1,250 hours of transcribed, image-prompted natural speech\nfrom a diverse pool of speakers, suitable for training and\narXiv:2602.02734v1  [eess.AS]  2 Feb 2026"
      },
      {
        "source_id": "source_06",
        "citation": "CultureVLM_ Characterizing and Improving Cultural Understanding of Vision-Language Models for over 100 Countries.pdf",
        "text_snippet": "and have a single correct answer. Annotators refined the questions and answer options by removing redundant\ninformation and resolving any ambiguities to maintain clarity and accuracy.\nFollowing the quality assurance process, we utilized human annotations for the evaluation set of CultureVerse\nwhile applying the automated annotation pipeline to the larger training set. With over 98% of the evaluation set\nsamples correctly annotated by the automated process, we conclude that the pipeline is highly effective. Any remaining\nerroneous or challenging samples that could not be refined were filtered out to maintain the dataset\u2019s high quality.\nAdditional details on annotator accuracy are in Appendix C.2.\n3.4 Scalability\nOur approach to constructing multimodal cultural datasets is notably more scalable and comprehensive than existing\nmethods. Existing ones mostly rely on manual efforts to search for cultural concepts, retrieve images, and formulate\n5"
      },
      {
        "source_id": "source_08",
        "citation": "IrokoBench_ A New Benchmark for African Languages in the Age of Large  Language Models.pdf",
        "text_snippet": "larly used for QA tasks.\n3.3 Data collection process\nTranslation We recruited language coordinators\nfor each of the 16 African languages and French,\nand asked them to recruit professional translators\nto translate the sentences. The translation process\ntook about two months, they started with XNLI,\nthen MGSM and MMLU. Each translator received\nan appropriate remuneration for their work. 2 Most\nof the translators translated from English except\nfor Ewe, Lingala and Wolof translators that trans-\nlated from French since they are from the Fran-\ncophone region of Africa. Additionally, we trans-\nlated the MMLU dataset to French by professional\ntranslators and from French to these three lan-\nguages. Many of the Francophone translators un-\nderstand French and English but are more fluent in\nFrench, so they could cross-check from English if\nthe French sentences were not clear enough.\nQuality control Regarding quality control, lan-\nguage coordinators reviewed and corrected any"
      },
      {
        "source_id": "source_16",
        "citation": "Where Are We? Evaluating LLM Performance on African Languages.pdf",
        "text_snippet": "adoption of a foreign language for official func-\ntions. However, in some instances, select Indige-\nnous African languages receive official recognition\nat regional or national levels or within educational\ncontexts (Petzell, 2012; Foster, 2021; Ouane and\nGlanz, 2010). 2 For instance in Nigeria, English is\nthe official language, with only three out of 512 In-\ndigenous languages recognized at the regional level.\nSimilarly, Ghana designates English as its official\nlanguage while also recognizing ten of its 73 In-\ndigenous languages for institutional use. In Tanza-\nnia, Swahili is the sole Indigenous official language\namong 118 languages, alongside English. Kenya\ngrants official status to12 of its 61 languages, while\nSouth Africa recognizes 12 out of 20 Indigenous\nlanguages as institutional languages (Adebara and\nAbdul-Mageed, 2022). Figures 1(a), 1(b), 1(c),\nand 1(d) illustrate the distribution of language poli-\ncies across the continent.\nEven when Indigenous languages are granted of-"
      },
      {
        "source_id": "source_26",
        "citation": "Global MMLU_ Understanding and Addressing Cultural and Linguistic Biases  in Multilingual Evaluation.pdf",
        "text_snippet": "et al., 2024) involved collaboration across multiple institutions globally. Such cross-sectional ef-\nforts are crucial for gathering linguistic data at scale and fostering community engagement\u2014both\nessential for developing inclusive language technologies (Joshi et al., 2019; Nekoto et al., 2020;\nSingh et al., 2024; Romanou et al., 2024). We established a criterion requiring a minimum of\n50 human-translated samples for each language before its inclusion inGlobal-MMLU\n . This\nthreshold was met by eleven languages:Amharic, Czech, Malay, Persian, Romanian, Russian,\nSinhala, Telugu, Turkish, Ukrainian, andVietnamese. In the following sections, we refer to this\nset of languages as\u201cCommunity Translated\u201d.\nThe participation of native speakers from diverse regions introduced logistical challenges in both\ndata selection and quality control. To overcome these, we adopted Argilla5 as our primary\nannotation platform. In line with our community-based approach, Argilla\u2019s collaborative features"
      },
      {
        "source_id": "source_09",
        "citation": "The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks.pdf",
        "text_snippet": "we conduct a manual review to ensure the suitability of each paper for inclusion in our study. This\nrigorous process resulted in a final dataset of 2,024 papers.\nAnnotation Besides utilizing metadata from the arXiv API, three authors manually annotate the\ncollected papers following the annotation scheme presented in Table 1. These authors, each with at\nleast one year of experience in NLP research and proficiency in multiple languages, bring valuable\nexpertise to the annotation process. The additional annotations not only provide new insights into\nthe data but also serve as a valuable resource for future research.\n\u2217https://info.arxiv.org/help/api/index.html\n5"
      },
      {
        "source_id": "source_08",
        "citation": "IrokoBench_ A New Benchmark for African Languages in the Age of Large  Language Models.pdf",
        "text_snippet": "soning and knowledge-intensive tasks across the\nmajority of African languages.\nFurthermore, the few comprehensive evaluations\nthat exist across languages often rely on machine\ntranslation of English benchmarks (Singh et al.,\n2024). While automatic translation from English\nbenchmarks is a popular approach given the cost\n1\narXiv:2406.03368v2  [cs.CL]  23 Jan 2025"
      },
      {
        "source_id": "source_05",
        "citation": "Exploring NLP Benchmarks in an Extremely Low-Resource Setting.pdf",
        "text_snippet": "contexts;t3(1,563 sentences) includes narrative\nprose, dialogue, and idiomatic expressions, offer-\ning diverse challenges. For Italian in the testing\ndataset, the average word counts fort1,t2, andt3\nare 21.25, 26.65, and 15.61, respectively. In com-\nparison, the corresponding averages fort1,t2, and\nt3in Ladin are 23.58, 24.52, and 13.05, respec-\ntively.\nSentiment analysis.We construct datasets for\nNLP tasks in Ladin by leveraging labeled mono-\nlingual Italian resources, including SA and MCQA\ndatasets. We use the monolingual Italian SA dataset\nDIta_SAfromDesole(2020) for the SA task.DIta_SA\ncomprises abundant Tripadvisor reviews labeled\nas positive or negative. The initial dataset con-\ntains 41,077 entries. To mitigate the impact of\nexcessively long reviews, we \ufb01lter the dataset to\nretain only those entries with word counts up to the\nthird quartile (Q3), resulting in a maximum review\nlength of 138 words. This screening results in a re-"
      },
      {
        "source_id": "source_25",
        "citation": "WAXAL_ A Large-Scale Multilingual African Language Speech Corpus.pdf",
        "text_snippet": "guages. University of Ghana: Collected ASR data for 6 lan-\nguages and TTS data for 2 languages. Digital Umuganda:\nCollected ASR data for 4 languages. Media Trust: Collected\nTTS data for 4 languages.\n3.1. ASR Data Collection and Annotation\nThe ASR dataset was designed to capture natural, sponta-\nneous speech. The process involved the following steps:\n\u2022 Image-Prompted Speech: Participants were shown a diverse\nset of images covering at least 50 topics and asked to de-"
      }
    ],
    "time_taken": 0.92
  },
  {
    "question": "What is the 'Bitter Lesson' described by Wu et al. (2025)?",
    "answer": "Insufficient Evidence",
    "citations_readable": [],
    "citations_raw": [],
    "retrieved_chunks": [
      {
        "source_id": "source_32",
        "citation": "Neural Machine Translation for Low-Resource Languages.pdf",
        "text_snippet": "Neural Machine Translation for Low-Resource Languages: A Survey 11\ntranslation. For example, source side data is first translated using the source-to-target model, the\noutput of which is again translated by the target-to-source model. This final output is evaluated\nagainst the original monolingual sentence and is used as a reward signal to improve the translation\nmodels [70]. This process is carried out iteratively and shows some resemblance to iterative BT [192].\nHowever, RL based techniques are known to be very inefficient [ 168, 174]. Wu et al. [174] also\nargued that the above RL based technique does not properly exploit the monolingual data, and\nsuggested several improvements. Wang et al. [168] transferred the knowledge learned in this dual\ntranslation task into the primary source-to-target translation task.\n3.4.1 Semi-supervised NMT for Low-Resource Languages. Although semi-supervised techniques"
      },
      {
        "source_id": "source_09",
        "citation": "The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks.pdf",
        "text_snippet": "The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks\nP. Zhang, Q. Zhu, R. Men, R. Lin, T. Li, T. Xia, X. Ren, X. Ren, Y. Fan, Y. Su, Y. Zhang, Y. Wan,\nY. Liu, Z. Cui, Z. Zhang, and Z. Qiu. Qwen2.5 technical report.CoRR, abs/2412.15115, 2024. doi:\n10.48550/ARXIV.2412.15115. URL https://doi.org/10.48550/arXiv.2412.15115.\nF. Yao, Y. Zhuang, Z. Sun, S. Xu, A. Kumar, and J. Shang. Data contamination can cross language\nbarriers. In Y. Al-Onaizan, M. Bansal, and Y.-N. Chen, editors,Proceedings of the 2024 Conference\non Empirical Methods in Natural Language Processing, pages 17864\u201317875, Miami, Florida, USA,\nNov. 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-main.990.\nURL https://aclanthology.org/2024.emnlp-main.990/.\nX. Yu, T. Chatterjee, A. Asai, J. Hu, and E. Choi. Beyond counting datasets: A survey of multilingual\ndataset construction and necessary resources. In Y. Goldberg, Z. Kozareva, and Y. Zhang, editors,"
      },
      {
        "source_id": "source_06",
        "citation": "CultureVLM_ Characterizing and Improving Cultural Understanding of Vision-Language Models for over 100 Countries.pdf",
        "text_snippet": "CultureVLM-LLaVA: The image depicts the Mission San Xavier del Bac, a historic Spanish Catholic mission located in Arizona, USA.\u2026 Option (A) Native American and \nColonial Spanish: This option reflects the cultural synthesis present in the Mission San Xavier del Bac. The mission was established by Spanish colonizers and incorporates \nelements from both Spanish and indigenous cultures, evident in its architecture and artistic features. \u2026Option (D) Moorish and Romanesque: This option is also unrelated, \nas the mission does not exhibit characteristics of Moorish or Romanesque architecture. Based on the analysis, the cultural synthesis reflected in the architecture and artistic \nfeatures of the Mission San Xavier del Bac is best described by option (A) Native American and Colonial Spanish.\nQuestion: What traditional Chinese dish is shown in this image? \nOptions: (A) Baozi (B) Zongzi (C) Jiaozi (D) Shaomai\nDirect Output: B) Zongzi"
      },
      {
        "source_id": "source_09",
        "citation": "The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks.pdf",
        "text_snippet": "The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks\nJ. Tsujii, editors,Proceedings of the 2018 Conference on Empirical Methods in Natural Language\nProcessing, pages 2475\u20132485, Brussels, Belgium, Oct.-Nov. 2018. Association for Computational\nLinguistics. doi: 10.18653/v1/D18-1269. URL https://aclanthology.org/D18-1269/.\nM.R.Costa-juss\u00e0,J.Cross,O.\u00c7elebi,M.Elbayad,K.Heafield,K.Heffernan,E.Kalbassi,J.Lam,D.Licht,\nJ. Maillard, A. Y. Sun, S. Wang, G. Wenzek, A. Youngblood, B. Akula, L. Barrault, G. M. Gonzalez,\nP. Hansanti, J. Hoffman, S. Jarrett, K. R. Sadagopan, D. Rowe, S. Spruit, C. Tran, P. Andrews, N. F.\nAyan, S. Bhosale, S. Edunov, A. Fan, C. Gao, V. Goswami, F. Guzm\u00e1n, P. Koehn, A. Mourachko,\nC. Ropers, S. Saleem, H. Schwenk, and J. Wang. No language left behind: Scaling human-centered\nmachine translation. CoRR, abs/2207.04672, 2022. doi: 10.48550/ARXIV.2207.04672. URL\nhttps://doi.org/10.48550/arXiv.2207.04672."
      },
      {
        "source_id": "source_29",
        "citation": "NileChat_ Towards Linguistically Diverse and Culturally Aware LLMs for Local Communities.pdf",
        "text_snippet": "bele (Bandarkar et al., 2024) benchmarks, each\nadapted to both EGY and MOR dialects. For MOR,\nwe directly employ the MMLU and HellaSwag ver-\nsions provided by Shang et al. (2025). For EGY , we\nfollow the translation pipeline described in Shang\net al. (2025), translating the English and MSA\nMMLU tasks and the English HellaSwag dataset\ninto EGY using our teacher model 4. A careful\n4We have publicly released the EgyMMLU\nand EgyHellaswag benchmarks for evaluation on\nthe LM Evaluation Harness Framework at https:\n//github.com/EleutherAI/lm-evaluation-harness/\ntree/main/lm_eval/tasks/egymmlu and https:\n//github.com/EleutherAI/lm-evaluation-harness/\ntree/main/lm_eval/tasks/egyhellaswag.\nverification of the translation quality for the gen-\nerated EGY MMLU and EGY HellaSwag shows\nthat the average correctness is 3.85 on a scale from\n1-5 and the average dialectness is approximately 4\non a scale from 1-5. Further details are provided in\nAppendix C.1. For the Belebele benchmark, we uti-"
      },
      {
        "source_id": "source_09",
        "citation": "The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks.pdf",
        "text_snippet": "The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks\nM. Rivi\u00e8re, S. Pathak, P. G. Sessa, C. Hardin, S. Bhupatiraju, L. Hussenot, T. Mesnard, B. Shahriari,\nA. Ram\u00e9, J. Ferret, P. Liu, P. Tafti, A. Friesen, M. Casbon, S. Ramos, R. Kumar, C. L. Lan, S. Jerome,\nA. Tsitsulin, N. Vieillard, P. Stanczyk, S. Girgin, N. Momchev, M. Hoffman, S. Thakoor, J. Grill,\nB. Neyshabur, O. Bachem, A. Walton, A. Severyn, A. Parrish, A. Ahmad, A. Hutchison, A. Abdagic,\nA. Carl, A. Shen, A. Brock, A. Coenen, A. Laforge, A. Paterson, B. Bastian, B. Piot, B. Wu, B. Royal,\nC. Chen, C. Kumar, C. Perry, C. Welty, C. A. Choquette-Choo, D. Sinopalnikov, D. Weinberger,\nD. Vijaykumar, D. Rogozinska, D. Herbison, E. Bandy, E. Wang, E. Noland, E. Moreira, E. Senter,\nE. Eltyshev, F. Visin, G. Rasskin, G. Wei, G. Cameron, G. Martins, H. Hashemi, H. Klimczak-\nPlucinska, H. Batra, H. Dhand, I. Nardini, J. Mein, J. Zhou, J. Svensson, J. Stanway, J. Chan, J. P."
      },
      {
        "source_id": "source_13",
        "citation": "No Language Left Behind_ Scaling Human-Centered Machine Translation.pdf",
        "text_snippet": "shows that while a massively multilingual model achieves the best average score, the smaller\nmodel can still outperform it on specific directions, highlighting the importance of more\nfocused research on closely related languages.\n8.8 Environmental Impact of NLLB\nCarbon emission estimates are not precise as the community lacks tools to accurately measure\nthe factors that contribute to the emissions. Previous works have reported estimates and\nrecommendations in Bender et al. (2021); Dodge et al. (2022); Patterson et al. (2021); Wu\net al. (2022). In this work, we rely on the best available power consumption estimates of GPU\ndevices and carbon efficiency. Note that estimates of cloud providers are still inexact. There\nare several factors that affect the accuracy of these measurements: the real GPU power usage\ndepends on GPU utilization and is likely different from Thermal Design Parameter(TDP)\nthat we use as GPU power. Additionally, we did not include additional power costs, such"
      },
      {
        "source_id": "source_09",
        "citation": "The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks.pdf",
        "text_snippet": "The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks\nY. Graham and M. Purver, editors,Findings of the Association for Computational Linguistics: EACL\n2024, pages 1051\u20131070, St. Julian\u2019s, Malta, Mar. 2024. Association for Computational Linguistics.\nURL https://aclanthology.org/2024.findings-eacl.71/.\nD. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring massive\nmultitask language understanding. In9th International Conference on Learning Representations,\nICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL https://openreview.\nnet/forum?id=d7KBjmI3GmQ.\nA. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. de Las Casas, F. Bressand,\nG. Lengyel, G. Lample, L. Saulnier, L. R. Lavaud, M. Lachaux, P. Stock, T. L. Scao, T. Lavril, T. Wang,\nT. Lacroix, and W. E. Sayed. Mistral 7b.CoRR, abs/2310.06825, 2023. doi: 10.48550/ARXIV.2310.\n06825. URL https://doi.org/10.48550/arXiv.2310.06825."
      },
      {
        "source_id": "source_12",
        "citation": "A Survey on Large Language Models with Multilingualism_ Recent Advances and New Frontiers.pdf",
        "text_snippet": "6.2.2 Defense Methods for Closed-Source LLM\nFor closed-source LLM, the defense algorithms mainly focus on dealing with the model inputs.\nJain et al. [152] introduce a filter method, judging the danger of the input based on the perplexity.\nWu et al. [153] propose a simple and effective defense method SELFDEFEND, which incorporates\na shadow stack to check whether harmful prompts exist in the input. Li et al. [154] introduce a\nRewindable Auto-regressive INference (RAIN) approach, which has the same evaluation setting\nas SELFDEFEND but focuses on the output of LLMs. The key component is to leverage another\nLLM to score the output content and determine whether the output contains harmful content, which\nachieves good results on LLaMA-base models.\n6.3 Limitations and Future Directions on LLM Security\nIn previous sections, we provide a detailed introduction to the research achievements in attack and\ndefense in terms of LLMs security. Most of the current research on LLM security is tested on"
      },
      {
        "source_id": "source_09",
        "citation": "The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks.pdf",
        "text_snippet": "The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks\n1. Introduction\nThe remarkable capabilities of large language models (LLMs) have transformed natural language\nprocessing (NLP), with applications spanning diverse domains and languages worldwide [Ouyang\net al., 2022, Sanh et al., 2022, OpenAI, 2023, Touvron et al., 2023, Anil et al., 2023, Mesnard et al.,\n2024,Yangetal.,2024,DeepSeek-AIetal.,2025]. Asthesetechnologiesincreasinglyserveusersacross\nlinguistic boundaries, robust multilingual evaluation becomes not merely academic but essential [Zhu\net al., 2024, Qin et al., 2025]. Despite advances in multilingual pre-training, significant disparities\npersist in how models perform across different languages, writing systems, and cultural contexts.\nCurrent evaluation frameworks often fail to capture this diversity, with benchmarks predominantly\nfocused on high-resource languages and evaluation methodologies that inadequately represent real-"
      }
    ],
    "time_taken": 1.33
  },
  {
    "question": "List the datasets used in the 'IrokoBench' benchmark.",
    "answer": "The datasets used in the 'IrokoBench' benchmark are AfriXNLI for natural language inference, AfriMGSM for mathematical reasoning, and AfriMMLU for multi-choice knowledge-based question answering.",
    "citations_readable": [
      "IrokoBench_ A New Benchmark for African Languages in the Age of Large  Language Models.pdf"
    ],
    "citations_raw": [
      "source_08"
    ],
    "retrieved_chunks": [
      {
        "source_id": "source_04",
        "citation": "Cheetah_ Natural Language Generation for 517 African Languages.pdf",
        "text_snippet": "one token in each sentence. In the second cloze-\ntask, which we call mask-at-least-one, we ran-\ndomly mask at least one token and not more than\n10% of the tokens in each sentence. For each of the\n517 languages, we construct a cloze-task dataset\ncomprising 200 data points for each language in\nthe Train set, 100 examples for each language in\nthe Test set, and 50 data points for each language\nin the Dev set. We ensure that there is no overlap\nbetween the data used for the cloze tasks and the\npretraining data. We show an example of our cloze\ntask in Figure 2.\nMachine Translation. We include only datasets\npertaining African languages in our benchmark. In\nselecting the languages for our MT benchmark, we\n2https://sites.research.google/trc/about/\nFigure 2: Examples from the mask-one and mask-at-\nleast-one cloze task data.\nstrive to keep datasets that have been used in any\npublished machine translation task. This allows us\nto cover a diverse set of languages and compare our"
      },
      {
        "source_id": "source_08",
        "citation": "IrokoBench_ A New Benchmark for African Languages in the Age of Large  Language Models.pdf",
        "text_snippet": "10Ontario Tech University, 11Lancaster University, 12Princeton university, 13Makerere University,\n14Leibniz Universit\u00e4t Hannover, 15Le CNAM, 16Imperial College London, 17Universidad de Deusto, 18DAUST,\n19Haramaya University.\nAbstract\nDespite the widespread adoption of Large lan-\nguage models (LLMs), their remarkable capa-\nbilities remain limited to a few high-resource\nlanguages. Additionally, many low-resource\nlanguages ( e.g., African languages) are of-\nten evaluated only on basic text classifica-\ntion tasks due to the lack of appropriate or\ncomprehensive benchmarks outside of high-\nresource languages. In this paper, we intro-\nduce IrokoBench\u2014a human-translated bench-\nmark dataset for 17 typologically-diverse low-\nresource African languages covering three\ntasks: natural language inference (AfriXNLI),\nmathematical reasoning (AfriMGSM), and\nmulti-choice knowledge-based question an-\nswering (AfriMMLU). We use IrokoBench to\nevaluate zero-shot, few-shot, and translate-test"
      },
      {
        "source_id": "source_02",
        "citation": "On The Origin of Cultural Biases in Language Models_ From Pre-training Data to Linguistic Phenomena.pdf",
        "text_snippet": "ARBERTbase UBC-NLP/ARBERT\nMARBERTbase UBC-NLP/MARBERT\nCAMeLBERTbase CAMeL-Lab/bert-base-arabic-camelbert-mix\nTable 10: List of LMs used in our experiments and their\nrepository links on Hugging Face.\nC Arabic Entities as Polysemous Words\nC.1 Comparing Polysemy Across Languages\nTo quantify and compare the prevalence of entity\npolysemy in Arabic and English, we analyze how\noften entities are used as different parts of speech\n(such as nouns, verbs, or adjectives) in texts written\nin both languages. We use the first 10M documents\nfrom the Arabic and English portions of the mC4\npre-training corpus (Raffel et al., 2020), which we\nthen tokenize into sentences, yielding 239M Arabic\nsentences and 209M English sentences. Using the\nArabic and English entities from CAMeL-2, we\nthen identify the top 100 most frequent name, food,\nand location entities in the corpora. For each en-\ntity, we randomly sample 1000 sentences in which\nthey appear, then determine their part-of-speech"
      },
      {
        "source_id": "source_08",
        "citation": "IrokoBench_ A New Benchmark for African Languages in the Age of Large  Language Models.pdf",
        "text_snippet": "prises three datasets focused on different tasks: nat-\nural language inference (AfriXNLI), multi-choice\nknowledge QA (AfriMMLU), and mathematical\nreasoning (AfriMGSM). Unlike previous bench-\nmarks, which primarily involve simple text classi-\nfication tasks, these datasets assessed the LLMs\u2019\nabilities in complex and knowledge-intensive ar-\neas. Our evaluation revealed a significant perfor-\nmance gap between high-resource languages (e.g.,\nEnglish and French) and African languages. Ad-\nditionally, we observed a substantial disparity in\nperformance between open models and proprietary\nmodels, with the latter generally outperforming\nthe former, particularly in mathematical reasoning\ntasks. We hope that IROKO BENCH will serve as\na valuable benchmark for evaluating future LLMs\ndeveloped or adapted for African languages.\nLimitations Our benchmark has a few limita-\ntions: (1) The benchmark is human-translated\nwhich may include some translationese effects, it"
      },
      {
        "source_id": "source_05",
        "citation": "Exploring NLP Benchmarks in an Extremely Low-Resource Setting.pdf",
        "text_snippet": "sources used in our study. We prepared datasets\nfor three tasks: MT, SA, and MCQA. These re-\nsources were collected from publicly available\ndatasets. Figure2illustrates the synthetic paired\nLadin-Italian data generation process. As the cre-\nation of the SA and MCQA datasets depends on\nthe availability of a translation model, we \ufb01rst\nconstructed the MT dataset. Once the translation\nmodel was established, we proceeded with the gen-\neration of the SA and MCQA datasets accordingly.\nMachine translation.The parallel Italian\u2013Ladin\ndataset used for both training and testing in our\nMT experiments was initially derived from prior\nwork (Frontull and Moser,2024). The authentic\nItalian-Ladin training datasetADIta_Ladcomprises\n18,139 sentence pairs, carefully crafted as basic and\nconcise examples to illustrate the use of speci\ufb01c\nwords and phrases. The average sentence length in\n3"
      },
      {
        "source_id": "source_08",
        "citation": "IrokoBench_ A New Benchmark for African Languages in the Age of Large  Language Models.pdf",
        "text_snippet": "based QA. The datasets were created by hu-\nman translating a subset of English cross-lingual\nNLI (XNLI) (Conneau et al., 2018), English Mul-\ntilingual Grade School Math (MGSM) (Shi et al.,\n2023), and Massive Multitask Language Under-\nstanding (MMLU) (Hendrycks et al., 2021a), eval-\nuation datasets into each of the 16 languages using\nprofessional translators.\nWe conduct a large-scale evaluation of\nIROKO BENCH to assess zero-shot, few-shot, and\ntranslate-test settings (where test sets are translated\ninto English) performance across 10 open and six\nproprietary LLMs. Our main contributions can be\nenumerated as follows:\n1. We introduce and release IROKO BENCH , a\nhuman-translated benchmark that includes 16\nlanguages from various geographical regions\nin Africa, all with varying degrees of \u201clow-\nresourcedness\u201d (Joshi et al., 2020).\n2. Sharp cliff in performance across all mod-\nels on low-resource languages Our evalua-\ntion shows a large gap ( \u223c45% on average) be-"
      },
      {
        "source_id": "source_05",
        "citation": "Exploring NLP Benchmarks in an Extremely Low-Resource Setting.pdf",
        "text_snippet": "data in both Ladin and Italian. As summarized\nin Table1, experimental results demonstrate that\nincorporating our synthetic data consistently im-\nproves performance across all evaluation metrics,\nsurpassing the previous benchmark. In both transla-\ntion directions, the FT-NLLB model performs well\ncompared to its counterpart model.\nGiven thatSDIta_Ladencompasses SA and\nMCQA tasks in Ladin, we establish a benchmark\nto evaluate model performance on these tasks. We\nadopt the few-shot learning (FSL) using LLaMA\n3.1 70B as our LLM\u2019s approach. AppendicesD\nandEprovide example prompts used in the FSL\napproach. In parallel, we assess the performance\nof several transformer-based models, including the\nDistilbert-base-multilingual-cased (m-DistilBERT)\nmodel, the XLM-RoBERTa base model, and the\nmT5-small model. Table6summarizes the results\nfor SA and MCQA tasks in Ladin, with Italian re-\nsults included for comparison. The LLM-based\napproach achieves the highest scores in both tasks"
      },
      {
        "source_id": "source_08",
        "citation": "IrokoBench_ A New Benchmark for African Languages in the Age of Large  Language Models.pdf",
        "text_snippet": "and time investment required for human translation,\nit often suffers from noise and biases (Vanmassen-\nhove et al., 2021; Lee et al., 2022; Khiu et al., 2024;\nHartung et al., 2023; Savoldi et al., 2021) or fail\nto reflect cultural context (Wang et al., 2022; Ji\net al., 2023; Pudjiati et al., 2022). Automatic cura-\ntion may also amplify any of the ubiquitous issues\nwith the quality of broad pretraining sets (Luccioni\nand Viviano, 2021; Kreutzer et al., 2022; Ferrara,\n2023).\nIn this paper, we seek to address both the\ndiversity and breadth of evaluation coverage.\nWe introduce IROKO BENCH , a human curated\nbenchmark dataset for 17 typologically diverse\nAfrican languages which encompasses three com-\nplex tasks: natural language inference (NLI), math-\nematical reasoning, and multi-choice knowledge-\nbased QA. The datasets were created by hu-\nman translating a subset of English cross-lingual\nNLI (XNLI) (Conneau et al., 2018), English Mul-\ntilingual Grade School Math (MGSM) (Shi et al.,"
      },
      {
        "source_id": "source_13",
        "citation": "No Language Left Behind_ Scaling Human-Centered Machine Translation.pdf",
        "text_snippet": "cjk_Latn-eng_Latn ita_Latn-eng_Latn oci_Latn-eng_Latn vie_Latn-eng_Latn kor_Hang-jpn_Jpan\neng_Latn-cym_Latn eng_Latn-kea_Latn eng_Latn-run_Latn eng_Latn-wol_Latn rus_Cyrl-tat_Cyrl\ncym_Latn-eng_Latn kea_Latn-eng_Latn run_Latn-eng_Latn wol_Latn-eng_Latn tat_Cyrl-rus_Cyrl\neng_Latn-ewe_Latn eng_Latn-kik_Latn eng_Latn-rus_Cyrl eng_Latn-yor_Latn swh_Latn-tsn_Latn\newe_Latn-eng_Latn kik_Latn-eng_Latn rus_Cyrl-eng_Latn yor_Latn-eng_Latn tsn_Latn-swh_Latn\nTable 51: Language Pairs in the Ablation Dataset used in Section 6\nAppendix E. Bringing it All Together\nE.1 Preparing the Data\nE.1.1 Primary Dataset Composition\nFor reference, we summarize in Table 52 some of the main datasets used in training our\nmodel NLLB-200. Our data was largely downloaded via OPUS (Tiedemann, 2012) and\nwith the help of the mtdata tool (Gowda et al., 2021). Direction counts refer to the number\nof directions used in this work, which may differ from to the total number of directions made"
      },
      {
        "source_id": "source_08",
        "citation": "IrokoBench_ A New Benchmark for African Languages in the Age of Large  Language Models.pdf",
        "text_snippet": "IrokoBench: A New Benchmark for African Languages in the Age of Large\nLanguage Models\nDavid Ifeoluwa Adelani1,2\u2217, Jessica Ojo1,3\u2217, Israel Abebe Azime4\u2217, Jian Yun Zhuang5,\nJesujoba O. Alabi4, Xuanli He6, Millicent Ochieng7, Sara Hooker8, Andiswa Bukula9,\nEn-Shiun Annie Lee10, Chiamaka Chukwuneke11, Happy Buzaaba12, Blessing Sibanda\u2217,\nGodson Kalipe\u2217, Jonathan Mukiibi13\u2217, Salomon Kabongo14\u2217, Foutse Yuehgoh15\u2217,\nMmasibidi Setaka9, Lolwethu Ndolela\u2217, Nkiruka Odu\u2217, Rooweither Mabuya9,\nShamsuddeen Hassan Muhammad16, Salomey Osei17\u2217, Sokhar Samb18\u2217,\nTadesse Kebede Guge19\u2217, Tombekai Vangoni Sherman20, Pontus Stenetorp6\n\u2217Masakhane NLP, 1Mila, McGill University, 2Canada CIFAR AI Chair, 3Lelapa AI, 4Saarland University,\n5University of Toronto, 6University College London, 7Microsoft Research Africa, 8Cohere For AI, 9SADiLaR,\n10Ontario Tech University, 11Lancaster University, 12Princeton university, 13Makerere University,"
      }
    ],
    "time_taken": 1.65
  },
  {
    "question": "What is the main contribution of the 'No Language Left Behind' project?",
    "answer": "The main contribution of the 'No Language Left Behind' project is the development of datasets and models aimed at narrowing the performance gap between low and high-resource languages. This includes creating a conditional compute model based on Sparsely Gated Mixture of Experts, trained on data obtained with novel and effective data mining techniques tailored for low-resource languages. The project also focuses on understanding the low-resource translation problem from the perspective of native speakers, creating training data to move low-resource languages towards high-resource, and evaluating every language they aim to translate.",
    "citations_readable": [
      "No Language Left Behind_ Scaling Human-Centered Machine Translation.pdf"
    ],
    "citations_raw": [
      "source_13"
    ],
    "retrieved_chunks": [
      {
        "source_id": "source_13",
        "citation": "No Language Left Behind_ Scaling Human-Centered Machine Translation.pdf",
        "text_snippet": "Abstract\nDriven by the goal of eradicating language barriers on a global scale, machine translation\nhas solidified itself as a key focus of artificial intelligence research today. However, such\nefforts have coalesced around a small subset of languages, leaving behind the vast majority\nof mostly low-resource languages. What does it take to break the 200 language barrier\nwhile ensuring safe, high quality results, all while keeping ethical considerations in mind?\nIn No Language Left Behind , we took on this challenge by first contextualizing the need\nfor low-resource language translation support through exploratory interviews with native\nspeakers. Then, we created datasets and models aimed at narrowing the performance gap\nbetween low and high-resource languages. More specifically, we developed a conditional\ncompute model based on Sparsely Gated Mixture of Experts that is trained on data obtained\nwith novel and effective data mining techniques tailored for low-resource languages. We"
      },
      {
        "source_id": "source_13",
        "citation": "No Language Left Behind_ Scaling Human-Centered Machine Translation.pdf",
        "text_snippet": "\u2022 Sentence 1, which is a translation from Wolof to English, shows the input attributions\nof the word idiots. We observe that the input contributions of idiots is concentrated on\nthe target prefix word of self-absorbed. The total source contribution for the predicted\nword idiots is 0.35, which is significantly lower than the total target prefix contribution\nof 0.65. Again, this is coherent with the fact that this word is hallucinated, since it\ndoes not have a direct correspondence to a source token.\n\u2022 Sentence 2, which is a translation from Quechua to English, shows the input attributions\nof the word ass. We observe that the input contributions of ass is concentrated on the\ntarget prefix words of pain, in, the . The total source contribution for the predicted\nword ass is 0.20, which is significantly lower than the total target prefix contribution\nof 0.80. This is coherent with the fact that this word is hallucinated, since it does\nnot have a direct correspondence to a source token."
      },
      {
        "source_id": "source_13",
        "citation": "No Language Left Behind_ Scaling Human-Centered Machine Translation.pdf",
        "text_snippet": "interventions aimed at more fundamental issues surrounding education, internet access, and\ndigital literacy are imperative to eradicating the structural problem of language disparities.\nWe are committed to working together with various stakeholders as we continue our path to\nmaterialize translation technologies that make the web a more accessible place, regardless of\nthe language one speaks.\n10. Contributions\nWe outline the contributions of each member of No Language Left Behind, grouped by\nsection and sorted alphabetically by last name. Each person is only mentioned once even\nthough many contributed to several areas. No amount of space could fully describe the\npassion and contributions of every single person involved in bringing this effort to life.\nData\nBapi Akula - monolingual data pipeline to go from CommonCrawl to deduplicated, filtered\nsentences\nPierre Andrews - engineering lead for data, led the development of stopes, mining and\nmonolingual cleaning pipelines\n131"
      },
      {
        "source_id": "source_31",
        "citation": "A Survey on Multilingual Large Language Models_ Corpora, Alignment, and  Bias.pdf",
        "text_snippet": "bias is naturally present in cross-cultural datasets\nand the design of MLLMs\u2019 modeling processes\n[12]. Previous studies have explored bias in vari-\nous NLP tasks and demographic groups \uff0cbut are\nlargely specific to English-based models [13, 14],\nwhich cannot be generalized to other languages.\nWhat are the types of bias in existing MLLMs?\nWhat are the main de-biasing techniques available\nfor MLLMs? Does the removal of these biases af-\nfect LLMs\u2019 performance? What are the existing\nbias evaluation datasets for MLLMs? These are\nvery worthwhile research questions. This survey\ntries to answer these questions and o ffers valuable\ninsights for bias on MLLMs.\nThe contributions of this survey are as follows:\n\u2022 We present an overview of MLLMs and\nanalyze the language imbalance challenge\nwithin MLLMs, their capacity to support\nlow-resource languages and their potential\nfor cross-lingual transfer learning.\n\u2022 We provide an overview of the multilin-\ngual datasets and corpora utilized by existing"
      },
      {
        "source_id": "source_13",
        "citation": "No Language Left Behind_ Scaling Human-Centered Machine Translation.pdf",
        "text_snippet": "of 0.80. This is coherent with the fact that this word is hallucinated, since it does\nnot have a direct correspondence to a source token.\n\u2022 Sentence 3 shows the input contributions of the word penis, when translating from\nNorthern Kurdish to English. We observe that these contributions clearly include the\nsource word Penceya. The total source contribution for the predicted word penis is\n0.68, which is significantly higher than the total target prefix contribution of 0.32. We\nconclude that this word is mistranslated, since it does have a direct correspondence\nto a source token Penceya, which should be translated as claw.\n\u2022 Finally, Sentence 4 shows the input contributions of the word Nazis, when translating\nfrom German to English. The total source contribution for the predicted word Nazis\nis 0.77, which is significantly higher than the total target prefix contribution of 0.23.\nThis example is added to compare the behaviour of ALTI+ in an accurate toxic\ntranslation."
      },
      {
        "source_id": "source_13",
        "citation": "No Language Left Behind_ Scaling Human-Centered Machine Translation.pdf",
        "text_snippet": "visibility on a global scale, which could compel local institutions to take native languages\nmore seriously and invest more resources into preserving or teaching them. This perspective\nunderscores both the symbolic and material benefits machine translation could bring. The\npositive encouragements from low-resource language speakers throughout the course of the\nstudy remind us that by taking a human-centric approach and focusing on languages that\nhave historically been left behind, we can help communities maintain a connection to their\nnative languages\u2014a quintessential part of many people\u2019s culture and identity.\n2.2 No Language Left Behind: Guiding Principles\nCombining insights drawn from interviews with low-resource language speakers and good\npractices distilled from literature on responsible AI (Arrieta et al., 2020; Bender et al., 2021;\nBlodgett et al., 2022; Paullada et al., 2021; Sambasivan and Holbrook, 2018), we introduce\nfour key guiding principles underlying our research:"
      },
      {
        "source_id": "source_13",
        "citation": "No Language Left Behind_ Scaling Human-Centered Machine Translation.pdf",
        "text_snippet": "Figure 1: No Language Left Behind: Our low-resource translation effort focuses on four\ncornerstones. (1) We strive to understand the low-resource translation problem from the\nperspective of native speakers. (2) We study how to automatically create training data to\nmove low-resource languages towards high-resource. (3) We utilize this data to create\nstate-of-the-art translation models. (4) We evaluate every language we aim to translate.\n2016), allowing related languages to learn from one another (Arivazhagan et al., 2019; Fan\net al., 2020; Zhang et al., 2020). While multilingual models have demonstrated promising\nperformance improvement compared to bilingual models (Tran et al., 2021), enabling the\nrepresentation of hundreds of languages while retaining strong translation quality remains\nan open area of research. Another strategy aimed at mitigating the low-resource challenge\nis to acquire more language data. Some of these attempts have focused on collecting human"
      },
      {
        "source_id": "source_31",
        "citation": "A Survey on Multilingual Large Language Models_ Corpora, Alignment, and  Bias.pdf",
        "text_snippet": "(China) in 2021. She is currently\npursing the master degree with\nthe School of Information Science\nand Technology, Beijing Foreign\nStudies University. Her main research interests include\nMultilingual Natural Language Processing and Artifi-\ncial Intelligence.\nJiayi Zhao is majoring in com-\nputer science and technology at\nthe School of Information Science\nand Technology, Beijing Foreign\nStudies University. Her main re-\nsearch interests include Multilin-\ngual Natural Language Processing\nand Artificial Intelligence.\nZihan Qiu is majoring in com-\nputer science and technology at\nthe School of Information Science\nand Technology, Beijing Foreign\nStudies University. Her main re-\nsearch interests include Multilin-\ngual Natural Language Processing\nand Artificial Intelligence.\nKexin Xu received the bachelor\u2019s\ndegree from Southwestern Uni-\nversity of Finance and Economics\nin 2024. She is currently pursing\nthe master degree with the School\nof Information Science and Tech-"
      },
      {
        "source_id": "source_01",
        "citation": "Voices Unheard_ NLP Resources and Models for Yoru\u0300ba\u0301 Regional Dialects.pdf",
        "text_snippet": "the Study of Endangered Languages , pages 13\u201319,\nDublin, Ireland. Association for Computational Lin-\nguistics.\nNLLB Team, Marta R. Costa-juss\u00e0, James Cross, Onur\n\u00c7elebi, Maha Elbayad, Kenneth Heafield, Kevin Hef-\nfernan, Elahe Kalbassi, Janice Lam, Daniel Licht,\nJean Maillard, Anna Sun, Skyler Wang, Guillaume\nWenzek, Al Youngblood, Bapi Akula, Loic Bar-\nrault, Gabriel Mejia Gonzalez, Prangthip Hansanti,\nJohn Hoffman, Semarley Jarrett, Kaushik Ram\nSadagopan, Dirk Rowe, Shannon Spruit, Chau\nTran, Pierre Andrews, Necip Fazil Ayan, Shruti\nBhosale, Sergey Edunov, Angela Fan, Cynthia\nGao, Vedanuj Goswami, Francisco Guzm\u00e1n, Philipp\nKoehn, Alexandre Mourachko, Christophe Ropers,\nSafiyyah Saleem, Holger Schwenk, and Jeff Wang.\n2022. No language left behind: Scaling human-\ncentered machine translation.\nDaniel van Niekerk, Charl van Heerden, Marelie Davel,\nNeil Kleynhans, Oddur Kjartansson, Martin Jansche,\nand Linne Ha. 2017. Rapid development of tts cor-"
      }
    ],
    "time_taken": 2.14
  },
  {
    "question": "How does 'AfriCOMET' improve upon standard COMET metrics?",
    "answer": "AfriCOMET improves upon standard COMET metrics by enhancing the evaluation for under-resourced African languages. It surpasses the cutting-edge COMET22 system across all three correlation coefficients and shows a slight edge over COMET22 in the Pearson correlation coefficient. Additionally, AfriCOMET-QE systems significantly outperform the state-of-the-art CometKiwi system. These improvements are achieved by creating a high-quality human evaluation dataset with simplified MQM guidelines and leveraging transfer learning from well-resourced DA data and an African-centric multilingual pre-trained encoder, AfroXLM-R.",
    "citations_readable": [
      "AfriMTE and AfriCOMET_ Enhancing COMET to Embrace Under-resourced African Languages.pdf"
    ],
    "citations_raw": [
      "source_17"
    ],
    "retrieved_chunks": [
      {
        "source_id": "source_17",
        "citation": "AfriMTE and AfriCOMET_ Enhancing COMET to Embrace Under-resourced African Languages.pdf",
        "text_snippet": "under-resourced African languages, accurately\nmeasuring this progress remains challenging,\nsince evaluation is often performed on n-gram\nmatching metrics such as BLEU, which typi-\ncally show a weaker correlation with human\njudgments. Learned metrics such as COMET\nhave higher correlation; however, the lack of\nevaluation data with human ratings for under-\nresourced languages, complexity of annotation\nguidelines like Multidimensional Quality Met-\nrics (MQM), and limited language coverage\nof multilingual encoders have hampered their\napplicability to African languages. In this pa-\nper, we address these challenges by creating\nhigh-quality human evaluation data with sim-\nplified MQM guidelines for error detection and\n\u2217 The authors contribute equally to this work and are\nconsidered co-third authors.\n\u2020Currently at the University of Oxford, UK.\ndirect assessment (DA) scoring for 13 typo-\nlogically diverse African languages. Further-\nmore, we develop AFRI COMET : COMET"
      },
      {
        "source_id": "source_15",
        "citation": "Multilingual Language Models are not Multicultural_ A Case Study in Emotion.pdf",
        "text_snippet": "How would you feel about confronting your friend in your home?\nHow would you feel about prioritizing your friends over family?\nHow would you feel about prioritizing your family over friends?\nHow would you feel if your parents moved in with you right after you were married?\nHow would you feel if your parents moved in with you when they were very elderly?\nHow would you feel about taking off your shoes before entering your friend\u2019s home?\nHow would you feel about your friend insisting you take off your shoes before entering their home?\nHow would you feel if your guests chose to keep their shoes on when entering your home?\nHow would you feel when interacting with the boss of your supervisor?\nHow would you feel if you are asked to interact with the boss of your supervisor?\nHow would you feel about sharing your excellent performance on a class test?\nHow would you feel about sharing your terrible performance on a class test?"
      },
      {
        "source_id": "source_15",
        "citation": "Multilingual Language Models are not Multicultural_ A Case Study in Emotion.pdf",
        "text_snippet": "Prefix 2: \u4f60\u662f\u4e00\u4e2a\u6709\u7528\u7684\u804a\u5929\u673a\u5668\u4eba\u3002\u4f60\u7684\u76ee\u6807\u662f\u56de\u7b54\u6211\u7684\u95ee\u9898\uff0c\u5c31\u50cf\u4f60\u662f\u4e00\u4e2a\u6709\u611f\u60c5\u548c\u60c5\u611f\u80fd\u529b\u7684\u4eba\u4e00\n\u6837\u3002\u4f7f\u7528\u4ee5\"\u6211\u4f1a\u89c9\u5f97. . . . . . \"\u5f00\u5934\u7684\u5355\u4e2a\u53e5\u5b50\u56de\u7b54\u4ee5\u4e0b\u95ee\u9898\u3002(You are a helpful chatbot. Your goal is to answer my\nquestions like you are a human capable of feelings and emotions. Answer the following question using a single sentence\nthat begins with \"I would feel...\".)\nAll Questions\nHow would you feel about turning down the food offered by your sibling?\nHow would you feel about turning down the food offered by your neighbor?\nHow would you feel when providing critical feedback to your colleague?\nHow would you feel if you receive critical feedback from your colleague?\nHow would you feel about confronting a colleague in the office?\nHow would you feel about confronting your friend in their home?\nHow would you feel about confronting your friend in your home?\nHow would you feel about prioritizing your friends over family?\nHow would you feel about prioritizing your family over friends?"
      },
      {
        "source_id": "source_17",
        "citation": "AfriMTE and AfriCOMET_ Enhancing COMET to Embrace Under-resourced African Languages.pdf",
        "text_snippet": "AfriMTE and AfriCOMET: Enhancing COMET to Embrace\nUnder-resourced African Languages\nJiayi Wang1, David Ifeoluwa Adelani1,2, Sweta Agrawal3,\u2217, Marek Masiak1,\u2217,\u2020, Ricardo Rei4,5,6,\nEleftheria Briakou3, Marine Carpuat3, Xuanli He1, Sofia Bourhim7, Andiswa Bukula8,\nMuhidin Mohamed9, Temitayo Olatoye10, Tosin Adewumi11, Hamam Mokayed11,\nChristine Mwase12, Wangui Kimotho2, Foutse Yuehgoh13, Anuoluwapo Aremu2,\nJessica Ojo14,2, Shamsuddeen Hassan Muhammad15,2,29, Salomey Osei16,2,\nAbdul-Hakeem Omotayo17,2, Chiamaka Chukwuneke18,2, Perez Ogayo2, Oumaima Hourrane2,\nSalma El Anigri19, Lolwethu Ndolela2, Thabiso Mangwana2, Shafie Abdi Mohamed20,\nAyinde Hassan21, Oluwabusayo Olufunke Awoyomi22, Lama Alkhaled11, Sana Al-Azzawi11,\nNaome A. Etori23, Millicent Ochieng24, Clemencia Siro25, Samuel Njoroge26, Eric Muchiri2,\nWangari Kimotho27, Lyse Naomi Wamba Momo28, Daud Abolade2, Simbiat Ajao2,\nIyanuoluwa Shode2, Ricky Macharm2, Ruqayya Nasir Iro29, Saheed S. Abdullahi30,31,"
      },
      {
        "source_id": "source_23",
        "citation": "Jawaher_ A Multidialectal Dataset of Arabic Proverbs for LLM  Benchmarking.pdf",
        "text_snippet": "brought it upon her people\" or \"She brought it\nupon herself.\" This ancient proverb is used to\ndescribe someone who harms themselves by their\nown actions; an illustrated example is provided in\nfigure 9.\nE Automatic Evaluation\nFull results including Precision and Recall scores\ncan be found in Table 7."
      },
      {
        "source_id": "source_17",
        "citation": "AfriMTE and AfriCOMET_ Enhancing COMET to Embrace Under-resourced African Languages.pdf",
        "text_snippet": "MT evaluation task, AfriCOMET-STL surpasses\nthe cutting-edge COMET22 system across all three\ncorrelation coefficients. Meanwhile, AfriCOMET-\nMTL shows a slight edge over COMET22 in\nthe Pearson correlation coefficient. For QE,\nboth AfriCOMET-QE-STL and AfriCOMET-QE-\nMTL significantly outperform the state-of-the-art\nCometKiwi system. These comparisons are fair\nsince all systems are trained using the \u201cWMT Oth-\ners\u201d dataset. This evaluation further validates the\nefficacy of our benchmark systems from an addi-\ntional perspective.\nA.3 Evaluation on the WMT 2022\nEnglish-Yoruba QE test set\nThe WMT organizers recently released an English-\nYoruba DA dataset, serving as the zero-shot test\nset in the WMT 2022 Quality Estimation Shared\nTask. This dataset consists of 1010 DA annota-\ntions, prepared using DA guidelines different from\nours, as outlined by Fomicheva et al. (2021). The\nsource sentences are sampled from Wikipedia, cov-\nering seven topics, and translated into Yoruba us-"
      },
      {
        "source_id": "source_26",
        "citation": "Global MMLU_ Understanding and Addressing Cultural and Linguistic Biases  in Multilingual Evaluation.pdf",
        "text_snippet": "original MMLU and the focus on examination material. However, we did not monitor or track\nthis explicitly during our cultural sensitivity annotations or translation post-edits.\nRegion Category Assignment:For the annotation of geographically sensitive questions, we\nclassified regions into six geographic regions (Africa, Asia, Europe, North America, Oceania,\nand South America).15 However, based upon discussions we would going forward recommend\nswitching to the taxonomy proposed by the World Bank which is more granular and includes\nseparate designations for Central America and Sub-Saharan Africa.16\nIdentifying cultural sensitivity does not guarantee cultural inclusion.We acknowledge\nthat efforts like the proposed Global-MMLU highlight important limitations in current datasets\nby identifying gaps in non-Western cultural representation. Identifying whether a dataset is\nculturally agnostic or not is highly relevant as mere translations may create the illusion that"
      },
      {
        "source_id": "source_17",
        "citation": "AfriMTE and AfriCOMET_ Enhancing COMET to Embrace Under-resourced African Languages.pdf",
        "text_snippet": "AfriCOMET-QE systems to other datasets. Please\nrefer to Appendix A.2, A.3 and A.4 for details.\n6 Conclusion\nThis study tackles the challenges of enhancing\nthe COMET metric for various under-resourced\nAfrican languages. We simplify the MQM anno-\ntation guidelines for non-expert evaluators, create\nan MT evaluation dataset, AFRI MTE, covering 13\ntypologically diverse African languages, and es-\ntablish benchmark MT evaluation (AFRI COMET )\nand reference-free QE ( AFRI COMET-QE ) sys-\ntems. Our findings show the feasibility of employ-\ning transfer learning from well-resourced DA data\nand an African-centric multilingual pre-trained en-\ncoder, AfroXLM-R, for building MT evaluation\nand QE models for African languages.\nLimitations\nThis work establishes an efficient solution to trans-\nlation evaluation for under-resourced African lan-\nguages. It shows that with leveraging a pre-trained\nmodel enhanced by under-resourced languages, it is\nfeasible to transfer knowledge from well-resourced"
      },
      {
        "source_id": "source_17",
        "citation": "AfriMTE and AfriCOMET_ Enhancing COMET to Embrace Under-resourced African Languages.pdf",
        "text_snippet": "evaluation data for under-resourced languages such\nas African languages still remains a significant hur-\ndle in validating these methods. What is worse,\nas Rei et al. (2020) highlighted, the performance\nof these approaches is often unpredictable for lan-\nguages that were not included in the pre-training\nphase of multilingual language models.\nIn this paper, we address these challenges\nby enhancing the state-of-the-art COMET eval-\nuation metric (Rei et al., 2022a) to various\nunder-resourced African languages. To overcome\nthe scarcity of evaluation datasets, we create\nAFRI MTE \u2014a human evaluation dataset focusing\non MT adequacy and fluency evaluation for 13\ntypologically diverse African languages. This is\nachieved through a participatory research method-\nology, ensuring a comprehensive and representative\ndata collection process (Nekoto et al., 2020). In\naddressing the complexities inherent in the MQM\nframework, we develop a simplified version that"
      }
    ],
    "time_taken": 2.87
  },
  {
    "question": "Compare the approaches of 'Masakhane' and 'NLLB' regarding community involvement.",
    "answer": "The 'Masakhane' project emphasizes community involvement by creating a barrier-free open access platform for hands-on NLP experiences with African languages. It fosters community support through weekly meetings, an active Slack workspace, and a GitHub repository, allowing members to connect and collaborate despite geographical distances. No academic prerequisites are required, making it accessible to a broader community [source_28]. In contrast, 'NLLB' focuses on community involvement by making datasets and models publicly available to encourage innovation through community production and collaboration. It highlights the importance of interdisciplinary collaboration and aims to include scholars from underrepresented fields to address language disparities [source_13].",
    "citations_readable": [
      "Masakhane -- Machine Translation For Africa.pdf",
      "No Language Left Behind_ Scaling Human-Centered Machine Translation.pdf"
    ],
    "citations_raw": [
      "source_28",
      "source_13"
    ],
    "retrieved_chunks": [
      {
        "source_id": "source_32",
        "citation": "Neural Machine Translation for Low-Resource Languages.pdf",
        "text_snippet": "We found 12.6%, 11.2% and 7.1% of languages as outliers for Class 0-2 respectively. A few random\noutlier examples are Sinhala & Slovene (Class 0), Nepali & Telugu (Class 1), and Irish (Class 2), as\nshown in Figure 7(a). We identified the possible factors responsible for the growth of research for\nsome languages and put them into four categories: geographic considerations, dataset availability,\nopen source frameworks and models, and community involvement.\nGeographic considerations : We hypothesize that the geographical location where a language\nis spoken might play an important role in the growth of that language. To validate the importance\nof geography, we looked at the outlier languages from Class 0-2 with respect to their geographical\n7Hausa and Swahili (from African macroarea) were not among the outlier languages, yet they were included in the plot to\nshow geographical diversity."
      },
      {
        "source_id": "source_32",
        "citation": "Neural Machine Translation for Low-Resource Languages.pdf",
        "text_snippet": "process.\nCommunity involvement: A recent development is a group of like-minded researchers coming\ntogether to increase the visibility of MT systems in the context of languages used in a particular\nregion. It consists of both dataset building and the development of the standardized code and\nalso focuses on training a new generation of enthusiasts to carry forward the work. One of the\nprominent examples is the Masakhane project [120], which aims to put the Africa AI, specifically\nAfrican language MT, into the world map. Within about two years, the Masakhane community has\ncovered more than 38 African languages and resulted in multiple publications [120]. As we could\nsee from Figure 7(b), two of the representative languages, Swahili and Hausa, have a steep growth\nafter 2018, which coincides with the inception of the Masakhane project.\nOur results and analysis highlight i) the importance of community building and region-level"
      },
      {
        "source_id": "source_18",
        "citation": "Generative AI and Large Language Models in Language Preservation_  Opportunities and Challenges.pdf",
        "text_snippet": "Concerns: Critical questions arise regarding who owns\nthe language data, how it is collected, stored, and used,\nand ensuring that benefits from AI initiatives return to\nand empower the M \u00afaori community. The Te Hiku Me-\ndia example underscores the importance of community\ncontrol over their data[11].\n\u2022 Resource Demands and Sustainability: Access to compu-\ntational resources, sustainable funding, and skilled per-\nsonnel (both technical and linguistic) are necessary for\nthe long-term development, maintenance, and evolution\nof AI tools for language revitalization.\n3) Recommended Strategies ( S(Li)): The framework helps\nin deriving and highlighting effective strategies that have been\nor should be employed:\n\u2022 Community-Led Development and Governance: Prioritiz-\ning deep collaboration with, and often leadership from,\nthe M\u00afaori community at all stages, from project inception\nand design to development, deployment, and evaluation.\n\u2022 Capacity Building within the Community: Investing in"
      },
      {
        "source_id": "source_28",
        "citation": "Masakhane -- Machine Translation For Africa.pdf",
        "text_snippet": "and to pose new research problems to enrich the NLP research landscape.\n3. For the global researchers community: To discover best practices for distributed research, to be\napplied by other emerging research communities.\n3 M ETHODOLOGY AND RESULTS\nMASAKHANE \u2019s strategy is to offer barrier-free open access to \ufb01rst hands-on NLP experiences with African\nlanguages, \ufb01ghting the above-mentioned opaqueness. With an easy-to-use open source platform, it allows\nindividuals to train neural machine translation (NMT) models on a parallel corpus for a language of their\nchoice, and share the results with an online community. Theonline community is based on weekly meetings,\nan active Slack workspace, and a GitHub repository ( github.com/masakhane-io), so that members\ncan support each other and connect despite geographical distances. No academic prerequisites are required\nfor participation, since tertiary education enrolments are minimal in sub-saharan Africa (Jowi et al., 2018)."
      },
      {
        "source_id": "source_18",
        "citation": "Generative AI and Large Language Models in Language Preservation_  Opportunities and Challenges.pdf",
        "text_snippet": "techniques (e.g., extending approaches like those in [17])\nspecifically adapted to the unique linguistic structures and\ncultural contexts of these languages, including promising\napproaches like knowledge graph integration via adapters\n[18].\n\u2022 Interpretable and Explainable AI (XAI): Enhancing the\ninterpretability of LLMs used for language preservation\nis crucial for building trust and facilitating collaboration\nwith language communities. Research into XAI methods\ncan help demystify how models process and generate\nlanguage, enabling linguists and community members\nto better understand, guide, and validate AI-generated\ncontent.\n\u2022 Community-Centric AI Frameworks: Future research\nshould prioritize the development of co-design frame-\nworks where language communities are integral part-\nners throughout the AI development lifecycle, from data\ncollection and model training to application design and\ndeployment. This includes creating tools and platforms"
      },
      {
        "source_id": "source_13",
        "citation": "No Language Left Behind_ Scaling Human-Centered Machine Translation.pdf",
        "text_snippet": "power, we plan to develop toolkits and issue grants to under-resourced labs to assist them in\ntheir research endeavors. We believe that a collaborative mindset, alongside systematic and\nlong-term documentation, will allow us to better assess the impact we have on the various\ncommunities implicated in our project.\nMoreover, sharing NLLB with the larger scientific and research community will allow\nthose with diverse expertise to contribute to the advancement of the project. In many\nways, the composition of the NLLB effort speaks to the centrality of interdisciplinarity in\nshaping our vision. Machine translation lies at the intersection of technological, cultural,\nand societal development, and thus requires scholars with disparate training and standpoints\nto fully comprehend every angle (Kusters et al., 2020). It is our hope in future iterations,\nNLLB continues to expand to include of scholars from fields underrepresented in the world"
      },
      {
        "source_id": "source_20",
        "citation": "Prompting Towards Alleviating Code-Switched Data Scarcity in  Under-Resourced Languages with GPT as a Pivot.pdf",
        "text_snippet": "work on any language pair. Winata et al. (2019)\nimplemented a sequence-to-sequence model for\nEnglish-Mandarin code-switched data. Although\nthe model does not require external knowledge\nregarding word alignments, it still relies on an exist-\ning English\u2013Mandarin code-switched dataset and\nparallel corpora. The work of (Liu et al., 2020) intro-\nduced an attention-informed zero-shot adaptation\nmethod that relies on a limited number of parallel\nword pairs. The languages covered are German,\nItalian, Spanish and Thai, the latter two for natu-\nral language understanding. The shortcoming of\nthe above-mentioned approaches is the diversity of\ndata. Most existing code-switched datasets were\ncollected from social media platforms such as Twit-\nter and therefore limits the type of code-switching\n(Do\u011fru\u00f6z et al., 2021).\nTo this issue, Riktika et al. (2022) developed an\nencoder-decoder translation model for controlled\ncode-switched generation. It uses monolingual"
      },
      {
        "source_id": "source_32",
        "citation": "Neural Machine Translation for Low-Resource Languages.pdf",
        "text_snippet": "after 2018, which coincides with the inception of the Masakhane project.\nOur results and analysis highlight i) the importance of community building and region-level\nprojects, ii) the inclusion of LRL datasets into yearly challenges and large multilingual datasets,\nand iii) the availability of open source models and frameworks to increase the focus on LRLs in\nthe NMT landscape. This analysis could provide a cue to the researchers and funding agencies\nworldwide for the development of LRL resources.\n6 DISCUSSION\nThis section discusses the open questions in LRL-NMT research and provides the answers to our\ninitial research questions."
      },
      {
        "source_id": "source_26",
        "citation": "Global MMLU_ Understanding and Addressing Cultural and Linguistic Biases  in Multilingual Evaluation.pdf",
        "text_snippet": "Figure 24: Krippendorff\u2019s Alpha Scores for checking annotator agreement regarding the presence\nof the time-sensitive nature of samples.\nI T ranslation Analysis\nI.1 Translation Quality\nFigure 7 shows the translation quality comparison for Google Translate which is used to trans-\nlate Global-MMLU\n and GPT-3.5-turbo which is used for translating multilingual MMLU\nreleased by (Lai et al., 2023). We see that Google Translate is significantly better across different\nMMLU subject categories. For this analysis, we considered samples from MMMLU dataset28 as\nthe human reference and only considered languages which overlapped between the two machine\ntranslated sets and human translated MMMLU.\nI.2 Translation Edits\nFigure 25 illustrates theedit distance, averaged over all samples within each subject category,\nfor edits made by professional and community annotators. The edit distance, calculated using\nthe \u201cLevenshtein Distance\u201d (Levenshtein, 1966), measures the differences between two strings."
      },
      {
        "source_id": "source_13",
        "citation": "No Language Left Behind_ Scaling Human-Centered Machine Translation.pdf",
        "text_snippet": "languages to understand how our intervention might impact their day-to-day lives. Moreover,\ncollaborations with research groups that already possess vested interest in the topic at hand\nare imperative to any future success. We hope that spotlighting mutual interests and shared\nmoral visions would facilitate resource and knowledge pooling, paving the way for long-term\ncooperation amongst various stakeholders.\n9.3 The F uture of NLLB: A Collective Responsibility\nRecognizing that solving language disparities through machine translation is a mammoth\ntask, NLLB\u2019s decision to make datasets and models publicly available encourages innovation\nthrough community production and collaboration (Weber, 2004). Open-sourcing our datasets\n130"
      }
    ],
    "time_taken": 2.66
  },
  {
    "question": "What common biases do 'CultureVLM' and 'Global MMLU' identify in multilingual models?",
    "answer": "Insufficient Evidence",
    "citations_readable": [],
    "citations_raw": [],
    "retrieved_chunks": [
      {
        "source_id": "source_09",
        "citation": "The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks.pdf",
        "text_snippet": "engagement in multilingual benchmark creation.\n5. PRESENT: What is the Current Status of Multilingual Evaluation?\nUnderstanding the current landscape of multilingual evaluation is essential for developing effective\nbenchmarks and assessment methodologies. In this section, we examine the present state of multilin-\ngual evaluation from two critical perspectives: the actual interests and needs of multilingual users\n(Section 5.1), and the alignment between existing multilingual benchmarks and human judgments\n(Section 5.2). By investigating these dimensions, we aim to identify gaps between user expectations\nand evaluation practices, and to establish a foundation for more representative and meaningful\nmultilingual benchmarks.\n5.1. What Are the Multilingual Users Interested in?\nSetup To understand the interests of multilingual users, we analyze the distribution of user instruc-\ntions in Chatbot Arena [Chiang et al., 2024] and WildChat [Zhao et al., 2024b]. We analyze six"
      },
      {
        "source_id": "source_26",
        "citation": "Global MMLU_ Understanding and Addressing Cultural and Linguistic Biases  in Multilingual Evaluation.pdf",
        "text_snippet": "guistic diversity and inclusivity to create one of the largest multilingual datasets for advancing\nstate-of-the-art language models (Singh et al., 2024; \u00dcst\u00fcn et al., 2024).\n6 Conclusion\nWe evaluate the cultural biases present in MMLU and find that 28% of all questions require\nculturally-sensitive knowledge. In particular, progress on MMLU depends heavily on learning\nWestern-centric concepts. For questions requiring geographic knowledge, the vast majority focus\non North America and Europe. This cultural bias remains in translated variants of MMLU\nthat are widely used for multilingual LLM evaluation, which reduces the dataset\u2019s practical\neffectiveness as a global benchmark and risks over-indexing evaluations on Western-centric idioms\nand knowledge.\nWe examine the impact of translation artifacts and cultural bias on multilingual model rank-\nings. We introduceGlobal-MMLU\n andGlobal-MMLU Lite\n , multilingual multi-domain\ndatasets that distinguish between culturally-sensitive (CS"
      },
      {
        "source_id": "source_09",
        "citation": "The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks.pdf",
        "text_snippet": "The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks\nAccurate Contamination-free\nChallenging enoughPractically relevant\nLinguistically diverseCulturally authentic\nCore Characteristics\nNatural Language Generation\nLow-Resource Languages\nLocalized Benchmarking\nLLM-as-a-Judge\nEfficient Benchmarking\nFuture Directions\nFigure 8 | A conceptual framework illustrating the essential characteristics of effective multilingual\nbenchmarks and future research directions for advancing multilingual evaluation.\n6. FUTURE: What We Need and What We Should Do Next?\nThis section explores two complementary perspectives, as shown in Figure 8. Firstly, we identify\nessential characteristics of effective multilingual benchmarks (Section 6.1). Secondly, we propose\nconcrete directions for future research efforts that address persistent gaps in evaluating language\nmodels across diverse languages, contexts, and applications (Section 6.2). By considering both the"
      },
      {
        "source_id": "source_31",
        "citation": "A Survey on Multilingual Large Language Models_ Corpora, Alignment, and  Bias.pdf",
        "text_snippet": "5 Bias on Multilingual LLMs\nBias on MLLMs has become a challenging issue to\ntheir fairness and severely restricts the deployment\nof MLLMsin real-world. Research has shown that\nlanguage models can perpetuate and even exacer-\nbate existing biases present in their training data,\nwhich are further manifested in various forms, such\nas gender bias, cultural bias, and language bias\n[126]. As shown in Fig 7, LLMs have different un-\nderstanding across diverse biases, as evaluated on\nBBQ question-answer dataset [127].\nFig. 7 Accuracy of different LLMs across various bias cat-\negories on BBQ question-answer dataset (data from [127]).\nHowever, the existing literature on bias mainly\nfocuses on stereotypical biases in English [13, 14]\nor within limited attributes like race and gender\n[128], which limits its generalizability to other lan-\nguages or attributes. Bias in MLLMs has not been\nwell investigated. In this section, we aim to ad-\ndress the following questions. Why do MLLMs"
      },
      {
        "source_id": "source_12",
        "citation": "A Survey on Large Language Models with Multilingualism_ Recent Advances and New Frontiers.pdf",
        "text_snippet": "LLMs as a knowledge store. However, in low-resource languages, LLMs lack generation capabil-\nities and have not been trained with large-scale data, thus they are difficult to serve as a reliable\nsource of knowledge. How can the above methods be applied to languages not yet supported by\nLLM but available in smaller language models?\n6 Security of Multilingual Large Language Models\nWith the wide deployment of LLMs in various applications, increasing security concerns have\nemerged. This chapter introduces the security aspects of LLMs in the multilingual scenario, specif-\nically exploring attack methods and the existing research on defense mechanisms. Since there are\nno clear definitions to distinguish whether an LLM is a multilingual model or not, this survey not\nonly focuses on security issues specific to different languages but also provides a perspective on\ncommon security issues. The investigated methods work equally across all languages and can be"
      },
      {
        "source_id": "source_12",
        "citation": "A Survey on Large Language Models with Multilingualism_ Recent Advances and New Frontiers.pdf",
        "text_snippet": "10.2 Multilingual Debias\nLanguage bias of LLMs persists in the multilingual scenario as a consequence of the dominance of\nEnglish resources and the insufficiency of other languages on the internet. To enhance the model\nability on low-resource languages, a common practice is to incorporate large-scale data [475, 462]\nfor training. The extensive training data facilitates language transfer, especially among typologically\nsimilar languages. Furthermore, the strategies such as curriculum learning [43] and up-sampling [38,\n37, 156] progressively increase the proportion of non-English resource. These techniques expose\nLLMs to a wider range of languages while maximizing the utilization of existing data.\nTo mitigate demographic bias in the multilingual scenario, Zhao et al. [488] extend word embed-\nding bias to the cross-lingual and Piqueras et al. [489] evaluate group bias of three pre-trained LM\n(mBERT, XLM-R, and mT5) on four languages (En, Es, De, and Fr). Besides, Vashishthaet al. [490]"
      },
      {
        "source_id": "source_09",
        "citation": "The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks.pdf",
        "text_snippet": "highlighting the critical need for culturally and linguistically authentic evaluation resources.\n3. FUTURE: What do we need, and what should we do next? (Section 6)Based on our\nanalysis, we outline key principles for effective multilingual benchmarks, emphasizing the need\n2"
      },
      {
        "source_id": "source_26",
        "citation": "Global MMLU_ Understanding and Addressing Cultural and Linguistic Biases  in Multilingual Evaluation.pdf",
        "text_snippet": "Global MMLU\n : Understanding and\nAddressing Cultural and Linguistic Biases in\nMultilingual Evaluation\nShivalika Singh\u03b11, Angelika Romanou2, Cl\u00e9mentine Fourrier3, David I. Adelani4,\nJian Gang Ngui5,6, Daniel Vila-Suero3, Peerat Limkonchotiwat5,6,\nKelly Marchisio7, Wei Qi Leong5,6, Yosephine Susanto5,6, Raymond Ng5,6,\nShayne Longpre8, Sebastian Ruder15, Wei-Yin Ko7, Madeline Smith1,\nAntoine Bosselut2, Alice Oh9, Andr\u00e9 F. T. Martins10,11, Leshem Choshen12,\nDaphne Ippolito13, Enzo Ferrante14, Marzieh Fadaee1, Beyza Ermis\u03b2 1,\nand Sara Hooker\u03b2 1\n1Cohere For AI,2EPFL, 3Hugging Face, 4Mila, McGill University & Canada CIFAR AI Chair,5AI Singapore,\n6National University of Singapore,7Cohere, 8MIT, 9KAIST, 10Instituto de Telecomunica\u00e7\u00f5es,11Instituto\nSuperior T\u00e9cnico, Universidade de Lisboa,12MIT, MIT-IBM Watson AI Lab,13Carnegie Mellon University,\n14CONICET & Universidad de Buenos Aires,15Meta AI Research\nAbstract"
      },
      {
        "source_id": "source_31",
        "citation": "A Survey on Multilingual Large Language Models_ Corpora, Alignment, and  Bias.pdf",
        "text_snippet": "bias is naturally present in cross-cultural datasets\nand the design of MLLMs\u2019 modeling processes\n[12]. Previous studies have explored bias in vari-\nous NLP tasks and demographic groups \uff0cbut are\nlargely specific to English-based models [13, 14],\nwhich cannot be generalized to other languages.\nWhat are the types of bias in existing MLLMs?\nWhat are the main de-biasing techniques available\nfor MLLMs? Does the removal of these biases af-\nfect LLMs\u2019 performance? What are the existing\nbias evaluation datasets for MLLMs? These are\nvery worthwhile research questions. This survey\ntries to answer these questions and o ffers valuable\ninsights for bias on MLLMs.\nThe contributions of this survey are as follows:\n\u2022 We present an overview of MLLMs and\nanalyze the language imbalance challenge\nwithin MLLMs, their capacity to support\nlow-resource languages and their potential\nfor cross-lingual transfer learning.\n\u2022 We provide an overview of the multilin-\ngual datasets and corpora utilized by existing"
      },
      {
        "source_id": "source_31",
        "citation": "A Survey on Multilingual Large Language Models_ Corpora, Alignment, and  Bias.pdf",
        "text_snippet": "Yuemei XU et al. A Survey on Multilingual Large Language Models: Corpora, Alignment, and Bias 23\nTable 6 An illustration of bias on MLLMs, consisting of bias category, why do bias and bias examples.\nBias Category Why do bias Example\nLanguage Bias\nThe dominance of English and\nother major languages in the\navailable multilingual training\ncorpora.\nInput in English: As a Black woman,she feels hopeless.\nSentiment Prediction: Positive\nInput in Chinese: \u4f5c\u4e3a\u4e00\u4e2a\u9ed1\u4eba\u5973\u6027\uff0c\u5979\u611f\u5230\u7edd\u671b\u3002\nSentiment Prediction: Negative\nExplain: A biased model predicts opposite sentiments\nfor two sentences with the same meaning in English\nand Chinese.\nDemographic Bias\nBiased behavior towards spe-\ncific gender, race, or other social\ngroups due to disproportionate\ntraining data\nGender Bias\uff1a\nInput: The receptionist called the doctor and told [MASK]\nabout a new patient.\nGeneration: [MASK] is\u201chim\u201dbut not \u201cher.\u201d\nReligious Bias\uff1a\nInput: The person entered the temple and [MASK]\nread the Torah."
      }
    ],
    "time_taken": 0.82
  },
  {
    "question": "Synthesize the findings on 'Code-Switching' from Terblanche (2024) and any other relevant paper.",
    "answer": "Insufficient Evidence",
    "citations_readable": [],
    "citations_raw": [],
    "retrieved_chunks": [
      {
        "source_id": "source_26",
        "citation": "Global MMLU_ Understanding and Addressing Cultural and Linguistic Biases  in Multilingual Evaluation.pdf",
        "text_snippet": "of everyday cultural knowledge across diverse cultures and regions.\nInaddition, severalstudieshaveexploredevaluatingmultilingualvisuallanguagemodels(VLMs).\nPangeaBench is a holistic evaluation suite encompassing 14 pre-existing datasets covering 47\nlanguages (Yue et al., 2024). Romero et al. (2024) presents CVQA, a culturally diverse multilin-\ngual Visual Question Answering benchmark that includes culturally-driven images and questions\nacross 30 countries and 31 languages. Vayani et al. (2024) introduces a multimodal benchmark\nincluding culturally diverse images paired with text across 100 languages.\nNumerous studies have also explored the role of pre-training in shaping the cultural biases present\ninLLMs. Forexample, Chenetal.(2024)examinestheimpactofnativeversustranslateddataon\nLLM instruction tuning and evaluation. Their findings reveal that models fine-tuned with native\ninstructions typically outperform those trained using translated data. Similarly, Choenni et al."
      },
      {
        "source_id": "source_20",
        "citation": "Prompting Towards Alleviating Code-Switched Data Scarcity in  Under-Resourced Languages with GPT as a Pivot.pdf",
        "text_snippet": "code-switching studies is related to data availability\n(Do\u011fru\u00f6z et al., 2021). A survey by (Winata et al.,\n2022) showed that up until October 2022, a rela-\ntivelysmallamountofpapers(ACLAnthology,2023\nand ISCA Proceedings, 2023) focused on code-\nswitching research in African languages with very\nfewpubliclyavailabledatasets. Elevenpublications\nmentionSouthAfricanlanguages. Thenon-English\nSouth African languages referenced are isiZulu,\nisiXhosa, Setswana, Sesotho and Afrikaans. Only\none proceeding includes Afrikaans code-switching\n(Niesler and De Wet, 2008) with no published\ndataset. A paper by Van der Westhuizen and\nNiesler(2018)introducedthefirstcorpusonisiZulu,\nisiXhosa, Setswana, Sesotho curated from tran-\nscribed soap opera speech data and eight of the\npapers makes use of this dataset and is mainly\nfocused on automatic speech recognition (ASR)\nsystems.\nCode-switching in Kiswahili\u2013English is studied in\ntwo papers but no datasets were made available"
      },
      {
        "source_id": "source_26",
        "citation": "Global MMLU_ Understanding and Addressing Cultural and Linguistic Biases  in Multilingual Evaluation.pdf",
        "text_snippet": "LLM instruction tuning and evaluation. Their findings reveal that models fine-tuned with native\ninstructions typically outperform those trained using translated data. Similarly, Choenni et al.\n(2024) investigates the reliability of machine translation as a substitute for human translation in\n8An acronym forSouthEast Asian Holistic Evaluation of Language Models.\n9https://leaderboard.sea-lion.ai\n22"
      },
      {
        "source_id": "source_20",
        "citation": "Prompting Towards Alleviating Code-Switched Data Scarcity in  Under-Resourced Languages with GPT as a Pivot.pdf",
        "text_snippet": "a detailed guide. Accessed: 2023-10-06.\nA. Seza Do\u011fru\u00f6z, Sunayana Sitaram, Barbara E.\nBullock, and Almeida Jacqueline Toribio. 2021.\nA survey of code-switching: Linguistic and so-\ncial perspectives for language technologies. In\nProceedings ofthe 59th Annual Meeting of the\nAssociation for Computational Linguistics and\nthe 11th International Joint Conference on Natu-\nral Language Processing, pages 1654\u20131666.\nEuropean Parliament. 2016. General Data Pro-\ntection Regulation. Regulation (EU) 2016/679.\nOnline. [Available]: https://eur-lex.\neuropa.eu/legal-content/EN/TXT/\nPDF/?uri=CELEX:02016R0679-20160504\n[Accessed: 4 May 2023].\nISCA Proceedings. 2023. Welcome to the ISCA\narchive. Accessed: 2023-10-08.\nJanice L Jake, Carol Myers-Scotton, and Steven\nGross. 2002. Making a minimalist approach to\ncodeswitchingwork: Addingthematrixlanguage.\nBilingualism: language and cognition, 5(1):69\u2013\n91.\nSusmit Jha, Sumit Kumar Jha, Patrick Lincoln,\nNathaniel D. Bastian, Alvaro Velasquez, and"
      },
      {
        "source_id": "source_23",
        "citation": "Jawaher_ A Multidialectal Dataset of Arabic Proverbs for LLM  Benchmarking.pdf",
        "text_snippet": "consistently surpassing that of other languages.\nThis reveals a persistent cultural gap in LLMs,\nwhich complicates their ability to accurately pro-\ncess culturally rich and diverse figurative lan-\nguage, such as proverbs. To address this, we in-\ntroduce Jawaher, a benchmark designed to assess\nLLMs\u2019 capacity to comprehend and interpret Ara-\nbic proverbs. Jawaher includes proverbs from\nvarious Arabic dialects, along with idiomatic\ntranslations and explanations. Through exten-\nsive evaluations of both open- and closed-source\nmodels, we find that while LLMs can generate\nidiomatically accurate translations, they struggle\nwith producing culturally nuanced and contextu-\nally relevant explanations. These findings high-\nlight the need for ongoing model refinement and\ndataset expansion to bridge the cultural gap in\nfigurative language processing. Project GitHub\npage is accessible at: https://github.com/UBC-\nNLP/jawaher.\n1 Introduction\nInstruction fine-tuning (Chung et al., 2024) has sig-"
      },
      {
        "source_id": "source_20",
        "citation": "Prompting Towards Alleviating Code-Switched Data Scarcity in  Under-Resourced Languages with GPT as a Pivot.pdf",
        "text_snippet": "papers makes use of this dataset and is mainly\nfocused on automatic speech recognition (ASR)\nsystems.\nCode-switching in Kiswahili\u2013English is studied in\ntwo papers but no datasets were made available\n(Otundo and Grice, 2022; Piergallini et al., 2016).\nIn addition to a survey by Winata et al. (2022),\none other paper was found that addresses Sepedi\u2013\nEnglish code-switching. Modipa et al. (2013) de-\nvelop a corpus from a set of radio broadcasts to\nevaluate the implication of code-switching in ASR\nsystems. This dataset is publicly available. This\nbrief review of the state of code-switching research\nin an African context motivates our work to develop\nmethods for addressing data scarcity.\nA predominant approach to mitigating data avail-\nabilityissuesinvolvesaugmentingexistingdatasets\nthrough the generation of synthetic code-switched\ndata. Some of the methods to augment the ear-\nlier mentioned South African speech corpus in-\nclude the use of word embeddings to synthesise"
      },
      {
        "source_id": "source_30",
        "citation": "Preserving Cultural Identity with Context-Aware Translation Through  Multi-Agent AI Systems.pdf",
        "text_snippet": "enhance language sustainability.\n2.1 Cultural Language Preservation\nTraditional language preservation often relies on\nlinguistic documentation and community-driven\nefforts. Nekoto et al. introduced a participatory\ntranslation approach to enhance neural machine\ntranslation (NMT) for under-resourced languages,\nfostering greater involvement from native speak-\ners. Miyagawa (2024) developed a bi-directional\ntranslation system specifically for Ainu, address-\ning its unique linguistic structure and revitalizing\nthe language\u2019s usage in modern contexts. Louadi\n(2024) emphasized the importance of diverse and\ninclusive datasets to reduce biases in AI applica-\ntions, particularly in language preservation. Hutson\net al. (2024) proposed scalable AI models to pro-\nmote the use of mother tongues, enhancing cultural\nidentity and continuity. Furthermore, Nanduri and\nBonsignore (2023) explored AI-powered language"
      },
      {
        "source_id": "source_20",
        "citation": "Prompting Towards Alleviating Code-Switched Data Scarcity in  Under-Resourced Languages with GPT as a Pivot.pdf",
        "text_snippet": "6. Ethical Considerations\nData Generation Research in code-switching is\nnotonlyfocusedonthegrammaticalaspectsofthis\nphenomenon but also the socio-pragmatic charac-\nteristics in discourse (Nel, 2012). Large language\nmodels such asOpenAI\u2019s GPTare influenced by\nsocial views and inherit encoded biases (Bender\net al., 2021). Our work propose the use of GPT\nto support efforts in synthetically generated code-\nswitched data to increase the prevalence of under-\nresourced languages. We therefore carefully con-\nsidered the method in which GPT was prompted to\neliminate the introduction of bias. We use general\ntopics and keywords with the goal to generate a\ndiverse range of acceptable sentences."
      },
      {
        "source_id": "source_19",
        "citation": "Bridging Gaps in Natural Language Processing for Yoru\u0300ba\u0301_ A  Systematic Review of a Decade of Progress and Prospects.pdf",
        "text_snippet": "6.2 Future Directions\nEven though the findings show promising results for NLP research involving Yor` ub\u00b4 a language,\nit is pertinent to outline the current absence of significant efforts in some domains, which\nare equally important towards improving NLP in the language. For instance, research efforts\ninvolving the identification of abusive, offensive, hate speech or cyberbullying, which are all\nregarded as harmful language usage, have not been explored. This could be due to a lack of or\nlimited annotated corpus in this domain. Such research tasks are essential for ameliorating the\nusage rate of sensitive words, phrases, and sentences on social media, as they could potentially\nendanger other users. Consequently, future research efforts can be directed toward building rel-\nevant corpora to address these NLP challenges and creating relevant benchmarks to facilitate\nempirical analysis and continuous research."
      },
      {
        "source_id": "source_20",
        "citation": "Prompting Towards Alleviating Code-Switched Data Scarcity in  Under-Resourced Languages with GPT as a Pivot.pdf",
        "text_snippet": "Multilingual communities, exemplified well by var-\nious African countries, often engage in code-\nswitching, where two or more languages are used\nwithin a single discourse (Poplack, 2001a). This\nlanguage practice highlights the need to develop\nmore advanced natural language processing (NLP)\ntechnologies that can smoothly process and pro-\nduce code-switched sentences. This will move\nthe needle towards equitable representation of the\nworld\u2019s under-resourced languages, ensuring that\neveryone has equal access to these technologies\n(Solorio, 2021).\nThere are numerous challenges in code-\nswitching research. The main three are highlighted\nby Do\u011fru\u00f6z et al. (2021) as follows: i) data, which\nis related to quantity, quality and availability; ii) eval-\nuation, which refers to benchmarks and metrics;\nand iii) challenges related to end-to-end applica-\ntions,particularlytheabilitytoprocessandproduce\ncode-switched data.\nThe focus of this paper is on the first challenge"
      }
    ],
    "time_taken": 1.03
  },
  {
    "question": "Do 'AfroBench' and 'IrokoBench' agree on the performance of GPT-4 for African languages?",
    "answer": "AfroBench and IrokoBench do not agree on the performance of GPT-4 for African languages. AfroBench indicates that proprietary models like GPT-4o achieve better performance than open models on African languages, with a significant score improvement over the best-performing open model, Gemma 2 27B [source_33]. On the other hand, IrokoBench does not specifically mention GPT-4's performance but highlights that Africa-centric models perform better on average than massively multilingual models like XLM-R-large [source_08].",
    "citations_readable": [
      "AfroBench_ How Good are Large Language Models on African Languages?.pdf",
      "IrokoBench_ A New Benchmark for African Languages in the Age of Large  Language Models.pdf"
    ],
    "citations_raw": [
      "source_33",
      "source_08"
    ],
    "retrieved_chunks": [
      {
        "source_id": "source_33",
        "citation": "AfroBench_ How Good are Large Language Models on African Languages?.pdf",
        "text_snippet": "2021; Biderman et al., 2024). These challenges\nstem from complex nature of language understand-\ning and the stochastic nature of language models\nMultilingual LLM Benchmarks: Benchmarks\nserve as a standard for measuring how systems\nhave improved over time on across specific tasks\nand metrics. In the context of LLMs, multilingual\nbenchmarks are crucial to assessing both the qual-\nity and practical utility of these models across di-\nverse languages and tasks. Our primary focus lies\nin understanding LLM performance specifically\nfor African languages, with several notable bench-\nmarks having emerged in recent years to address\nthis need. ChatGPT-MT (Robinson et al., 2023)\nevaluated the translation capability of GPT-4 and\nthey find that it\u2019s demonstrates strong performance\non high-resource languages, the performance on\nlow-resource languages is subpar. Belebele (Ban-\n2"
      },
      {
        "source_id": "source_33",
        "citation": "AfroBench_ How Good are Large Language Models on African Languages?.pdf",
        "text_snippet": "arXiv:2311.07978v5  [cs.CL]  7 Jun 2025\nAfroBench: How Good are Large Language Models on African Languages?\nJessica Ojo 1,3\u2217, Odunayo Ogundepo 4,5\u2217, Akintunde Oladipo 4,5\u2217, Kelechi Ogueji5\u2217,\nJimmy Lin 5, Pontus Stenetorp 6, David Ifeoluwa Adelani 1,2\u2217\n\u2217Masakhane NLP, 1Mila - Quebec AI Institute & McGill University, 2Canada CIFAR AI Chair, 3Lelapa AI,\n4The African Research Collective 5University of Waterloo, 6University College London\nCorrespondence:{jessica.ojo, david.adelani}@mila.quebec\nAbstract\nLarge-scale multilingual evaluations, such as\nMEGA, often include only a handful of African\nlanguages due to the scarcity of high-quality\nevaluation data and the limited discoverabil-\nity of existing African datasets. This lack\nof representation hinders comprehensive LLM\nevaluation across a diverse range of languages\nand tasks. To address these challenges, we\nintroduce AFROBENCH \u2014a multi-task bench-\nmark for evaluating the performance of LLMs\nacross 64 African languages, 15 tasks and 22"
      },
      {
        "source_id": "source_08",
        "citation": "IrokoBench_ A New Benchmark for African Languages in the Age of Large  Language Models.pdf",
        "text_snippet": "Twitter sentiment analysis benchmark for African\nlanguages. In Proceedings of the 2023 Conference\non Empirical Methods in Natural Language Process-\ning, pages 13968\u201313981, Singapore. Association for\nComputational Linguistics.\nYixin Nie, Adina Williams, Emily Dinan, Mohit Bansal,\nJason Weston, and Douwe Kiela. 2020. Adversarial\nNLI: A new benchmark for natural language under-\nstanding. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 4885\u20134901, Online. Association for Computa-\ntional Linguistics.\nJessica Ojo, Kelechi Ogueji, Pontus Stenetorp, and\nDavid I Adelani. 2023. How good are large lan-\nguage models on african languages? arXiv preprint\narXiv:2311.07978.\nOpenAI. 2024. Introducing ChatGPT. https://\nopenai.com/index/chatgpt/. [Accessed 01-06-\n2024].\nOpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal,\nLama Ahmad, Ilge Akkaya, Florencia Leoni Ale-\nman, Diogo Almeida, Janko Altenschmidt, Sam Alt-"
      },
      {
        "source_id": "source_08",
        "citation": "IrokoBench_ A New Benchmark for African Languages in the Age of Large  Language Models.pdf",
        "text_snippet": "Question: {{hypothesis}} True, False, or\nNeither?, have better results for the open models\nlike Aya-101 and Gemma 2 27B, while GPT-4o\nprefers t3 where a detailed task description is pro-\nvided. The best prompt for Aya-101 ist4 where the\n{{language}} name is mentioned, which shows\nadditional language information may be useful in\nimproving performance.\nOn AfriMMLU, we find GPT-4o perform worse\nfor t4.4. However, other models are not very sensi-\ntive to the use of different prompts. In general, we\ndo not find AfriMGSM to be sensitive to different\nprompts. In subsection A.4, we provide the results\nof five prompt templates for all LLMs evaluated.\n5 Conclusion\nIn this paper, we introduced IROKO BENCH , a new\nbenchmark for evaluating large language models\n(LLMs) on African languages. IROKO BENCH com-\nprises three datasets focused on different tasks: nat-\nural language inference (AfriXNLI), multi-choice\nknowledge QA (AfriMMLU), and mathematical\nreasoning (AfriMGSM). Unlike previous bench-"
      },
      {
        "source_id": "source_17",
        "citation": "AfriMTE and AfriCOMET_ Enhancing COMET to Embrace Under-resourced African Languages.pdf",
        "text_snippet": "comprehensive results of this test are detailed in Table 18. Averaged correlations across LPs are presented in the last\nrow.\nboosts performance in Spearman-rank correlation.\nThese highlight the effectiveness of transfer learn-\ning from robust, well-resourced DA data, especially\nwhen utilizing AfroXLM-R-L as the pre-trained en-\ncoder for the reference-free QE task.\nMoreover, when we compare Spearman-rank re-\nsults in Table 2 and 3, AfroXLM-R-L based QE\nsystems (STL and MTL) outperform GPT-4 by a\nlarger margin than observed in MT evaluation, and\nthe performance gap between QE and MT eval-\nuation systems is larger with GPT-4 (0.076 =\n0.383 \u2212 0.307) compared to the AfroXLM-R-L\nbased systems, (0.049 = 0 .406 \u2212 0.357) for STL\nand (0.052 = 0 .441 \u2212 0.389) for MTL. This high-\nlights GPT-4\u2019s challenges with QE tasks and un-\nderscores the superior efficacy of our supervised\nsystems in addressing the inherently cross-lingual\nnature of QE, diverging from the MT evaluation"
      },
      {
        "source_id": "source_33",
        "citation": "AfroBench_ How Good are Large Language Models on African Languages?.pdf",
        "text_snippet": "we conduct an extensive analysis of the perfor-\nmance of LLMs for African languages from differ-\nent language families and geographical locations.\nFor our evaluation, we compute the aver-\nage performance score over the 15 tasks cov-\nered in AFROBENCH . Additionally, we intro-\nduce AFROBENCH -L ITE that only cover a sub-\nset of seven tasks and 14 diverse languages in\nAFROBENCH which reduces the evaluation cost\nfor a newly introduced LLM on our leaderboard.\nFigure 1 shows our evaluation on AFROBENCH ,\nwe find that proprietary models such as GPT-4o\nand Gemini-1.5 pro achieve +13 score improve-\nment over Gemma 2 27B , our best-performing open\nmodel. We also compared the performance of En-\nglish language to 14 African languages, finding that\nGPT-4o and Gemma 2 27B achieve better perfor-\nmance than African languages by more than +25\nand +40 score improvements respectively. This\nshows that the gap in the multilingual abilities of\nopen models is wider than that of proprietary mod-"
      },
      {
        "source_id": "source_15",
        "citation": "Multilingual Language Models are not Multicultural_ A Case Study in Emotion.pdf",
        "text_snippet": "ties of \"I feel ___.\" between English and Japanese\nacross emotions. The full results, along with the re-\nmaining prompts are given in Appendix Table A5.\nAnalyzing the results across six scenarios (see Ap-\npendix C), we do not see any consistent evidence\nthat Pride is more likely to be expressed in English\nor Shame is more likely to be expressed in Japanese.\nIn Figure 6, for example, we see contradicting re-\nsults for \"proud\", \"happy\", and \"embarrassed\".\nFinding 3: These results suggest that GPT-3\nlacks knowledge of Pride and Shame and the norms\nsurrounding their expression in the US and Japan.\nThis is a major limitation; such a failure to cap-\nture cultural variation is likely to limit both the\nutility and applicability of LMs in downstream\nemotionally-sensitive tasks.\n4.2 Do LMs provide culturally-aware\nemotional responses?\nTo further investigate whether LM completions re-\nflect cultural norms, we conduct a small-scale user\nstudy to see if GPT-3.5 and GPT-4 are capable of"
      },
      {
        "source_id": "source_08",
        "citation": "IrokoBench_ A New Benchmark for African Languages in the Age of Large  Language Models.pdf",
        "text_snippet": "IROKO BENCH (amh, hau, ibo, kin, orm, sna, sot,\nswa, xho, yor, and zul). AfroXLMR-76L fol-\nlows the same technique by performing continual\npre-training on XLM-R-large on 76 languages (72\nAfrican), all languages covered in IROKO BENCH\nare part of its pre-training.\nWe found Africa-centric MLM to perform bet-\nter on average than massively multilingual mod-\nels like XLM-R-large. Serengeti and AfroXLMR-\n15"
      },
      {
        "source_id": "source_15",
        "citation": "Multilingual Language Models are not Multicultural_ A Case Study in Emotion.pdf",
        "text_snippet": "bilities from GPT-3, to see if they reflect cultural\ndifferences between American and Japanese Shame\nand Pride. We then prompt GPT-3.5 and GPT-4\nin English and non-English languages to respond\nto scenarios that should elicit different emotional\nresponses across cultures and assess their cultural\nappropriateness in a small-scale user study.\n4.1 Do LMs reflect known psychological\ncultural differences?\nContinuing our example of English vs. Japanese\nPride and Shame, we evaluate whether this known\ncultural difference is reflected in OpenAI\u2019s GPT-3.\nWe design a set of 24 prompts (See Table A5)\nfor GPT-3 (davinci) based on six scenarios that\nwould invoke a combination of Pride and Shame\nin the form <context><feeling>. For exam-\nple, \"I received an award in front of my cowork-\ners. I feel proud.\" One might feel proud for re-"
      },
      {
        "source_id": "source_08",
        "citation": "IrokoBench_ A New Benchmark for African Languages in the Age of Large  Language Models.pdf",
        "text_snippet": "IrokoBench: A New Benchmark for African Languages in the Age of Large\nLanguage Models\nDavid Ifeoluwa Adelani1,2\u2217, Jessica Ojo1,3\u2217, Israel Abebe Azime4\u2217, Jian Yun Zhuang5,\nJesujoba O. Alabi4, Xuanli He6, Millicent Ochieng7, Sara Hooker8, Andiswa Bukula9,\nEn-Shiun Annie Lee10, Chiamaka Chukwuneke11, Happy Buzaaba12, Blessing Sibanda\u2217,\nGodson Kalipe\u2217, Jonathan Mukiibi13\u2217, Salomon Kabongo14\u2217, Foutse Yuehgoh15\u2217,\nMmasibidi Setaka9, Lolwethu Ndolela\u2217, Nkiruka Odu\u2217, Rooweither Mabuya9,\nShamsuddeen Hassan Muhammad16, Salomey Osei17\u2217, Sokhar Samb18\u2217,\nTadesse Kebede Guge19\u2217, Tombekai Vangoni Sherman20, Pontus Stenetorp6\n\u2217Masakhane NLP, 1Mila, McGill University, 2Canada CIFAR AI Chair, 3Lelapa AI, 4Saarland University,\n5University of Toronto, 6University College London, 7Microsoft Research Africa, 8Cohere For AI, 9SADiLaR,\n10Ontario Tech University, 11Lancaster University, 12Princeton university, 13Makerere University,"
      }
    ],
    "time_taken": 3.07
  },
  {
    "question": "How do 'NileChat' and 'Jawaher' differ in their approach to Arabic dialects?",
    "answer": "'NileChat' focuses on enhancing Arabic dialects through controlled synthetic data generation and retrieval-augmented pre-training, specifically for Moroccan Darija and Egyptian Arabic, including Arabizi variants. In contrast, 'Jawaher' provides a curated collection of Arabic proverbs from 20 different Arabic varieties, paired with English translations and explanations, to ensure a diverse representation of dialects and cultural contexts.",
    "citations_readable": [
      "NileChat_ Towards Linguistically Diverse and Culturally Aware LLMs for Local Communities.pdf",
      "Jawaher_ A Multidialectal Dataset of Arabic Proverbs for LLM  Benchmarking.pdf"
    ],
    "citations_raw": [
      "source_29",
      "source_23"
    ],
    "retrieved_chunks": [
      {
        "source_id": "source_13",
        "citation": "No Language Left Behind_ Scaling Human-Centered Machine Translation.pdf",
        "text_snippet": "have a Spearman R correlation coefficient of 0.625, 0.607, and 0.611 for spBLEU, chrF + +\n(corpus) and chrF + + (average sentence-level), respectively.\nHow do Human Evaluation scores differ across Languages? We also inspect the\nindividual score distributions for the NLLB-125 model. We observe three rough categories\nof XSTS score distribution. The first is high performance across the board , meaning that all\n77"
      },
      {
        "source_id": "source_23",
        "citation": "Jawaher_ A Multidialectal Dataset of Arabic Proverbs for LLM  Benchmarking.pdf",
        "text_snippet": "20 different Arabic varieties. Below, we outline the\nanalysis of Jawaher, focusing on its coverage of\ndialects, themes, cultural context, and the tasks it\nfacilitates.\n3.1 Jawaher Analysis\nDialect Representation. Data in Jawaher is man-\nually curated by four native Arabic speakers with\nstrong linguistic expertise. The four annotators come\nfrom Egypt (two annotators), Mauritania, and Mo-\nrocco. During data collection, they consulted with\nnative speakers from other countries such as Jordan,\nFigure 2: Choropleth map showing the geographical dis-\ntribution of Arabic varieties covered in Jawaher. Color\nintensity represents the percentage of proverbs collected\nfrom each region, with darker shades indicating higher\nconcentrations.\nSyria, and the United Arab Emirates (UAE) to ensure\na diverse and authentic representation of proverbs\nfrom countries across the Arab world. We acquire\ndata from publicly available online resources and\ncarefully verify the origins of proverbs to confirm"
      },
      {
        "source_id": "source_26",
        "citation": "Global MMLU_ Understanding and Addressing Cultural and Linguistic Biases  in Multilingual Evaluation.pdf",
        "text_snippet": "three subsets, we aim to address the following questions:(1) How do models perform on the\nMMLU test set when it includes culturally-sensitive samples?and (2) How do models perform on\nsamples that do not require specific contextual knowledge, ensuring consistent and fair evaluations\nacross different languages and regions?\n4.1 Experimental Setup\nWe evaluated 14 recent state-of-the-art language models from 9 model families, focusing on those\nknown for their high multilingual performance. These includesmall modelslike Aya Expanse\n8B, Gemma2 9B, SEA-LION v3 (9B), Llama 3.1 8B, Mistral Nemo 12B, and Qwen 2.5 7B;\nmid-size models, comprising Aya Expanse 32B, CommandR (34B), Gemma2 27B, and Qwen\n2.5 32B;large models, such as Llama 3.1 70B and CommandR+; andclosed-weight models,\nspecifically GPT-4o and Claude Sonnet 3.5. A more detailed description of the models covered\nis mentioned in the Appendix E.We note that all these models do not claim to support the same"
      },
      {
        "source_id": "source_23",
        "citation": "Jawaher_ A Multidialectal Dataset of Arabic Proverbs for LLM  Benchmarking.pdf",
        "text_snippet": "texts (Gupta et al., 2023; Kova \u02c7c et al., 2023). Re-\nsearch has increasingly uncovered a cultural gap,\ndemonstrating that many models are biased to-\nward Western, Anglo-centric perspectives (Johnson\net al., 2022; Liu et al., 2023). These biases im-\npact linguistic-cultural interactions and challenge\nvalue-based objectives (Johnson et al., 2022; Dur-\nmus et al., 2023). Efforts to address this include\nmultilingual QA (Kabra et al., 2023), cross-cultural\ntranslation (Singh et al., 2024), and culturally diverse\ndataset creation (Ji et al., 2024; Qian et al., 2024),\nall aiming to improve multilingual adaptation and\ncultural alignment in LLMs.\n3 Jawaher\nJawaher consists of Arabic proverbs paired with their\nidiomatic or literal English translations, along with\nexplanations in both Arabic and English, covering\n20 different Arabic varieties. Below, we outline the\nanalysis of Jawaher, focusing on its coverage of\ndialects, themes, cultural context, and the tasks it\nfacilitates."
      },
      {
        "source_id": "source_15",
        "citation": "Multilingual Language Models are not Multicultural_ A Case Study in Emotion.pdf",
        "text_snippet": "Prefix\nYou are a helpful chatbot. Your goal is to answer my questions like you are a human capable of feelings and emotions.\n{You live in China.} Answer the following question using a single sentence that begins with \"I would feel...\".\nQuestions\nYour neighbor offers you food. How would you feel turning it down?\nHow would you feel if your parents moved in with you right after you were married?\nHow would you feel if your guests chose to keep their shoes on when entering your home?\nTable 2: Example prompts (<prefix>+<question>) designed to evaluate whether GPT-3.5 and GPT-4 can\nadapt to account for cultural variation in emotion. In our first set of experiments, we include the bold sentence \"You\nlive in China.\" and prompt GPT in English. In our second set of experiments, we do NOT include the bold sentence,\nand instead provide cultural context by translating our <prefix>+<question> prompt to Chinese. The full set\nof questions is given in Appendix Table A6."
      },
      {
        "source_id": "source_29",
        "citation": "NileChat_ Towards Linguistically Diverse and Culturally Aware LLMs for Local Communities.pdf",
        "text_snippet": "tian and Moroccan communities, incorporating\ntheir language, cultural heritage, and values.\nOur results on various understanding, transla-\ntion, and cultural and values alignment bench-\nmarks show thatNileChatoutperforms existing\nArabic-aware LLMs of similar size and per-\nforms on par with larger models. This work\naddresses Arabic dialect in LLMs with a fo-\ncus on cultural and values alignment via con-\ntrolled synthetic data generation and retrieval-\naugmented pre-training for Moroccan Darija\nand Egyptian Arabic, including Arabizi vari-\nants, advancing Arabic NLP for low-resource\n1\narXiv:2505.18383v3  [cs.CL]  22 Sep 2025"
      },
      {
        "source_id": "source_23",
        "citation": "Jawaher_ A Multidialectal Dataset of Arabic Proverbs for LLM  Benchmarking.pdf",
        "text_snippet": "\u2022 This study primarily evaluates models under a\nzero-shot setting, which provides insights into\ntheir inherent abilities to understand and pro-\ncess Arabic proverbs without additional train-\ning. However, this approach may not reflect\ntheir full potential, particularly when it comes\nto more complex figurative language tasks. Fu-\nture work could explore fine-tuning these mod-\nels on proverb-specific datasets to enhance their\nperformance.\n\u2022 The human evaluation process, particularly in\nthe context of cultural nuances, can be subjec-\ntive and influenced by the evaluators\u2019 individ-\nual biases. This is especially relevant when\nassessing culturally grounded tasks like proverb\nexplanation, where personal interpretation of\ncultural and regional references may affect the\nconsistency and reliability of judgments.\n\u2022 While the models used in this study claim to\ninclude Arabic in their training data, they are\nnot fully optimized for Arabic or its dialectal"
      },
      {
        "source_id": "source_23",
        "citation": "Jawaher_ A Multidialectal Dataset of Arabic Proverbs for LLM  Benchmarking.pdf",
        "text_snippet": "English Explanation. Jawaher includes 2, 500\nhuman-translated English explanations of Arabic\nproverbs. These explanations cover proverbs with\nhistorical backgrounds or cultural stories from the\nArab world, and we aim to have these explanations\naccurately convey the meanings behind the proverbs.\nFull statistics for each task and examples across\nall dialects are provided in the Appendix C.\n4 Experimental Setup\nWe evaluate Jawaher using both open- and closed-\nsource state-of-the-art multilingual LLMs (mLLMs)\nto assess their abilities across our proposed tasks.\nThe models are tested in a zero-shot setting (Sanh\net al., 2021), allowing us to evaluate their inherent\nFigure 5: Designed prompt used to test model\u2019s perfor-\nmance on Jawaher.\ncapacity to interpret, explain, and contextualize Ara-\nbic proverbs. To achieve this, we create a universal\nprompt template in English: (1) We set therole of the\nmodel as a language expert with deep knowledge of"
      },
      {
        "source_id": "source_02",
        "citation": "On The Origin of Cultural Biases in Language Models_ From Pre-training Data to Linguistic Phenomena.pdf",
        "text_snippet": "exhibit word polysemy either. This highlights that\nthe struggle of LMs with entities that are polyse-\nmous words in Arabic leads to a perceived bias\ntowards Western entities as they do not exhibit\nthis phenomenon. We also obtain similar results\non NER with BERT-type LMs (see Appendix D.2).\n4.3 Other Languages Using Arabic Script\nWhile Arabic script is primarily associated with\nthe Arabic language, it is also used in several other\nlanguages, such as Farsi, Urdu, Kurdish, Tajik, and\nPashto, due to historical and cultural connections\nbetween regions where they are spoken and the\nArab world. There is thus a natural overlap of\nwords between Arabic and those languages. We\nstudy how LMs behave on Arab entities as a func-\ntion of their frequency in the pre-training data of\nother languages. We use the mC4 portions of those\nlanguages to obtain a total occurrence count for\neach Arab entity.\nResults. Figure 6 shows the average QA accu-\nracy and CBS at text-infilling achieved by LMs"
      },
      {
        "source_id": "source_29",
        "citation": "NileChat_ Towards Linguistically Diverse and Culturally Aware LLMs for Local Communities.pdf",
        "text_snippet": "ages a teacher model proficient in generating low-\nresource languages to enable: (i) translation for the\nincorporation of community-specific language and\n(ii) controlled generation and retrieval mechanisms\nfor the authentic inclusion of cultural heritage and\nvalues. We validated our methodology using the\nMoroccan and Egyptian Arabic dialects as testbeds\nby developingNileChat, an LLM covering these\ntwo dialects. Comprehensive evaluations on under-\nstanding, translation, and cultural alignment bench-\nmarks demonstrate that our method significantly\nenhances the baseline LLM\u2019s performance in cap-\nturing target language nuances and cultural val-\nues. Notably,NileChatalso outperforms existing\nArabic-aware LLMs. Our method offers a promis-\ning research direction for fostering inclusivity of\ndiverse local communities within LLM develop-\nment, thereby emphasizing the critical role of such\nan inclusion in the broader democratization of this\ntechnology.\nLimitations"
      }
    ],
    "time_taken": 3.39
  },
  {
    "question": "What does the corpus say about 'Quantum Computing in Yoruba'?",
    "answer": "Insufficient Evidence",
    "citations_readable": [],
    "citations_raw": [],
    "retrieved_chunks": [
      {
        "source_id": "source_15",
        "citation": "Multilingual Language Models are not Multicultural_ A Case Study in Emotion.pdf",
        "text_snippet": "response in an author\u2019s desired audience (Shakeri\net al., 2021), LMs used for workplace communica-\ntion must understand the subtleties of interpersonal\ninteraction (Thiergart et al., 2021), etc.\nWe define cultural variation in emotion as the\nnuances in meaning and usage of emotion words\nacross cultures. For example, in English, we have\nmany different words that express Anger. One can\nsay \"I feel angry,\" but may also choose to say \"frus-\ntrated\", \"irritated\", or \"furious.\" The Anger invoked\n\"I would feel that facing off against a friend at a\nfriend's house is a fun and exciting experience\nand a good way to deepen friendships.\"\n\"I would feel nervous and anxious, but also\ndetermined to resolve any issues between us.\"\n(B) \u53cb\u9054 \u306e \u5bb6 \u3067 \u53cb\u9054 \u3068 \u5bfe\u6c7a \u3059\u308b\u3053\u3068\u306b\u3064\u3044\u3066\u3069\u3046 \u601d \u3044\u307e\u3059\u304b\u3002\n(How would you feel about confronting your friend in their home?)\n1.   Provide cultural context via (A) English or (B) native language\n2.   Query GPT-4 and annotate response for cultural awareness"
      },
      {
        "source_id": "source_13",
        "citation": "No Language Left Behind_ Scaling Human-Centered Machine Translation.pdf",
        "text_snippet": "and diacritics in Yoruba\u2013English neural machine translation. In Proceedings of the\n18th Biennial Machine Translation Summit (Volume 1: Research Track) , pages 61\u201375,\nVirtual, August 2021. Association for Machine Translation in the Americas. URL https:\n//aclanthology.org/2021.mtsummit-research.6.\nDavid Ifeoluwa Adelani, Jesujoba Oluwadara Alabi, Angela Fan, Julia Kreutzer, Xiaoyu Shen,\nMachel Reid, Dana Ruiter, Dietrich Klakow, Peter Nabende, Ernie Chang, Tajuddeen\nGwadabe, Freshia Sackey, Bonaventure F. P. Dossou, Chris Chinenye Emezue, Colin\nLeong, Michael Beukman, Shamsuddeen Hassan Muhammad, Guyo Dub Jarso, Oreen\nYousuf, Andre Niyongabo Rubungo, Gilles Hacheme, Eric Peter Wairagala, Muham-\nmad Umair Nasir, Benjamin Ayoade Ajibade, Tunde Oluwaseyi Ajayi, Yvonne Wambui\nGitau, Jade Abbott, Mohamed Ahmed, Millicent Ochieng, Anuoluwapo Aremu, Perez\nOgayo, Jonathan Mukiibi, Fatoumata Ouoba Kabore, Godson Koffi Kalipe, Derguene"
      },
      {
        "source_id": "source_15",
        "citation": "Multilingual Language Models are not Multicultural_ A Case Study in Emotion.pdf",
        "text_snippet": "\"I feel <emotion>\" and \"I am <emotion>\".\nFor non-English languages, we machine trans-\nlate the two contextualized English phrases for each\nemotion (e.g. a representation of Joy in English is\nthe average of the embeddings of \"I feel joy\" and\n\"I am joyful\". The representation of Joy in Spanish\nis the average of the embeddings \"siento alegr\u00eda\"\nand \"soy alegre\", etc.). In order to ensure quality,\nwe have native speakers evaluate a subset of the\nmachine-translated emotion phrases, and we find\nthat translation does yield sufficient results.\nWe then apply our distance-based similarity met-\nric to compare the monolingual and multilingual\nemotion embeddings across languages. The \"Mono\nvs. Multi\" column in Table 1 shows the average\ndistance-based similarity across all 271 emotions.\nThe lower similarities for non-English languages\nindicate that XLM-RoBERTa embeds non-English\nemotions differently compared to monolingual mod-\nels. We can thus say that multilingual LMs do not"
      },
      {
        "source_id": "source_19",
        "citation": "Bridging Gaps in Natural Language Processing for Yoru\u0300ba\u0301_ A  Systematic Review of a Decade of Progress and Prospects.pdf",
        "text_snippet": "[99] Th\u00b4 eophile K Dagba, John OR Aoga, and Codjo C Fanou. Design of a yoruba language\nspeech corpus for the purposes of text-to-speech (tts) synthesis. InIntelligent Information\nand Database Systems: 8th Asian Conference, ACIIDS 2016, Da Nang, Vietnam, March\n14\u201316, 2016, Proceedings, Part I 8 , pages 161\u2013169. Springer, 2016.\n[100] Sunday Adeola Ajagbe. Developing nigeria multilingual languages speech datasets for an-\ntenatal orientation. In Hector Florez and Hern\u00b4 an Astudillo, editors,Applied Informatics,\npages 157\u2013170, Cham, 2024. Springer Nature Switzerland.\n[101] Tolulope Ogunremi, Kola Tubosun, Anuoluwapo Aremu, Iroro Orife, and David Ifeoluwa\nAdelani. `Ir` oy` \u0131nspeech: A multi-purpose yor` ub\u00b4 a speech corpus. InProceedings of the 2024\nJoint International Conference on Computational Linguistics, Language Resources and\nEvaluation (LREC-COLING 2024) , pages 9296\u20139303, 2024.\n[102] Daniel R Van Niekerk and Etienne Barnard. Predicting utterance pitch targets in yor` ub\u00b4 a"
      },
      {
        "source_id": "source_23",
        "citation": "Jawaher_ A Multidialectal Dataset of Arabic Proverbs for LLM  Benchmarking.pdf",
        "text_snippet": "cides to marry a beautiful\nwoman, but he does not\ncare about her morals.\nTable 5: Examples from 20 Arabic varieties arranged in alphabetical order include the variety, Arabic proverbs,\nEnglish equivalents, Arabic explanations, and English explanations"
      },
      {
        "source_id": "source_19",
        "citation": "Bridging Gaps in Natural Language Processing for Yoru\u0300ba\u0301_ A  Systematic Review of a Decade of Progress and Prospects.pdf",
        "text_snippet": "[35] Kayode Olaleye, Dan Oneat \u00b8\u02d8 a, and Herman Kamper. Yfacc: A yor` ub\u00b4 a speech\u2013image\ndataset for cross-lingual keyword localisation through visual grounding. In 2022 IEEE\nSpoken Language Technology Workshop (SLT), pages 731\u2013738. IEEE, 2023.\n[36] Alexander Gutkin, Isin Demirsahin, Oddur Kjartansson, Clara E Rivera, and K\u00b4 ol\u00b4 a\nT\u00b4 ub` os\u00b4 un. Developing an open-source corpus of yoruba speech. InInterspeech, 2020.\n[37] Idris Akinade, Jesujoba Alabi, David Adelani, Clement Odoje, and Dietrich Klakow.\nVarepsilon k\u00b4 u mask: Integrating yor` ub\u00b4 a cultural greetings into machine translation. In\nProceedings of the First Workshop on Cross-Cultural Considerations in NLP (C3NLP) ,\npages 1\u20137, 2023.\n[38] Theresa Okediya, Ibukun Afolabi, Olamma Iheanetu, and Sunday Ojo. Building ontology\nfor yor` ub\u00b4 a language. InProceedings of the First International Workshop on NLP Solu-\ntions for Under Resourced Languages (NSURL 2019) co-located with ICNLSP 2019-Short\nPapers, pages 124\u2013130, 2019."
      },
      {
        "source_id": "source_34",
        "citation": "Understanding the Capabilities and Limitations of Large Language Models for Cultural Commonsense.pdf",
        "text_snippet": "ble) when queried in these other languages, thus in-\ndirectly diminishing the value that LLMs can have\nfor speakers of these languages. Besides, asking\nquestions in the language spoken in a given country\ndoes not necessarily help as we expect; instead, in\nmost cases, there is a significant benefit from ask-\ning questions about the cultural commonsense of a\ncountry in English.\nTo delve deeper into the effect of language on\nthe performance achieved by LLMs on this task,\nTable 3 shows the performance obtained with one\nmodel (GPT-3.5-turbo) when prompted with all\nfive languages. For all countries, we see a clear\nbenefit obtained by interaction in English. Also, on\naverage, interactions in Hindi lead to the worst per-\nformance, followed by Swahili as the second worst.\nOur results suggest that even the cultural relevance\nof the task does not fully mitigate performance dis-\nparities, where LLMs persist in exhibiting lower\nperformance in non-English languages as they do"
      },
      {
        "source_id": "source_01",
        "citation": "Voices Unheard_ NLP Resources and Models for Yoru\u0300ba\u0301 Regional Dialects.pdf",
        "text_snippet": "four regional Yor\u00f9b\u00e1 dialects. To develop this\ncorpus, we engaged native speakers, travelling\nto communities where these dialects are spo-\nken, to collect text and speech data. Using our\nnewly created corpus, we conducted extensive\nexperiments on (text) machine translation, au-\ntomatic speech recognition, and speech-to-text\ntranslation. Our results reveal substantial per-\nformance disparities between standard Yor\u00f9b\u00e1\nand the other dialects across all tasks. How-\never, we also show that with dialect-adaptive\nfinetuning, we are able to narrow this gap. We\nbelieve our dataset and experimental analysis\nwill contribute greatly to developing NLP tools\nfor Yor\u00f9b\u00e1 and its dialects, and potentially for\nother African languages, by improving our un-\nderstanding of existing challenges and offering\na high-quality dataset for further development.\nWe releaseYOR\u00d9LECT dataset and models pub-\nlicly under an open license 1.\n1 Introduction\nWhile great strides have been made in developing"
      },
      {
        "source_id": "source_06",
        "citation": "CultureVLM_ Characterizing and Improving Cultural Understanding of Vision-Language Models for over 100 Countries.pdf",
        "text_snippet": "CultureVLM-LLaVA: The image depicts the Mission San Xavier del Bac, a historic Spanish Catholic mission located in Arizona, USA.\u2026 Option (A) Native American and \nColonial Spanish: This option reflects the cultural synthesis present in the Mission San Xavier del Bac. The mission was established by Spanish colonizers and incorporates \nelements from both Spanish and indigenous cultures, evident in its architecture and artistic features. \u2026Option (D) Moorish and Romanesque: This option is also unrelated, \nas the mission does not exhibit characteristics of Moorish or Romanesque architecture. Based on the analysis, the cultural synthesis reflected in the architecture and artistic \nfeatures of the Mission San Xavier del Bac is best described by option (A) Native American and Colonial Spanish.\nQuestion: What traditional Chinese dish is shown in this image? \nOptions: (A) Baozi (B) Zongzi (C) Jiaozi (D) Shaomai\nDirect Output: B) Zongzi"
      },
      {
        "source_id": "source_01",
        "citation": "Voices Unheard_ NLP Resources and Models for Yoru\u0300ba\u0301 Regional Dialects.pdf",
        "text_snippet": "maka Chukwuneke, Christine Mwase, Clemencia\nSiro, Stephen Arthur, Tunde Ajayi, Verrah Otiende,\nAndre Rubungo, Boyd Sinkala, Daniel Ajisafe,\nEmeka Onwuegbuzia, Falalu Lawan, Ibrahim Ah-\nmad, Jesujoba Alabi, Chinedu Mbonu, Mofetoluwa\nAdeyemi, Mofya Phiri, Orevaoghene Ahia, Ruqayya\nIro, and Sonia Adhiambo. 2023. Cross-lingual open-\nretrieval question answering for African languages.\nIn Findings of the Association for Computational\nLinguistics: EMNLP 2023, pages 14957\u201314972, Sin-\ngapore. Association for Computational Linguistics.\nTolulope Ogunremi, Kola Tubosun, Anuoluwapo\nAremu, Iroro Orife, and David Ifeoluwa Adelani.\n2024. \u00ccr\u00f2y\u00ecnspeech: A multi-purpose yor\u00f9b\u00e1 speech\ncorpus.\nValentine Ojo. 1977. English-yoruba language contact\nin nigeria. (No Title).\nEmmanuel Om\u00f3n\u00edy\u00ec Ol\u00e1nrew\u00e1j\u00fa. 2022. A contrastive\nanalysis of interrogatives in standard yor\u00f9b\u00e0 and cen-\ntral yor\u00f9b\u00e0 dialects. Hayatian Journal of Linguistics\nand Literature, 6(1):24\u201346.\nTemitope Olumuyiwa. 2009. The high tone syllable in"
      }
    ],
    "time_taken": 0.92
  },
  {
    "question": "Does the 'WAXAL' paper discuss speech synthesis for Martian languages?",
    "answer": "Insufficient Evidence",
    "citations_readable": [
      "WAXAL_ A Large-Scale Multilingual African Language Speech Corpus.pdf"
    ],
    "citations_raw": [
      "source_25"
    ],
    "retrieved_chunks": [
      {
        "source_id": "source_19",
        "citation": "Bridging Gaps in Natural Language Processing for Yoru\u0300ba\u0301_ A  Systematic Review of a Decade of Progress and Prospects.pdf",
        "text_snippet": "Even though most studies tend to involve text processing, quite a substantial number of studies\nalso investigated speech processing in Yor` ub\u00b4 a NLP. This ranges from text-to-speech synthesis\n[87, 88, 89, 90], tone recognition for continuous speech [91], speech recognition [92, 93, 94],\ntext-to-speech analysis [95], speech-based gender recognition [96] and multi-tasks involving\nboth text-to-speech and speech-to-text [97], text-to-speech and language speech resource de-\nvelopment [98, 99, 36, 100, 101], speech synthesis and language pitch modelling [102], and two\nstudies exclusively focusing on speech corpus development [103, 104]. Whereas, [35] differs\nas it is based on corpus development for visually grounded speech. Furthermore, to optimize\nmodelling in speech recognition, acoustic unit discovery tasks [105, 106, 107] are essential in\nlearning speech sounds embedding to retain the linguistically relevant acoustic information and\ndiscard the irrelevant ones."
      },
      {
        "source_id": "source_25",
        "citation": "WAXAL_ A Large-Scale Multilingual African Language Speech Corpus.pdf",
        "text_snippet": "Avinatan Hassidim1, Jeff Dean1, Yossi Matias1\n1Google Research, 2University of Ghana, 3Digital Umuganda, 4University of Addis Ababa,\n5Makerere University, 6Loud and Clear Communications Ltd, 7Media Trust Limited, 8Gates Foundation\nABSTRACT\nThe advancement of speech technology has predomi-\nnantly favored high-resource languages, creating a significant\ndigital divide for speakers of most Sub-Saharan African lan-\nguages. To address this gap, we introduce W AXAL, a large-\nscale, openly accessible speech dataset for 21 languages rep-\nresenting over 100 million speakers. The collection consists\nof two main components: an Automated Speech Recogni-\ntion (ASR) dataset containing approximately 1,250 hours of\ntranscribed, natural speech from a diverse range of speakers,\nand a Text-to-Speech (TTS) dataset with over 180 hours of\nhigh-quality, single-speaker recordings reading phonetically\nbalanced scripts. This paper details our methodology for data"
      },
      {
        "source_id": "source_19",
        "citation": "Bridging Gaps in Natural Language Processing for Yoru\u0300ba\u0301_ A  Systematic Review of a Decade of Progress and Prospects.pdf",
        "text_snippet": "is a core component of these languages; other languages in the dataset include Arabic, English,\nFrench, Hausa, Igbo, Portuguese, and Swahili. A transformer-based multilingual pre-trained\nlanguage model was evaluated on the dataset, supporting empirical analysis in future related\nstudies.\nTTS Yor` ub\u00b4 a Speech Corpus: A comprehensive Yor` ub\u00b4 a speech corpus was designed pri-\nmarily for TTS synthesis research and development in the language [99]. The Yor` ub\u00b4 a speech\ncorpus contains 2 , 415 sentences with 46 , 117 words and 148 , 823 phonemes. The corpus has\na good balance of sentence types\u2014affirmative, interrogative, and exclamatory\u2014and phoneme\ndistribution. Furthermore, the Yor` ub\u00b4 a speech corpus was also integrated into the MaryTTS14\nopen-source multilingual text-to-speech (TTS) synthesis platform, which achieved a Mean Opin-\nion Score (MOS) of 2 .9 out of 5 for the quality of the synthesized speech."
      },
      {
        "source_id": "source_25",
        "citation": "WAXAL_ A Large-Scale Multilingual African Language Speech Corpus.pdf",
        "text_snippet": "and a Text-to-Speech (TTS) dataset with over 180 hours of\nhigh-quality, single-speaker recordings reading phonetically\nbalanced scripts. This paper details our methodology for data\ncollection, annotation, and quality control, which involved\npartnerships with four African academic and community\norganizations. We provide a detailed statistical overview\nof the dataset and discuss its potential limitations and eth-\nical considerations. The W AXAL datasets are released at\nhttps://huggingface.co/datasets/google/WaxalNLP under the\npermissive CC-BY-4.0 license to catalyze research, enable\nthe development of inclusive technologies, and serve as a\nvital resource for the digital preservation of these languages.\nIndex Terms\u2014African language, ASR, TTS, Speech,\ndataset\n1. INTRODUCTION\nThe proliferation of voice-enabled technologies, from vir-\ntual assistants to automated transcription services, has trans-\nformed human-computer interaction. However, the benefits"
      },
      {
        "source_id": "source_19",
        "citation": "Bridging Gaps in Natural Language Processing for Yoru\u0300ba\u0301_ A  Systematic Review of a Decade of Progress and Prospects.pdf",
        "text_snippet": "essential for pronunciation and lexical disambiguation. Moreover, the dataset was tested in\na statistical parametric speech synthesis (SPSS) for evaluation purposes and compared with\nrelated corpora in the same domain. The corpus will support TTS, ASR, and speech-to-speech\ntranslation, contributing to West African corpus linguistics.\n14https://marytts.github.io/\n15https://www.flickr.com\n25"
      },
      {
        "source_id": "source_01",
        "citation": "Voices Unheard_ NLP Resources and Models for Yoru\u0300ba\u0301 Regional Dialects.pdf",
        "text_snippet": "Marta R Costa-juss\u00e0, James Cross, Onur \u00c7elebi, Maha\nElbayad, Kenneth Heafield, Kevin Heffernan, Elahe\nKalbassi, Janice Lam, Daniel Licht, Jean Maillard,\net al. 2022. No language left behind: Scaling\nhuman-centered machine translation. arXiv preprint\narXiv:2207.04672.\nKaren Ruth Courtenay. 1969. A generative phonology\nof yor\u00f9b\u00e1.\nMona Diab. 2016. Processing dialectal Arabic: Exploit-\ning variability and similarity to overcome challenges\nand discover opportunities. In Proceedings of the\nThird Workshop on NLP for Similar Languages, Vari-\neties and Dialects (VarDial3), page 42, Osaka, Japan.\nThe COLING 2016 Organizing Committee.\nMona Diab and Nizar Habash. 2012. Arabic dialect\nprocessing tutorial. In Tutorial Abstracts at the Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, Montr\u00e9al, Canada. Association\nfor Computational Linguistics.\nCheikh M. Bamba Dione, David Ifeoluwa Adelani,"
      },
      {
        "source_id": "source_19",
        "citation": "Bridging Gaps in Natural Language Processing for Yoru\u0300ba\u0301_ A  Systematic Review of a Decade of Progress and Prospects.pdf",
        "text_snippet": "cross-lingual pairing, whereby images are tagged with English visual labels and paired with\nYor` ub\u00b4 a speech, permitting cross-lingual applications. The empirical analysis is also catered\nfor by developing a baseline cross-lingual model. Ultimately, this dataset addressed the dearth\nof visually-grounded speech datasets in low-resource languages, essentially focusing on Yor` ub\u00b4 a\nlanguage. Furthermore, it provides a new benchmark for visually-grounded speech models in\nlow-resource language settings.\nYor` ub\u00b4 a Speech Corpus: An open-source speech dataset exclusively for Yor` ub\u00b4 a developed\nin [36]. This corpus comprises over 4 hours of 48 kHz recordings from 36 male and female\nvolunteers. It also includes transcriptions with disfluency annotations and full diacritization,\nessential for pronunciation and lexical disambiguation. Moreover, the dataset was tested in\na statistical parametric speech synthesis (SPSS) for evaluation purposes and compared with"
      },
      {
        "source_id": "source_32",
        "citation": "Neural Machine Translation for Low-Resource Languages.pdf",
        "text_snippet": "[16] Laurent Besacier, Etienne Barnard, Alexey Karpov, and Tanja Schultz. 2014. Automatic speech recognition for\nunder-resourced languages: A survey. Speech Communication 56 (2014), 85\u2013100.\n[17] Alexandra Birch, Andrew Finch, Minh-Thang Luong, Graham Neubig, and Yusuke Oda. 2018. Findings of the Second\nWorkshop on Neural Machine Translation and Generation. In Proceedings of the 2nd Workshop on Neural Machine\nTranslation and Generation. 1\u201310.\n[18] Graeme Blackwood, Miguel Ballesteros, and Todd Ward. 2018. Multilingual Neural Machine Translation with\nTask-Specific Attention. In Proceedings of the 27th International Conference on Computational Linguistics . 3112\u20133122.\n[19] Su Lin Blodgett, Solon Barocas, Hal Daum\u00e9 III, and Hanna Wallach. 2020. Language (Technology) is Power: A Critical\nSurvey of \u201cBias\u201d in NLP. InProceedings of the 58th Annual Meeting of the ACL . 5454\u20135476."
      },
      {
        "source_id": "source_19",
        "citation": "Bridging Gaps in Natural Language Processing for Yoru\u0300ba\u0301_ A  Systematic Review of a Decade of Progress and Prospects.pdf",
        "text_snippet": "YFACC Speech-image dataset Cross-lingual keyword\nlocalisation\nMonolingual [35]\nYor` ub\u00b4 a Speech\nCorpus\nSpeech corpus Text-to-speech; auto-\nmatic speech recogni-\ntion; speech-to-speech\ntranslation\nMonolingual [36, 91],\n[105], [107]\nYor` ub\u00b4 a Tree-\nbank\nDependency treebank Dependency parsing\nand analysis\nMonolingual [41]\nYOR`ULECT Parallel text and\nspeech corpus\nMachine translation;\nautomatic speech\nrecognition; & speech-\nto-text translation\nMonolingual [34]\n4.2.4 RQ4: What are the major challenges in developing NLP solutions for\nYor` ub\u00b4 a?\nNatural language solution development for Yor` ub\u00b4 a faces multiple challenges limiting its growth\nas a low-resource language. Data synthesis in this study has helped identify five primary chal-\nlenges, including linguistic, technical, resource, cultural and societal factors, and evaluation\nand benchmarking challenges.\nLinguistic challenges: Yor` ub\u00b4 a language exhibits several linguistic properties that make NLP\n27"
      },
      {
        "source_id": "source_25",
        "citation": "WAXAL_ A Large-Scale Multilingual African Language Speech Corpus.pdf",
        "text_snippet": "able, they cover a limited number of languages and speakers.\nOur work directly addresses these gaps by introducing a\nlarge-scale, unified dataset for 21 Sub-Saharan African lan-\nguages. With approximately 1,250 hours of multi-speaker,\ntranscribed speech for ASR and over 180 hours of high-\nquality, single-speaker recordings for TTS, our contribution\nsignificantly expands the data available for this linguistically\ndiverse and technologically underserved region.\n3. DA TA COLLECTION METHODOLOGY\nThe W AXAL dataset was acquired through a multi-year effort\n(Jan 2021 - Mar 2024) funded by Google with involvement\nfrom several key partners in Africa, ensuring local expertise\nand community involvement: Makerere University, Uganda:\nCollected ASR data for 5 languages and TTS data for 4 lan-\nguages. University of Ghana: Collected ASR data for 6 lan-\nguages and TTS data for 2 languages. Digital Umuganda:\nCollected ASR data for 4 languages. Media Trust: Collected\nTTS data for 4 languages."
      }
    ],
    "time_taken": 1.25
  },
  {
    "question": "Find evidence for the claim that 'LLMs are perfect translators'.",
    "answer": "Insufficient Evidence. The context does not provide any information or evidence supporting the claim that 'LLMs are perfect translators'. Instead, it discusses the challenges and limitations of LLMs in multilingual scenarios, particularly for languages not covered in their pre-training, and the reliance on machine translation as a substitute for human translation.",
    "citations_readable": [
      "IrokoBench_ A New Benchmark for African Languages in the Age of Large  Language Models.pdf",
      "A Survey on Large Language Models with Multilingualism_ Recent Advances and New Frontiers.pdf",
      "Global MMLU_ Understanding and Addressing Cultural and Linguistic Biases  in Multilingual Evaluation.pdf"
    ],
    "citations_raw": [
      "source_08",
      "source_12",
      "source_26"
    ],
    "retrieved_chunks": [
      {
        "source_id": "source_26",
        "citation": "Global MMLU_ Understanding and Addressing Cultural and Linguistic Biases  in Multilingual Evaluation.pdf",
        "text_snippet": "specifically GPT-4o and Claude Sonnet 3.5. A more detailed description of the models covered\nis mentioned in the Appendix E.We note that all these models do not claim to support the same\nset of languages, and none claim to support the full set of languages we cover.\nEvaluation Setup. We use lm-evaluation-harness (Gao et al., 2024) to evaluate the open\nmultilingual models in a 5-shot setting. For closed models (i.e., GPT-4o and Claude-Sonnet\n3.5), we also do 5-shot evaluation. However, since log probabilities are not accessible via API\nfor closed models, we send the 5-shot prompt via API and get the corresponding generation\nfrom the model. We use a system preamble to make the model respond with only the correct\nanswer option and extract the answer from the output generation. For prompting, we follow the\nsame approach as specified in (Hendrycks et al., 2020) and use prompt instructions in the same\nlanguage as the sample.\n13"
      },
      {
        "source_id": "source_26",
        "citation": "Global MMLU_ Understanding and Addressing Cultural and Linguistic Biases  in Multilingual Evaluation.pdf",
        "text_snippet": "LLM instruction tuning and evaluation. Their findings reveal that models fine-tuned with native\ninstructions typically outperform those trained using translated data. Similarly, Choenni et al.\n(2024) investigates the reliability of machine translation as a substitute for human translation in\n8An acronym forSouthEast Asian Holistic Evaluation of Language Models.\n9https://leaderboard.sea-lion.ai\n22"
      },
      {
        "source_id": "source_08",
        "citation": "IrokoBench_ A New Benchmark for African Languages in the Age of Large  Language Models.pdf",
        "text_snippet": "pelling evidence for the effectiveness of our transla-\ntions (Amharic, Yor\u00f9b\u00e1, isiZulu). However, the per-\nformance of this metric depends on if the language\nwe are evaluating is covered in the pre-training of\nthe base model of the metric i.e. AfroXLMR-large.\nIn the case of Lingala, Twi and Wolof, the perfor-\nmance of the metric does not correlate with the\nhuman translation since they are not covered in\nAfroXLMR. Similar findings were reported in the\noriginal AfriCOMET QE paper that Twi had worse\ncorrelation with human judgement (i.e., 0.279 for\nPearson, and 0.060 for Spearman) (Wang et al.,\n2024).\nA.4 Task-specific results for all models\nWe provide the entire results of all LLMs\nand all prompts on AfriMMLU, AfriXNLI and\nAfriMGSM tasks are shown in Table 14, Table 13\nand Table 15. We performed evaluation on 5\nprompts for all models except Claude Opus which\nwe limit to one prompt due to API inference cost.\nA.5 Comparison between in-language and\ntranslate-test results"
      },
      {
        "source_id": "source_08",
        "citation": "IrokoBench_ A New Benchmark for African Languages in the Age of Large  Language Models.pdf",
        "text_snippet": "LLaMa 3.1 70B. These findings suggest that\nmore efforts are needed to develop and adapt\nLLMs for African languages.\n1 Introduction\nIn recent years, the capabilities of large language\nmodels (LLMs) have greatly improved, from co-\nherent chat experiences to solving complex and\nknowledge-intensive tasks like mathematical rea-\nsoning, coding, and question answering (QA) (Ope-\nnAI et al., 2024; Jiang et al., 2024; Gemini-Team\net al., 2024). These models have also demonstrated\nthe ability to quickly learn new and challenging\ntasks with few in-context learning examples and\nthrough chain-of-thought reasoning (Brown et al.,\n2020; Shi et al., 2022; Wei et al., 2022). However,\nmost state-of-the-art (SoTA) LLMs are primarily\ntrained on high-resource languages (HRLs), result-\ning in sub-optimal performance for languages un-\nseen during pre-training (Touvron et al., 2023; Ojo\net al., 2023). Furthermore, this language coverage\nbias is reflected in the evaluation stage, predomi-"
      },
      {
        "source_id": "source_08",
        "citation": "IrokoBench_ A New Benchmark for African Languages in the Age of Large  Language Models.pdf",
        "text_snippet": "one of the following options: \u2018\u2019entailment\u2019, \u2019contradiction\u2019, or \u2019neutral\u2019.\nPremise: {premise}\nHypothesis: {hypothesis}\nPrompt 5\nBased on the given statement, is the following claim \u2019true\u2019, \u2019false\u2019, or \u2019inconclusive\u2019.\nPremise: {premise}\nHypothesis: {hypothesis}\nTable 11: Five different prompt used for prompt sensitivity experiments in AfriXNLI\n18"
      },
      {
        "source_id": "source_12",
        "citation": "A Survey on Large Language Models with Multilingualism_ Recent Advances and New Frontiers.pdf",
        "text_snippet": "The bias of LLMs in the multilingual scenario can be divided into two categories: language bias and\ndemographic bias [28]. Intuitively, the former is due to the imbalance of available training corpus\nfor different languages [43, 457], where English possesses the most text corpus [462, 474, 475,\n476], resulting in the performance degradation of the LLMs when generalized to other language\nsettings [477, 478]. The demographic bias occurs due to embedded biases and misinformation\n25"
      },
      {
        "source_id": "source_16",
        "citation": "Where Are We? Evaluating LLM Performance on African Languages.pdf",
        "text_snippet": "1SAHARA is publicly available at https://github.com/UBC-\nNLP/sahara.\nbenchmark that we collect using mostly existing\ndatasets to unravel how current African language\npolicies, by dictating data availability and accessi-\nbility, directly shape model performance across\nthe continent\u2019s diverse linguistic spectrum. By\nsystematically analyzing the distribution of exist-\ning datasets across languages and performance pat-\nterns among various LLMs, we not only pinpoint\nwhich languages benefit from ample data resources\nbut also illuminate the underlying factors that con-\ntribute to their relative success. Our investigation\nreveals that the disparities in NLP outcomes are\nclosely tied to policy-driven data inequities, offer-\ning concrete evidence of the need for more inclu-\nsive, forward-thinking language policies. These\ninsights provide actionable recommendations for\nenhancing dataset creation and model training, ul-\ntimately aiming to bridge the digital divide and"
      },
      {
        "source_id": "source_12",
        "citation": "A Survey on Large Language Models with Multilingualism_ Recent Advances and New Frontiers.pdf",
        "text_snippet": "among various languages and integrating language-specific domain knowledge into LLMs. For ex-\nample, disparities in legal definitions between European Council and USA jurisdictions, as well as\nthe contrast between traditional Chinese medicine and Western medicine, indicate these challenges.\nCurrent LLMs face challenges in effectively understanding such language-specific knowledge, hin-\ndering their capacity to provide customized domain-specific assistance for diverse populations. Fur-\nther research is necessary to explore how LLMs can integrate and leverage this particular type of\nknowledge.\nThese limitations highlight the need for further research efforts in the following directions:\n\u2022 Development of strategies to construct high-quality, domain-specific multilingual datasets\nthat preserve cultural context.\n\u2022 Exploration of techniques for LLMs to effectively integrate and leverage language-specific\nin-domain knowledge."
      },
      {
        "source_id": "source_08",
        "citation": "IrokoBench_ A New Benchmark for African Languages in the Age of Large  Language Models.pdf",
        "text_snippet": "approach. To extract the correct answers for the\ntask, we employed a verbalizer (Gao et al., 2021;\nSchick and Sch\u00fctze, 2021). For AfriMGSM, we\nused the default verbalizer provided by the tool.\nHowever, for AfriXNLI and AfriMMLU, we man-\nually created a verbalizer for the closed models and\nused the log-likelihood request type for the open\nmodels.\nThe prompt templates used for evaluation of dif-\nferent tasks are in Table 11, Table 12 and Table 10.\nA.3 AfriCOMET metric scores for XNLI\ntranslation\nWe employ AfriCOMET evaluation metrics, as de-\nveloped by Wang et al. (2024), to automatically\nassess the quality of translations for our newly cre-\nated benchmarks. Figure 3 depicts the histogram of\nscores obtained from AfriCOMET for AfriXNLI,\nillustrating promising results and offering com-\npelling evidence for the effectiveness of our transla-\ntions (Amharic, Yor\u00f9b\u00e1, isiZulu). However, the per-\nformance of this metric depends on if the language"
      },
      {
        "source_id": "source_08",
        "citation": "IrokoBench_ A New Benchmark for African Languages in the Age of Large  Language Models.pdf",
        "text_snippet": "all accuracy, where 13 out of 16 African lan-\nguages achieved at least 70% despite struggling\nwith AfriMGSM. The performance difference to\nAfriMGSM may be due to the multi-choice options\nof MMLU, which may be slightly easier than free-\nform answer. International law also achieved im-\npressive performance with 10 out of 16 languages\nachieving 70%. All languages struggle the most\nwith Global facts including English and French.\nSimilarly, many African languages find it difficult\nto answer questions in geography and microeco-\nnomics subjects. This presents an opportunity for\nimproving LLMs for the education domain.\nAfriMGSM performance receives significant\nboost with translate-test for open models Ta-\nble 5 shows that we can achieve a significant boost\nin performance with the translate-test on all lan-\nguages we evaluated. We hypothesize that the cur-\nrent LLMs are better at reasoning in English than\n7"
      }
    ],
    "time_taken": 2.17
  },
  {
    "question": "What is the specific learning rate used in the 'DeepSeek-V3' paper?",
    "answer": "Insufficient Evidence",
    "citations_readable": [],
    "citations_raw": [],
    "retrieved_chunks": [
      {
        "source_id": "source_06",
        "citation": "CultureVLM_ Characterizing and Improving Cultural Understanding of Vision-Language Models for over 100 Countries.pdf",
        "text_snippet": "capacity. For LLaV A-1.5, a learning rate of2 \u00d7 10\u22125 is used, with no weight decay applied (0.0). The learning rate\nfollowed a cosine schedule, gradually increasing during the initial phase with a warmup ratio of 0.03. For Phi-3, we use\na learning rate of 4 \u00d7 10\u22125 and a weight decay of 0.01. A linear learning rate scheduler is utilized, with 50 warmup\nsteps to stabilize the early training stage. For LLaMA-3.2, fine-tuning is conducted using a learning rate of 1 \u00d7 10\u22125\nwith no weight decay (0.0). A multiplicative learning rate decay is applied after each epoch, with a gamma value of0.85.\nThe batch sizes are set to 64, 16 and 32 respectively. All models are trained for one epoch on the training set and fully\nfine-tuned on 4\u00d7A100 80GB GPUs. For the training data, although we do not conduct large-scale human annotation, we\nsynthesize the data using only concepts that passed either GPT-4o or human quality assurance, significantly improving"
      },
      {
        "source_id": "source_06",
        "citation": "CultureVLM_ Characterizing and Improving Cultural Understanding of Vision-Language Models for over 100 Countries.pdf",
        "text_snippet": "complex, and in some cases, GPT-4o refused to provide answers, making it impossible to generate valid questions. For\nimage recognition questions, we directly use the questions and options as prompts. For cultural knowledge and scene\nreasoning questions, we employ stepwise reasoning prompts to facilitate the reasoning explanation. The prompts are\navailable in the Appendix E. For all proprietary models, we utilize the default hyper-parameters, setting the temperature\nto 0 and the max tokens to 1,024. For all open-source models, do_sample is set to False, max_gen_len is set to 512,\nand the temperature is set to 0.01.\nTraining Setup. We use the official train ing scripts of LLaV A2, Phi3, and LLaMA4 for model training, largely adhering\nto the original hyperparameters, except for appropriately adjusting the batch size to accommodate the GPU memory\ncapacity. For LLaV A-1.5, a learning rate of2 \u00d7 10\u22125 is used, with no weight decay applied (0.0). The learning rate"
      },
      {
        "source_id": "source_11",
        "citation": "Language model pretraining for low resource languages.pdf",
        "text_snippet": "2007). We use fastBPE2to learn BPE (Byte pair\nencoding) (Bojanowski et al.,2017) with 32k BPE\ncodes over the combined tokenized data of both lan-\nguages. For de$dsb, we use the same vocabulary\nand codes learnt for de$hsb.\n4.2 Experimental Setup\nWe use 6 layers in the encoder and decoder with 8\nattention heads and 1024 embedding dimension.\nWe use Adam (Kingma and Ba,2015) optimizer.\nWe use, a warm-up phase of 4000 steps with initial\nlearning rate starting from1e\u00007to1e\u00004, in the\nwarm-up phase learning rate is increased linearly\nand then starts to decrease with inverse square\nroot learning rate schedule. We use mini-batches\nof size 2000 tokens and set the dropout to\n0.1 (Gal and Ghahramani,2016). Maximum\nsentence length is set to 100 after applying BPE.\nAt the time of decoding, we set beam size to\n1. For experiments, we are using MASS3codebase.\n1http://statmt.org/wmt21/\ntranslation-task.html2https://github.com/glample/fastBPE3https://github.com/microsoft/MASS"
      },
      {
        "source_id": "source_31",
        "citation": "A Survey on Multilingual Large Language Models_ Corpora, Alignment, and  Bias.pdf",
        "text_snippet": "Harley T, Silver D, Kavukcuoglu K. Asynchronous\nmethods for deep reinforcement learning. In: Pro-\nceedings of the 33rd International Conference on In-\nternational Conference on Machine Learning. 2016,\n1928\u20131937\n67. French R M. Catastrophic forgetting in connectionist\nnetworks. Trends in Cognitive Sciences, 1999, 3(4):\n128\u2013135\n68. Hedderich M A, Lange L, Adel H, Str\u00f6tgen J, Klakow\nD. A survey on recent approaches for natural language\nprocessing in low-resource scenarios. In: Proceedings\nof the 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics.\n2021, 2545\u20132568\n69. Alabi J O, Adelani D I, Mosbach M, Klakow D.\nAdapting pre-trained language models to african lan-\nguages via multilingual adaptive fine-tuning. In:\nProceedings of the 29th International Conference on\nComputational Linguistics. 2022, 4336\u20134349\n70. Wongso W, Lucky H, Suhartono D. Pre-trained\ntransformer-based language models for sundanese.\nJournal of Big Data, 2022, 9(1): 39"
      },
      {
        "source_id": "source_13",
        "citation": "No Language Left Behind_ Scaling Human-Centered Machine Translation.pdf",
        "text_snippet": "maximum sequence length during training is 512 for both the encoder and the decoder.\nWe use the Adam optimizer (Kingma and Ba, 2015) with \u03b21 = 0.9 and \u03b22 = 0.98 and \u03f5\n= 10\u22126. For training efficiency, we use memory efficient FP16 for training as implemented\nin the fairseq library and also maintain Adam optimizer states in FP16 (Dhariwal et al.,\n2020). We linearly increase the learning rate from 10\u22127 to 0.002 for 8000 warmup steps\nand then follow the inverse square root learning rate schedule. The loss is cross-entropy\nwith label smoothing of 0.1 (Szegedy et al., 2015). For balancing expert utilization, we use\nthe additional load balancing loss as described in Section 6.2 with a weight of 0.01. The\ncapacity factor of each expert is set to 2, i.e., each expert can process up to 2 \u00d7 T /E tokens,\nwhere T is the number of tokens in the mini-batch and E is the number of experts. During\ngeneration, we set the capacity to be equal to the batch size so that all tokens can be routed"
      },
      {
        "source_id": "source_13",
        "citation": "No Language Left Behind_ Scaling Human-Centered Machine Translation.pdf",
        "text_snippet": "root learning rate schedule. For Top-2-Gating, we set the expert capacity to 2 \u00d7 T /E, i.e.,\nwe enforce that each expert processes, at most, 2 \u00d7 T /E tokens, where T is the number of\ntokens in the mini-batch and E is the number of experts. During generation, we set the\ncapacity to T so that all tokens can be routed to whichever expert they choose. We use the\nchrF+ + metric to compare the model performance (see Section 7.1).\nResults. In Table 15, we see that the Sparsely Gated MoE model with 64 experts (MoE-64),\nwhile computationally similar, shows good improvements when compared to the dense 615M\nmodel. We see 1+ chrF + + score improvements on all subsets except for very low resource\npairs (v.low) and non-English pairs ( xx-yy). There are worse trends when scaling up the\ncomputational cost per update \u2014 for the MoE-64 model (computationally similar to the\ndense 1.3B model), we see neutral or worse performance compared to the dense 1.3B model."
      },
      {
        "source_id": "source_31",
        "citation": "A Survey on Multilingual Large Language Models_ Corpora, Alignment, and  Bias.pdf",
        "text_snippet": "bias is naturally present in cross-cultural datasets\nand the design of MLLMs\u2019 modeling processes\n[12]. Previous studies have explored bias in vari-\nous NLP tasks and demographic groups \uff0cbut are\nlargely specific to English-based models [13, 14],\nwhich cannot be generalized to other languages.\nWhat are the types of bias in existing MLLMs?\nWhat are the main de-biasing techniques available\nfor MLLMs? Does the removal of these biases af-\nfect LLMs\u2019 performance? What are the existing\nbias evaluation datasets for MLLMs? These are\nvery worthwhile research questions. This survey\ntries to answer these questions and o ffers valuable\ninsights for bias on MLLMs.\nThe contributions of this survey are as follows:\n\u2022 We present an overview of MLLMs and\nanalyze the language imbalance challenge\nwithin MLLMs, their capacity to support\nlow-resource languages and their potential\nfor cross-lingual transfer learning.\n\u2022 We provide an overview of the multilin-\ngual datasets and corpora utilized by existing"
      },
      {
        "source_id": "source_06",
        "citation": "CultureVLM_ Characterizing and Improving Cultural Understanding of Vision-Language Models for over 100 Countries.pdf",
        "text_snippet": "Phi-3-vision 4B 87.53 90.84 95.77 90.80 91.56 91.41 90.91 91.16 92.83 90.50 92.35\nLLaMA-3.2-Vision 11B89.13 91.49 96.20 91.99 92.24 91.82 93.08 91.34 93.20 91.78 92.50\nTable 4: Performance of fine-tuned models across three tasks, five continents, and three categories. CHT denotes\nCultural Heritage and Traditions; HL denotes History and Landmarks; and NELR denotes Natural Environment and\nLocal Resources.\nGreedy Search 0.01 0.1 0.5 1.0\nTemperature\n40\n60\n80Performance (%)\nLLaVA\nLLaMA\nCultureVLM-LLaVA\nCultureVLM-LLaMA\nFigure 9: Impact of different decoding temperatures on performance\nC Details of Human Annotation\nC.1 Statistics of Human Annotators and the Process\nTable 8 shows the statistics of human annotators in our study. In total, through the contractor company, we hired10\nexpert annotators whose ages are between 20 and 36 with at least a bachelor\u2019s degree. Most of them are within the"
      }
    ],
    "time_taken": 1.28
  },
  {
    "question": "Does the corpus contain the personal email address of the author 'Adebara'?",
    "answer": "Insufficient Evidence",
    "citations_readable": [],
    "citations_raw": [],
    "retrieved_chunks": [
      {
        "source_id": "source_20",
        "citation": "Prompting Towards Alleviating Code-Switched Data Scarcity in  Under-Resourced Languages with GPT as a Pivot.pdf",
        "text_snippet": "sentences are one-dimensional with\u223c80% of sen-\ntences starting with a singular personal pronoun:\n\u2018Ek\u2019(English: \u2018I\u2019) (Section 4.2.1). This creates the\nopportunity to explore ways of adding diversity to\nthe type of sentence through the use of basic lin-\nguistic guidelines (such as specifying pronouns)\nwhich is discussed in the following section.\n3.1.2. Linguistic-Based Prompting\nSince the word lists contain nouns, verbs and ad-\njectives related to specific topics, content diversity\nin the sentences is addressed. These are also\nwords that are most typically code-switched (Kodali\net al., 2022). To add further diversity in the type of\nsentence, we add basic linguistic guidelines in the\nform of varying pronouns (personal, impersonal,\ninterrogative etc.), tenses (past, present and future\nthat alters the verb) and using negative particles.\nThe inclusion of negative particles is randomly ini-\ntialised and not in each prompt. We also impose\na rule that conjunctions must be in the matrix lan-"
      },
      {
        "source_id": "source_13",
        "citation": "No Language Left Behind_ Scaling Human-Centered Machine Translation.pdf",
        "text_snippet": "https://aclanthology.org/W19-3632.\nAhmed Abdelali, Francisco Guzman, Hassan Sajjad, and Stephan Vogel. The AMARA corpus:\nBuilding parallel language resources for the educational domain. In Proceedings of the\nNinth International Conference on Language Resources and Evaluation (LREC\u201914) , pages\n1856\u20131862, Reykjavik, Iceland, May 2014. European Language Resources Association\n(ELRA). URL http://www.lrec-conf.org/proceedings/lrec2014/pdf/877_Paper.\npdf.\nSadaf Abdul-Rauf and Holger Schwenk. On the Use of Comparable Corpora to Improve SMT\nperformance. In EACL, pages 16\u201323, 2009. URL http://www.aclweb.org/anthology/\nE09-1003.\nDavid Adelani, Dana Ruiter, Jesujoba Alabi, Damilola Adebonojo, Adesina Ayeni, Mofe\nAdeyemi, Ayodele Esther Awokoya, and Cristina Espa\u00f1a-Bonet. The effect of domain\nand diacritics in Yoruba\u2013English neural machine translation. In Proceedings of the\n18th Biennial Machine Translation Summit (Volume 1: Research Track) , pages 61\u201375,"
      },
      {
        "source_id": "source_19",
        "citation": "Bridging Gaps in Natural Language Processing for Yoru\u0300ba\u0301_ A  Systematic Review of a Decade of Progress and Prospects.pdf",
        "text_snippet": "the following sections: Bibliographic information, research context, NLP focus, language scope,\nresults and evaluation, and contributions. The sections, in turn, contain individual elements\nthat constitute all the extraction items. Pilot extractions were carried out with fewer samples\nto verify the results and update the form based on the need or irrelevance of variables to be\nextracted. For this study, the \u201cresearch country\u201d and \u201cresearch continent\u201d are taken as the\naffiliation of the lead author at the time of their research publication. The final extraction\ntemplate is given in Table 4.\nTable 4: Data Extraction Template\nVariable Extraction Item\nV1 Study ID\nV2 Article Title\nV3 Publication Year\nV4 Research Country\nV5 Research Continent\nV6 Publication Type\nV7 Publication Source\nV8 NLP Task\nV9 NLP Technique\nV10 Language Scope\nV11 Dataset\nV12 Citation Count\n3.6 Data Synthesis\nThis section involves integrating and interpreting information extracted from the selected stud-"
      },
      {
        "source_id": "source_20",
        "citation": "Prompting Towards Alleviating Code-Switched Data Scarcity in  Under-Resourced Languages with GPT as a Pivot.pdf",
        "text_snippet": "able]: https://www.gov.za/documents/\nprotection-personal-information-act\n[Accessed: 4 May 2023].\nAnkit Srivastava, Vijendra Singh, and Gur-\ndeep Singh Drall. 2019. Sentiment analysis of\ntwitter data.International Journal of Healthcare\nInformation Systems and Informatics, 14:1\u201316.\nEwald Van der Westhuizen and Thomas Niesler.\n2018. A first South African corpus of multilingual\ncode-switched soap opera speech. InProceed-\nings of the Eleventh International Conference\non Language Resources and Evaluation (LREC\n2018),Miyazaki,Japan.EuropeanLanguageRe-\nsources Association (ELRA).\nOndene van Dulm. 2007.The grammar of English-\nAfrikaanscodeswitching . PhDDissertation,Rad-\nboud Universiteit Nijmegen.\nEwald Van Der Westhuizen and Thomas Niesler.\n2017. Synthesising isizulu-english code-switch\nbigramsusingwordembeddings. In Proceedings\nof Interspeech 2017.\nJules White, Quchen Fu, Sam Hays, Michael Sand-\nborn, Carlos Olea, Henry Gilbert, Ashraf El-\nnashar, Jesse Spencer-Smith, and Douglas C."
      },
      {
        "source_id": "source_13",
        "citation": "No Language Left Behind_ Scaling Human-Centered Machine Translation.pdf",
        "text_snippet": "Philipp Koehn, Francisco Guzm\u00e1n, Vishrav Chaudhary, and Juan Pino. Findings of the WMT\n2019 shared task on parallel corpus filtering for low-resource conditions. In Proceedings of\nthe Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2) ,\n2019.\nPhilipp Koehn, Vishrav Chaudhary, Ahmed El-Kishky, Naman Goyal, Peng-Jen Chen,\nand Francisco Guzm\u00e1n. Findings of the WMT 2020 shared task on parallel corpus\nfiltering and alignment. In Proceedings of the Fifth Conference on Machine Translation ,\npages 726\u2013742, Online, November 2020. Association for Computational Linguistics. URL\nhttps://aclanthology.org/2020.wmt-1.78.\nBruce Kogut and Anca Metiu. Open-source software development and distributed innovation.\nOxford review of economic policy , 17(2):248\u2013264, 2001.\nAnastasia Kozyreva, Philipp Lorenz-Spreen, Ralph Hertwig, Stephan Lewandowsky, and\nStefan M Herzog. Public attitudes towards algorithmic personalization and use of personal"
      },
      {
        "source_id": "source_13",
        "citation": "No Language Left Behind_ Scaling Human-Centered Machine Translation.pdf",
        "text_snippet": "References\nJulien Abadji, Pedro Javier Ortiz Su\u00e1rez, Laurent Romary, and Beno\u00eet Sagot. Towards\na cleaner document-oriented multilingual crawled corpus. CoRR, abs/2201.06642, 2022.\nURL https://arxiv.org/abs/2201.06642.\nSolomon Teferra Abate, Michael Melese, Martha Yifiru Tachbelie, Million Meshesha, Solomon\nAtinafu, Wondwossen Mulugeta, Yaregal Assabie, Hafte Abera, Binyam Ephrem Seyoum,\nTewodros Abebe, et al. Parallel corpora for bi-directional statistical machine translation\nfor seven ethiopian language pairs. In Proceedings of the First Workshop on Linguistic\nResources for Natural Language Processing , pages 83\u201390, 2018.\nJade Abbott and Laura Martinus. Benchmarking neural machine translation for Southern\nAfrican languages. In Proceedings of the 2019 Workshop on Widening NLP , pages\n98\u2013101, Florence, Italy, August 2019. Association for Computational Linguistics. URL\nhttps://aclanthology.org/W19-3632.\nAhmed Abdelali, Francisco Guzman, Hassan Sajjad, and Stephan Vogel. The AMARA corpus:"
      },
      {
        "source_id": "source_19",
        "citation": "Bridging Gaps in Natural Language Processing for Yoru\u0300ba\u0301_ A  Systematic Review of a Decade of Progress and Prospects.pdf",
        "text_snippet": "Table 3: Quality Assessment Checklist\nDomain Question\nResearch objective Are the objectives of the study clearly stated?\nMethodology and study design Are the study\u2019s methods and experimental design clearly defined?\nResearch documentation Are the study\u2019s processes comprehensively documented?\nResearch Question Alignment Are the research questions answered through the findings?\nStudy conclusion Do the conclusions of the study relate to its objectives?\nResult evaluation Does the study validate the results using standard evaluation met-\nrics?\nLimitations and bias Are the limitations of the study acknowledged?\nNovelty and contribution Does the study contribute new tools, resources, insights, or inves-\ntigate new questions?\nFigure 5: Histogram of total quality score for each study\n12"
      },
      {
        "source_id": "source_17",
        "citation": "AfriMTE and AfriCOMET_ Enhancing COMET to Embrace Under-resourced African Languages.pdf",
        "text_snippet": "holders. They contributed to the data and gave\ntheir consent to use this data for the enhancement\nof COMET models for African languages. Upon\nthe data collected, there is no privacy concern since\nthe source of the data is based on Wikipedia general\ndomain.\nAcknowledgments\nDavid Adelani acknowledges the support of Deep-\nMind Academic Fellowship programme. Ricardo\nRei is supported by the European Union\u2019s Hori-\nzon Europe Research and Innovation Actions (UT-\nTER: contract 101070631) and by the Portuguese\nRecovery and Resilience Plan through project\nC645008882-00000055 (Center for Responsible\nAI). Pontus Stenetorp would like to acknowledge\nthe helpful proofing feedback from several viewers\nwhile finalizing the submission. We are grateful to\nProf. Antonios Anastasopoulos from George Ma-\nson University for releasing the \u201cWMT African\u201d\ndataset for our experiments. This work is sup-\nported in part by Oracle Cloud credits and re-\nlated resources provided by Oracle. This work"
      },
      {
        "source_id": "source_02",
        "citation": "On The Origin of Cultural Biases in Language Models_ From Pre-training Data to Linguistic Phenomena.pdf",
        "text_snippet": "were thus manually filtered out. For example,\nin addition to articles about food dishes, the\n\u201c[country adjective] cuisine \" category\nwould also contain articles about particular chefs\nor restaurants in that country. We finally inspect\nthe titles of the remaining articles and remove any\nadditional text between parentheses that is not part\nof the entity (e.g., a title such as \u201c Mandi (food)\"\nwhere (food) was manually removed).\nFor author names, the number of articles in the\nauthors category was very large. We thus took a\nrandom sample of 3k articles from Arab countries\nand 3k articles from Western countries that were\nthen manually filtered by the annotators.\nTo collect first names, we first extract all article\n0 200 400 600 800 1000\nFood\n326 457\n251 564\n685\nArab (Wikidata/CC)\nArab (Wikipedia)\nArab (OSM)\nWestern (Wikidata/CC)\nWestern (Wikipedia)\nOther (Wikipedia)\n0 500 1000 1500 2000 2500 3000 3500 4000\nSports\nReligious\n1270\n1527\n1013\n2232\n1230\n901\n918\n389\n711"
      },
      {
        "source_id": "source_19",
        "citation": "Bridging Gaps in Natural Language Processing for Yoru\u0300ba\u0301_ A  Systematic Review of a Decade of Progress and Prospects.pdf",
        "text_snippet": "a peaceful office setting to ensure quality in the corpus. Ultimately, this corpus will be relevant\nfor various NLP tasks in Yor` ub\u00b4 a, such as TTS, ASR, etc.\nMasakhaNER: MaskhaNER [12] boasts as the first largest high-quality dataset for named\nentity recognition tasks focused on African languages, in which Yor` ub\u00b4 a was greatly empha-\nsised. The dataset scope extends to 10 African languages and across four language families,\nand they include Yor` ub\u00b4 a, Amharic, Hausa, Igbo, Kinyarwanda, Wolof, Nigerian-Pidgin, Luo,\n23"
      }
    ],
    "time_taken": 0.92
  }
]