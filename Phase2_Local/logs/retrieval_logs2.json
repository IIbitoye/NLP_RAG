[
  {
    "question": "What specific failures does the 'AfroBench' paper identify in current LLMs?",
    "answer": "The 'AfroBench' paper identifies several specific failures in current LLMs, including large performance gaps between high-resource languages like English and African languages across most tasks. The performance also varies based on the availability of monolingual data resources, with languages having more data performing better. Additionally, the paper highlights that current LLMs, especially open models, are biased towards English and a few high-resource languages, and there is a need for better adaptation techniques to improve performance on African languages.",
    "citations_readable": [
      "AfroBench_ How Good are Large Language Models on African Languages?.pdf"
    ],
    "citations_raw": [
      "source_33"
    ],
    "retrieved_chunks": [
      {
        "source_id": "source_33",
        "citation": "AfroBench_ How Good are Large Language Models on African Languages?.pdf",
        "text_snippet": "(2) Evaluation of LLMs needs to be continuous\nsince many new LLMs have been released with im-\nproved multilingual abilities, but a comprehensive\nevaluation is not available for African languages.\nIn this paper, we address the challenges of pre-\nvious large-scale LLM evaluation by introduc-\ning a new carefully curated benchmark known as\nAFROBENCH which comprises 15 tasks, 22 eval-\nuation data, and 64 indigeneous African lan-\nguages. AFROBENCH consists of nine natural\nlanguage understanding tasks, six text generation\ntasks, six knowledge and question answering tasks,\nand one mathematical reasoning task. Finally, we\ncreated a new evaluation datasets, AFRI ADR for\ndiacritic restoration of tonal marks and accents on\nAfrican language texts. Leveraging AFROBENCH ,\nwe conduct an extensive analysis of the perfor-\nmance of LLMs for African languages from differ-\nent language families and geographical locations.\nFor our evaluation, we compute the aver-"
      },
      {
        "source_id": "source_33",
        "citation": "AfroBench_ How Good are Large Language Models on African Languages?.pdf",
        "text_snippet": "and tasks. To address these challenges, we\nintroduce AFROBENCH \u2014a multi-task bench-\nmark for evaluating the performance of LLMs\nacross 64 African languages, 15 tasks and 22\ndatasets. AFROBENCH consists of nine natural\nlanguage understanding datasets, six text gen-\neration datasets, six knowledge and question\nanswering tasks, and one mathematical reason-\ning task. We present results comparing the\nperformance of prompting LLMs to fine-tuned\nbaselines based on BERT and T5-style models.\nOur results suggest large gaps in performance\nbetween high-resource languages, such as En-\nglish, and African languages across most tasks;\nbut performance also varies based on the avail-\nability of monolingual data resources. Our find-\nings confirm that performance on African lan-\nguages continues to remain a hurdle for current\nLLMs, underscoring the need for additional\nefforts to close this gap.1\n1 Introduction\nLarge language models (LLMs) have risen to the"
      },
      {
        "source_id": "source_33",
        "citation": "AfroBench_ How Good are Large Language Models on African Languages?.pdf",
        "text_snippet": "still beneficial since the capabilities of LLMs lags\nbehind. We provide task and per-language results\nin Appendix A and I.\n5.2 AfroBench-Lite Evaluation\nIn the AFROBENCH -LITE evaluation, we restrict\nthe evaluation to seven LLMs, and seven tasks, and\ncompare performance gap to English.\nLarge gap in performance when compared to\nEnglish One striking observation is that open\nmodels such as LlaMa 3.1 70B and Gemma 2 27B\n78-shot samples is the standard setting for MGSM datasets\n6"
      },
      {
        "source_id": "source_16",
        "citation": "Where Are We? Evaluating LLM Performance on African Languages.pdf",
        "text_snippet": "2024b) evaluated LLMs on African languages, re-\nvealing stark performance gaps between high- and\nlow-resource languages. Adebara et al. (2022) de-\nveloped AfroLID, a neural language identification\nmodel covering 517 African languages, showing"
      },
      {
        "source_id": "source_08",
        "citation": "IrokoBench_ A New Benchmark for African Languages in the Age of Large  Language Models.pdf",
        "text_snippet": "prises three datasets focused on different tasks: nat-\nural language inference (AfriXNLI), multi-choice\nknowledge QA (AfriMMLU), and mathematical\nreasoning (AfriMGSM). Unlike previous bench-\nmarks, which primarily involve simple text classi-\nfication tasks, these datasets assessed the LLMs\u2019\nabilities in complex and knowledge-intensive ar-\neas. Our evaluation revealed a significant perfor-\nmance gap between high-resource languages (e.g.,\nEnglish and French) and African languages. Ad-\nditionally, we observed a substantial disparity in\nperformance between open models and proprietary\nmodels, with the latter generally outperforming\nthe former, particularly in mathematical reasoning\ntasks. We hope that IROKO BENCH will serve as\na valuable benchmark for evaluating future LLMs\ndeveloped or adapted for African languages.\nLimitations Our benchmark has a few limita-\ntions: (1) The benchmark is human-translated\nwhich may include some translationese effects, it"
      },
      {
        "source_id": "source_33",
        "citation": "AfroBench_ How Good are Large Language Models on African Languages?.pdf",
        "text_snippet": "points on AFROBENCH -L ITE. These results sug-\ngest that current LLMs especially the open models,\nare more biased towards English and a few high-\nresource languages. Adapting LLMs for a region\nof African languages could help bridge the gap.\nFor instance, we see that continually pre-training\nLLaMa 3 8B, that resulted in LLaMaX 8B shows\nslight overall performance of +1.4 or more over\nvanilla LLaMa 3 8B in Table 4. However, to further\nboost performance, better adaptation techniques are\nneeded.\nPerformance varies across languages Figure 3\nshows the results for per-language performance\nscores of 14 languages in AFROBENCH -LITE. Our\nresult shows that performance correlates with the\navailable monolingual text on the web (Kudugunta\net al., 2023). We find that Swahili (swa) with over\n2.4GB of monolingual text has the highest perfor-\nmance among the African languages, while Wolof\nwith the smallest monolingual data (5MB) has the\nlowest performance. While this data size estimates"
      },
      {
        "source_id": "source_09",
        "citation": "The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks.pdf",
        "text_snippet": "The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks\nbegun addressing these issues by incorporating more diverse cultural perspectives [Li et al., 2024, Son\net al., 2024, Zhao et al., 2024a, Chiu et al., 2024], but there remains significant room for benchmarks\nthat assess models on their ability to handle locally meaningful applications.\nLLM-as-a-Judge Recent research has demonstrated the potential of using LLMs themselves as\nevaluation tools for assessing the quality of model-generated text in English [Zheng et al., 2023,\nDubois et al., 2024]. This approach offers promising opportunities for multilingual evaluation by\nextending these techniques across diverse languages and tasks. However, deploying LLMs as judges\nin multilingual contexts also introduces unique challenges, including potential evaluation biases that\nmirror the language disparities in the judge models themselves and questions about their cross-lingual"
      },
      {
        "source_id": "source_12",
        "citation": "A Survey on Large Language Models with Multilingualism_ Recent Advances and New Frontiers.pdf",
        "text_snippet": "\u2022 Unified framework. The number of benchmarks is sufficient but there is a lack of an authori-\ntative and unified evaluation framework, which can be updated over time, and the evaluation\naspect is more comprehensive based on the proposed framework. This phenomenon can be\nattributed to the dominance of English and individuals\u2019 focus on their native languages. To\naddress this issue, collaborative efforts from the multilingual community are essential.\n\u2022 Data leakage. The main differences between LLMs lie in the training data and model size.\nThe existing systems primarily prioritize real focus on user experience (open test), rather\nthan assessing effectiveness through closed test sets that align with fair training data. Thus,\nthere is a potential for data leakage as models may have inadvertently learned from the test\nset, especially the closed LLMs. It requires the evaluation methods to adopt a more secure\nstrategy to reduce the risk of data leakage."
      },
      {
        "source_id": "source_08",
        "citation": "IrokoBench_ A New Benchmark for African Languages in the Age of Large  Language Models.pdf",
        "text_snippet": "but they do propagate errors of machine transla-\ntion (MT) engines. Moreover, this is not applicable\nto low-resource languages with low-quality MT\nsystems (Adelani et al., 2022a; Costa-juss\u00e0 et al.,\n2024). Our research advances this by evaluating\nLLMs on more complex tasks using newly devel-\noped, human-annotated benchmarks specifically\nfor African languages.\nAfrican Benchmark Datasets: Due to the lim-\nited representation of African languages in the field\nof NLP, there has been a growing effort to create\n2"
      },
      {
        "source_id": "source_33",
        "citation": "AfroBench_ How Good are Large Language Models on African Languages?.pdf",
        "text_snippet": "D AfroBench Evaluation with Confidence\nScores\nWe computed 95% confidence intervals AfroBench\nresults to quantify statistical significance. The cal-\nculation was based on the results of 5 prompts\nfor each task (3 prompts for NLG tasks). Table 9\npresents the average performance and confidence\nintervals accross prompts to assess variability and\nsignificance.\nE Newer LLM evaluation on\nAfroBench-Lite\nWe extended our evaluation for AFROBENCH -\nLITEto includes newer LLMs such as Lugha-\nLLaMa (an African-centric LLM) (Buzaaba et al.,\n2025), GPT-4.1, Gemini-2.0-Flash, and LLaMa 4\n400B (Maverick) in Table 8.\nF Languages covered in the evaluation\nTable 10 shows the languages and tasks we evalu-\nated on.\n18"
      },
      {
        "source_id": "source_33",
        "citation": "AfroBench_ How Good are Large Language Models on African Languages?.pdf",
        "text_snippet": "arXiv:2311.07978v5  [cs.CL]  7 Jun 2025\nAfroBench: How Good are Large Language Models on African Languages?\nJessica Ojo 1,3\u2217, Odunayo Ogundepo 4,5\u2217, Akintunde Oladipo 4,5\u2217, Kelechi Ogueji5\u2217,\nJimmy Lin 5, Pontus Stenetorp 6, David Ifeoluwa Adelani 1,2\u2217\n\u2217Masakhane NLP, 1Mila - Quebec AI Institute & McGill University, 2Canada CIFAR AI Chair, 3Lelapa AI,\n4The African Research Collective 5University of Waterloo, 6University College London\nCorrespondence:{jessica.ojo, david.adelani}@mila.quebec\nAbstract\nLarge-scale multilingual evaluations, such as\nMEGA, often include only a handful of African\nlanguages due to the scarcity of high-quality\nevaluation data and the limited discoverabil-\nity of existing African datasets. This lack\nof representation hinders comprehensive LLM\nevaluation across a diverse range of languages\nand tasks. To address these challenges, we\nintroduce AFROBENCH \u2014a multi-task bench-\nmark for evaluating the performance of LLMs\nacross 64 African languages, 15 tasks and 22"
      },
      {
        "source_id": "source_12",
        "citation": "A Survey on Large Language Models with Multilingualism_ Recent Advances and New Frontiers.pdf",
        "text_snippet": "Despite current efforts, there is still a lack of robust LLMs capable of effectively and comprehen-\nsively performing law-related multilingual tasks, highlighting the need for further exploration in\nthis domain. Beyond the issue of data scarcity, the accumulation of language-specific legal knowl-\nedge compounds the complexity, as legal systems and jurisdictions vary significantly across regions.\nGiven that existing LLMs already struggle with representing semantic features in low-resource\nlanguages, accurately capturing legal nuances across diverse jurisdictions poses an even greater\nchallenge. Moreover, the temporal dimension adds complexity, as laws undergo constant revision,\namendment, or abolition, necessitating that models remain continuously updated.\n7.3 Limitations and Future Directions on Multi-Domain\nDespite the remarkable advancements in multilingual LLMs, persistent limitations and challenges"
      }
    ],
    "time_taken": 2.95
  },
  {
    "question": "According to Conneau (2020), how does XLM-R compare to mBERT?",
    "answer": "XLM-R outperforms mBERT in several aspects. It sets a new state of the art on XNLI, obtaining 80.9% accuracy, which is 14.6% higher than mBERT. Additionally, XLM-R performs better than mBERT in low-resource languages such as Swahili and Urdu, with improvements of 23.5% and 15.8% respectively. XLM-R also outperforms mBERT by 2.42 F1 on average for cross-lingual transfer in named entity recognition tasks.",
    "citations_readable": [
      "Unsupervised Cross-Lingual Representation.pdf",
      "Cross-lingual transfer of multilingual models on low resource African Languages.pdf"
    ],
    "citations_raw": [
      "source_03",
      "source_27"
    ],
    "retrieved_chunks": [
      {
        "source_id": "source_03",
        "citation": "Unsupervised Cross-Lingual Representation.pdf",
        "text_snippet": "sive study to assess this claim on the XNLI bench-\nmark. We extend our comparison between multilin-\ngual XLM models and monolingual BERT models\non 7 languages and compare performance in Ta-\nble5. We train 14 monolingual BERT models on\nWikipedia and CommonCrawl (capped at 60 GiB),\nand two XLM-7 models. We increase the vocab-\nulary size of the multilingual model for a better\ncomparison. We found thatmultilingual models\ncan outperform their monolingual BERT counter-\nparts. Speci\ufb01cally, in Table5, we show that for\ncross-lingual transfer, monolingual baselines out-\nperform XLM-7 for both Wikipedia and CC by\n1.6% and 1.3% average accuracy. However, by\nmaking use of multilingual training (translate-train-\nall) and leveraging training sets coming from mul-\ntiple languages, XLM-7 can outperform the BERT\nmodels: our XLM-7 trained on CC obtains 80.0%\naverage accuracy on the 7 languages, while the\naverage performance of BERT models trained on\nCC is 77.5%. This is a surprising result that shows"
      },
      {
        "source_id": "source_21",
        "citation": "NaijaSenti_ A Nigerian Twitter Sentiment Corpus for Multilingual Sentiment Analysis.pdf",
        "text_snippet": "Face Transformer (Wolf et al., 2020). Appendix B pro-\nvides the details of the hyper-parameters used for train-\ning.\nmBERT is a multilingual variant of BERT pre-\ntrained on 104 languages, including one Nige-\nrian language\u2014Yor\u00f9b\u00e1. mBERT was pre-trained\nusing masked language modeling (MLM) and\nnext-sentence prediction task. We \ufb01ne-tune the\nmBERT-base-cased model with 172M model\nparameters by adding a linear classi\ufb01cation layer on\ntop of the pre-trained transformer model.\nXLM-R Similar to mBERT, XLM-R (Conneau et al.,\n2020) is a multilingual variant of RoBERTa pre-trained\non 100 languages, including Hausa as the only Nigerian\nlanguage. Unlike mBERT, XLM-R only uses MLM\nduring pre-training. We use XLR-base with 270M"
      },
      {
        "source_id": "source_17",
        "citation": "AfriMTE and AfriCOMET_ Enhancing COMET to Embrace Under-resourced African Languages.pdf",
        "text_snippet": "respectively in Appendix A.1. Both AfroXLM-R-L\nbased systems (STL and MTL) tend to outperform\nN-gram matching based metrics, BERTScore and\nCOMET22, and show comparable or superior re-\nsults to GPT-4.\nParticularly, our results reveal improvements for\neng-ibo and eng-yor (FLORES, News, and TED\ntalks) when we utilize AfroXLM-R-L instead of\nXLM-R-L as encoder, aligning with their language\ncoverage in Table 9 in Appendix A.1. Addition-\nally, languages initially supported by XLM-R-L,\nsuch as eng-hau, eng-som and eng-xho, also experi-\nence enhancements with the adoption of AfroXLM-\nR-L. Interestingly, eng-kik and eng-luo transla-\ntion evaluations show marked improvements even\nthough Kikuyu and Luo are not explicitly cov-\nered by AfroXLM-R-L. Further analysis of correla-\ntions across four eng-yor domain-specific datasets\nshow that models trained based on AfroXLM-R-L\nhave the potential to surpass the performance of\nCOMET22, indicating its generalization for dif-"
      },
      {
        "source_id": "source_03",
        "citation": "Unsupervised Cross-Lingual Representation.pdf",
        "text_snippet": "argue that in real applications, a small amount of\nsupervised data is often available for validation in\neach language.\nXLM-Rsets a new state of the art on XNLI. On\ncross-lingual transfer,XLM-Robtains 80.9% accu-\nracy, outperforming the XLM-100 and mBERT\nopen-source models by 10.2% and 14.6% aver-\nage accuracy. On the Swahili and Urdu low-\nresource languages,XLM-Routperforms XLM-100\nby 15.7% and 11.4%, and mBERT by 23.5% and\n15.8%. WhileXLM-Rhandles 100 languages, we\nalso show that it outperforms the former state of\nthe art Unicoder (Huang et al.,2019) and XLM\n(MLM+TLM), which handle only 15 languages, by\n5.5% and 5.8% average accuracy respectively. Us-\ning the multilingual training of translate-train-all,\nXLM-Rfurther improves performance and reaches\n83.6% accuracy, a new overall state of the art for\nXNLI, outperforming Unicoder by 5.1%. Multi-\nlingual training is similar to practical applications\nwhere training sets are available in various lan-"
      },
      {
        "source_id": "source_03",
        "citation": "Unsupervised Cross-Lingual Representation.pdf",
        "text_snippet": "5.3 Multilingual versus Monolingual\nIn this section, we present results of multilingual\nXLM models against monolingual BERT models.\nGLUE:XLM-Rversus RoBERTa.Our goal is\nto obtain a multilingual model with strong perfor-\nmance on both, cross-lingual understanding tasks\nas well as natural language understanding tasks\nfor each language. To that end, we evaluateXLM-\nRon the GLUE benchmark. We show in Table4,\nthatXLM-Robtains better average dev performance\nthan BERTLargeby 1.6% and reaches performance\non par with XLNetLarge. The RoBERTa model out-\nperformsXLM-Rby only 1.0% on average. We\nbelieve future work can reduce this gap even fur-\nther by alleviating the curse of multilinguality and"
      },
      {
        "source_id": "source_03",
        "citation": "Unsupervised Cross-Lingual Representation.pdf",
        "text_snippet": "Table 2:Results on named entity recognitionon\nCoNLL-2002 and CoNLL-2003 (F1 score). Results\nwith\u2020are fromWu and Dredze(2019). Note that\nmBERT andXLM-Rdo not use a linear-chain CRF, as\nopposed toAkbik et al.(2018) andLample et al.(2016).\non each of the target languages in three different\nsettings: (i) train on English data only (en) (ii) train\non data in target language (each) (iii) train on data\nin all languages (all). Results of mBERT are re-\nported fromWu and Dredze(2019). Note that we\ndo not use a linear-chain CRF on top ofXLM-R\nand mBERT representations, which gives an advan-\ntage toAkbik et al.(2018). Without the CRF, our\nXLM-Rmodel still performs on par with the state\nof the art, outperformingAkbik et al.(2018) on\nDutch by2.09points. On this task,XLM-Ralso\noutperforms mBERT by 2.42 F1 on average for\ncross-lingual transfer, and 1.86 F1 when trained\non each language. Training on all languages leads\nto an average F1 score of 89.43%, outperforming"
      },
      {
        "source_id": "source_27",
        "citation": "Cross-lingual transfer of multilingual models on low resource African Languages.pdf",
        "text_snippet": "Punjabi. Results demonstrated that IndicSBERT\ngenerally outperforms LaBSE, showcasing strong\nmultilingual and cross-lingual capabilities. (Fei\nand Li, 2020) evaluate the Multi-View Encoder-\nClassifier (MVEC) model against various mod-\nels like multilingual BERT (mBERT) and XLM\nfor cross-lingual sentiment classification. MVEC\noutperformed these models in 8 out of 11 senti-\nment classification tasks across five language pairs,\nemploying unsupervised machine translation and\nlanguage discriminator to align latent space be-\ntween languages. (Conneau et al., 2019) introduce\nXLM-R, a large-scale multilingual language model\ntrained on 100 languages using two terabytes of\nCommonCrawl data. XLM-R offers better per-\nformance than models such as mBERT, particu-\nlarly in low-resource languages such as Swahili\nand Urdu. The study also highlights increasing\nthe model\u2019s capacity helps mitigate capacity chal-\nlenges as the languages increase. (Abdul-Mageed\net al., 2020) introduce ARBERT and MARBERT,"
      },
      {
        "source_id": "source_21",
        "citation": "NaijaSenti_ A Nigerian Twitter Sentiment Corpus for Multilingual Sentiment Analysis.pdf",
        "text_snippet": "other hand, PLMs have a minimum F1-score of 70%,\ndemonstrating their usefulness for sentiment analysis.\nMultilingual PLMs are quite similar in most cases\nwith about a 1\u2212 3% performance difference. The\nperformance may depend on the language being seen\nduring pre-training. mBERT has a slightly better per-\nformance ( +0.7%) for yor than XLM-R likely be-\ncause yor was seen during pre-training. Similarly,\nXLM-R performs better for hau. RemBERT achieves\nslightly better performance than mBERT and XLM-\nR-base, demonstrating that a model with more capac-\nity can improve performance. Surprisingly, we found\nmDeBERTaV3 that has only seen hau gives better re-\nsults (77.8%) than other models except for AfriBERTa\nthat has been exclusively trained on African languages.\nmDeBERTaV3 makes use of replaced token detection\n(RTD), which has been shown to give superior per-\nformance for English (Clark et al., 2020). Overall,\nwe found AfriBERTa to be the best baseline model"
      },
      {
        "source_id": "source_17",
        "citation": "AfriMTE and AfriCOMET_ Enhancing COMET to Embrace Under-resourced African Languages.pdf",
        "text_snippet": "in (Zerva et al., 2022b). Our results, showcased\nin Table 3, along with the Perm-Input hypothe-\nsis test results in Table 18 in Appendix A.1, re-\nveal the following insights. The InfoXLM-L STL\nmodel, trained on \u201cWMT Others\u201d, performs on par\nwith CometKiwi under the same encoder configura-\ntions. However, the AfroXLM-R-L STL model\nexhibits significant improvements in both Pear-\nson and Spearman-rank correlations, superior over\nCometKiwi. Additionally, MTL training further\n12We follow the hyper-parameter settings at https://\ngithub.com/Unbabel/COMET/tree/master/configs, use\nthe same batch size and gradient accumulation, and utilize the\nsame hardware as when training the MT evaluation models.\n13We re-query the GPT-4 API up to five times for each\nexample, in an effort to obtain successful responses. Despite\nthese efforts, certain instances persist where error responses\nare encountered even after five attempts."
      },
      {
        "source_id": "source_10",
        "citation": "Targeted Multilingual Adaptation for Low-resource Language Families.pdf",
        "text_snippet": "data can be found in Table 8 of Appendix B, along\nwith more details on our evaluation methodology.\nBaselines Our simplest baseline is \u201coff-the-shelf\u201d\nXLM-R \u2014 the pre-trained model from Conneau\net al. (2020a) with no modifications. We also test\nXLM-R adapted with LAPT, but without vocabu-\nlary specialization. LAPT alone is a strong baseline.\nHowever, as Downey et al. (2023) note, keeping\na large \u201ccross-lingual\u201d vocabulary during LAPT\nincurs considerable extra computational cost com-\npared to training a smaller, specialized vocabulary.\nGiven the observation that cross-lingual tokeniz-\ners are inefficient and ineffective for low-resource\nlanguages (\u00c1cs, 2019; Rust et al., 2021), we hy-\npothesize a specialized vocabulary will show a per-\nformance advantage in addition to the reduction in\ncomputational cost.\nWe also compare our multilingual models to\nbaselines adapted to single languages. While mul-\ntilingualism is known to help low-resource lan-"
      },
      {
        "source_id": "source_03",
        "citation": "Unsupervised Cross-Lingual Representation.pdf",
        "text_snippet": "1 Introduction\nThe goal of this paper is to improve cross-lingual\nlanguage understanding (XLU), by carefully study-\ning the effects of training unsupervised cross-\nlingual representations at a very large scale. We\npresentXLM-Ra transformer-based multilingual\nmasked language model pre-trained on text in 100\nlanguages, which obtains state-of-the-art perfor-\nmance on cross-lingual classi\ufb01cation, sequence la-\nbeling and question answering.\n\u21e4Equal contribution.\nCorrespondence to{aconneau,kartikayk}@fb.com\n1https://github.com/facebookresearch/(fairseq-py,pytext,xlm)\nMultilingual masked language models (MLM)\nlike mBERT (Devlin et al.,2018) and XLM (Lam-\nple and Conneau,2019) have pushed the state-\nof-the-art on cross-lingual understanding tasks\nby jointly pretraining large Transformer mod-\nels (V aswani et al.,2017) on many languages.\nThese models allow for effective cross-lingual\ntransfer, as seen in a number of benchmarks in-\ncluding cross-lingual natural language inference"
      },
      {
        "source_id": "source_03",
        "citation": "Unsupervised Cross-Lingual Representation.pdf",
        "text_snippet": "8442\nFigure 1: Amount of data in GiB (log-scale) for the 88 languages that appear in both the Wiki-100 corpus used for\nmBERT and XLM-100, and the CC-100 used for XLM-R. CC-100 increases the amount of data by several orders\nof magnitude, in particular for low-resource languages.\nenization directly on raw text data using Sentence\nPiece (Kudo and Richardson,2018) with a unigram\nlanguage model (Kudo,2018). We sample batches\nfrom different languages using the same sampling\ndistribution asLample and Conneau(2019), but\nwith\u21b5=0.3. UnlikeLample and Conneau(2019),\nwe do not use language embeddings, which allows\nour model to better deal with code-switching. We\nuse a large vocabulary size of 250K with a full soft-\nmax and train two different models:XLM-RBase(L\n= 12, H = 768, A = 12, 270M params) andXLM-R\n(L = 24, H = 1024, A = 16, 550M params). For all\nof our ablation studies, we use a BERTBasearchitec-\nture with a vocabulary of 150K tokens. AppendixB"
      }
    ],
    "time_taken": 2.45
  },
  {
    "question": "What are the three main challenges in preserving cultural identity according to Anik (2025)?",
    "answer": "The three main challenges in preserving cultural identity according to Anik (2025) are: 1) AI-driven translation models often fail to capture cultural nuances, idiomatic expressions, and historical significance, 2) Existing models prioritize efficiency over cultural authenticity, and 3) Translations can marginalize linguistic diversity.",
    "citations_readable": [
      "Preserving Cultural Identity with Context-Aware Translation Through  Multi-Agent AI Systems.pdf"
    ],
    "citations_raw": [
      "source_30"
    ],
    "retrieved_chunks": [
      {
        "source_id": "source_18",
        "citation": "Generative AI and Large Language Models in Language Preservation_  Opportunities and Challenges.pdf",
        "text_snippet": "content, and support for linguistic research.\n3. Identify Challenges Data scarcity and quality for a less-resourced language, technical complexity in cap-\nturing linguistic nuances, ensuring cultural authenticity and avoiding misrepresentation,\nupholding community ownership and data sovereignty, and addressing resource demands\n(computational, financial, personnel).\nOutputs\nOpportunities (O(Li, TAI)) Summarized as: Creation of accessible digital archives, engaging educational resources,\nincreased language visibility, and advanced research capabilities.\nChallenges (C(Li, TAI)) Summarized as: Need to overcome data limitations, technical hurdles, cultural/ethical\nrisks, and resource constraints.\nStrategies (S(Li)) Community-led development and governance, capacity building within the M \u00afaori\ncommunity, adoption of ethical AI guidelines prioritizing cultural protocols, iterative\ndevelopment with human oversight, and focus on low-resource AI techniques."
      },
      {
        "source_id": "source_30",
        "citation": "Preserving Cultural Identity with Context-Aware Translation Through  Multi-Agent AI Systems.pdf",
        "text_snippet": "sion, and marginalizes minority identities. Despite\ngrowing awareness, traditional preservation meth-\nods remain inadequate; documentation efforts fail\nto capture cultural complexity, while machine trans-\nlation distorts contextual meaning (Hutson et al.,\n2024). The digital linguistic divide further excludes\nunderrepresented languages, limiting their digital\npresence and corpus availability (Bella et al., 2023).\nAdditionally, economic pressures favor dominant\nglobal languages, leading younger generations to\nabandon their native tongues. While technologi-\ncal advancements offer potential solutions, current\napproaches often focus on efficiency rather than\ncultural authenticity, overlooking the need for lin-\nguistic preservation beyond translation (Mufwene,\n2005). As AI-driven methods become central to\nlanguage processing, it is essential to rethink how\narXiv:2503.04827v1  [cs.CL]  5 Mar 2025"
      },
      {
        "source_id": "source_12",
        "citation": "A Survey on Large Language Models with Multilingualism_ Recent Advances and New Frontiers.pdf",
        "text_snippet": "that preserve cultural context.\n\u2022 Exploration of techniques for LLMs to effectively integrate and leverage language-specific\nin-domain knowledge.\nBy addressing these two challenges, researchers can pave the way for the development of truly robust\nand equitable LLMs that serve a global audience in multilingual scenarios.\n23"
      },
      {
        "source_id": "source_30",
        "citation": "Preserving Cultural Identity with Context-Aware Translation Through  Multi-Agent AI Systems.pdf",
        "text_snippet": "2024. Preserving linguistic diversity in the digital\nage: a scalable model for cultural heritage continuity.\nJournal of Contemporary Language Research, 3(1).\nGraham M Jones, Shai Satran, and Arvind Satyanarayan.\n2025. Toward cultural interpretability: A linguistic\nanthropological framework for describing and eval-\nuating large language models. Big Data & Society,\n12(1):20539517241303118.\nAnne Kandler and Roman Unger. 2023. Modeling lan-\nguage shift. In Diffusive spreading in nature, tech-\nnology and society, pages 365\u2013387. Springer.\nNir Kshetri. 2024. Linguistic challenges in generative\nartificial intelligence: Implications for low-resource\nlanguages in the developing world.\nHaley Lepp and Parth Sarin. 2024. A global ai com-\nmunity requires language-diverse publishing. arXiv\npreprint arXiv:2408.14772.\nJiangfeng Liu, Ziyi Wang, Jing Xie, and Lei Pei. 2024.\nFrom chatgpt, dall-e 3 to sora: How has generative\nai changed digital humanities research and services?"
      },
      {
        "source_id": "source_30",
        "citation": "Preserving Cultural Identity with Context-Aware Translation Through  Multi-Agent AI Systems.pdf",
        "text_snippet": "Preserving Cultural Identity with Context-Aware Translation Through\nMulti-Agent AI Systems\nMahfuz Ahmed Anik1 Abdur Rahman1, Azmine Toushik Wasi1\u2020, Md Manjurul Ahsan2\n1Shahjalal University of Science and Technology, Sylhet, Bangladesh\n2University of Oklahoma, Norman, OK 73019, USA\n\u2020Correspondence: azmine32@student.sust.edu\nAbstract\nLanguage is a cornerstone of cultural identity,\nyet globalization and the dominance of major\nlanguages have placed nearly 3,000 languages\nat risk of extinction. Existing AI-driven trans-\nlation models prioritize efficiency but often fail\nto capture cultural nuances, idiomatic expres-\nsions, and historical significance, leading to\ntranslations that marginalize linguistic diver-\nsity. To address these challenges, we propose\na multi-agent AI framework designed for cul-\nturally adaptive translation in underserved lan-\nguage communities. Our approach leverages\nspecialized agents for translation, interpreta-\ntion, content synthesis, and bias evaluation,"
      },
      {
        "source_id": "source_18",
        "citation": "Generative AI and Large Language Models in Language Preservation_  Opportunities and Challenges.pdf",
        "text_snippet": "5\nFig. 3. A human-centered framework for GenAI initiatives, illustrating\nthe dual phases of problem identification and solution implementation, with\niterative feedback between readiness, strategy, use case discovery, operating\nmodel, infrastructure, and awareness. Adapted and redesigned based on\nPwC [30].\nC. Ethical and Cultural Dimensions\nThe application of AI in language preservation necessitates\ncareful navigation of ethical and cultural landscapes. Key\nconsiderations from previous sections will be integrated and\nexpanded here. For instance, using AI for language preserva-\ntion raises important ethical questions, particularly regarding\nownership and cultural sensitivity of linguistic data. Commu-\nnities whose languages are endangered may have little control\nover how their languages are represented or monetized. As\nemphasized by Peter-Lucas Jones, CEO of Te Hiku Media, in-\ndigenous communities must maintain control and governance"
      },
      {
        "source_id": "source_13",
        "citation": "No Language Left Behind_ Scaling Human-Centered Machine Translation.pdf",
        "text_snippet": "content could then be channeled to performing other more important tasks.\nBecause language is intrinsically tied to culture, for many low-resource languages facing\nendangerment, the threat of losing one\u2019s language could also mean the erosion of one\u2019s\nheritage (Sallabank, 2013). NLLB could motivate more low-resource language writers or\ncontent creators to share localized knowledge or various aspects of their culture with both\ncultural insiders and outsiders through social media platforms or websites like Wikipedia.\nGiving individuals access to new translation tools could thus open up a valuable avenue for\nbidirectional learning. In the long run, such generative processes could create dents on the\nglobal knowledge system, challenge Western-centric modes of knowledge production and\ndissemination, and aid in the revitalization of certain minority cultures and languages (Bird,\n2019; Bird and Chiang, 2012).\n9.2 The Janus-faced Nature of Digital Participation"
      },
      {
        "source_id": "source_18",
        "citation": "Generative AI and Large Language Models in Language Preservation_  Opportunities and Challenges.pdf",
        "text_snippet": "community, adoption of ethical AI guidelines prioritizing cultural protocols, iterative\ndevelopment with human oversight, and focus on low-resource AI techniques.\nfrom audio recordings and transcribe them into text[15].\nThese examples demonstrate the potential of AI-driven speech\nrecognition in preserving endangered languages, particularly\nwhen developed in collaboration with language communities\nand leveraging existing archival materials.\nB. Challenges in AI-driven Language Preservation\nDespite its vast potential, generative AI also poses several\nchallenges that must be addressed to ensure effective and\nethical language preservation.\nData Scarcity: One of the most significant hurdles is the\nlack of sufficient data in many endangered languages. While\nmajor languages such as English, Spanish, and Mandarin\nboast enormous digital footprints, most endangered languages\nlack comprehensive written records. This deficit constrains the"
      },
      {
        "source_id": "source_13",
        "citation": "No Language Left Behind_ Scaling Human-Centered Machine Translation.pdf",
        "text_snippet": "Taiwan, November 2017. Asian Federation of Natural Language Processing. URL https:\n//aclanthology.org/I17-2050.\nAbu Sadat Nurullah. Globalisation as a challenge to islamic cultural identity. International\nJournal of Interdisciplinary Social Sciences , 3(6):45\u201352, 2008.\nAkintunde Oladipo, Odunayo Ogundepo, Kelechi Ogueji, and Jimmy Lin. An exploration of\nvocabulary size and transfer effects in multilingual language models for african languages. In\n3rd Workshop on African Natural Language Processing , 2022. URL https://openreview.\nnet/forum?id=HOZmF9MV8Wc.\nIroro Orife, Julia Kreutzer, Blessing Sibanda, Daniel Whitenack, Kathleen Siminyu, Laura\nMartinus, Jamiil Toure Ali, Jade Z. Abbott, Vukosi Marivate, Salomon Kabongo, Musie\nMeressa, Espoir Murhabazi, Orevaoghene Ahia, Elan Van Biljon, Arshath Ramkilowan,\nAdewale Akinfaderin, Alp \u00d6ktem, Wole Akin, Ghollah Kioko, Kevin Degila, Herman\nKamper, Bonaventure Dossou, Chris Emezue, Kelechi Ogueji, and Abdallah Bashir."
      },
      {
        "source_id": "source_13",
        "citation": "No Language Left Behind_ Scaling Human-Centered Machine Translation.pdf",
        "text_snippet": "the usage and application of their native languages, many of them would face endangerment\nin time to come.\nDecline of Native Language and Culture. The fear that the low-resource languages\nmight be undergoing a state of decline reverberated throughout the interviews. Such asser-\ntions typically attributed the decline to two causes: cultural and economical. Cultural theory\nsuggests that as more and more aspects of our lives become digitally-mediated, prolonged\nexposures to content found on the web and social media platforms (e.g., YouTube, Facebook)\nleads to the prioritization of high-resource languages. By extension, this phenomenon\nspotlights Western epistemology and ideas over other ways of knowing (Nurullah, 2008). As\nfew interviewees pointed out, the cultural dominance of the West applies intense pressure\nonto more localized media productions. As low-resource language speakers gravitate towards"
      },
      {
        "source_id": "source_18",
        "citation": "Generative AI and Large Language Models in Language Preservation_  Opportunities and Challenges.pdf",
        "text_snippet": "of preservation efforts. The United Nations, through bodies\nlike UNESCO, emphasizes the right to freedom of expression\nin a language of one\u2019s choice, promoting multilingualism and\nthe development of multilingual digital content and systems\n[3]. Language endangerment leads to the loss of values and\nknowledge that cannot be easily replaced[4]. Interestingly,\nwhile factors like increased road density (facilitating popula-\ntion movement) and higher average years of formal schooling\ncorrelate with increased endangerment, direct contact with\nother languages is not, in itself, a primary driver of language\nloss[2].\nTraditional methods of language preservation, such as field\nphotography, oral history recording, and dictionaries, have\nbeen effective in documenting endangered languages. How-\never, these methods often fail to achieve the desired results due\nto limited resources, limited time, and limited native speaker\nskills. In recent years, there has been great interest in using"
      },
      {
        "source_id": "source_09",
        "citation": "The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks.pdf",
        "text_snippet": "highlighting the critical need for culturally and linguistically authentic evaluation resources.\n3. FUTURE: What do we need, and what should we do next? (Section 6)Based on our\nanalysis, we outline key principles for effective multilingual benchmarks, emphasizing the need\n2"
      }
    ],
    "time_taken": 1.94
  },
  {
    "question": "How does the 'Cheetah' paper propose to handle 517 African languages?",
    "answer": "The 'Cheetah' paper proposes to handle 517 African languages by developing a massively multilingual language model specifically designed for natural language generation (NLG) in these languages. Cheetah is trained on a 42G multi-domain corpus that includes diverse sources such as news articles, health documents, religious texts, legal documents, and social media feeds. This approach addresses the scarcity of NLG resources for African languages and supports linguistic diversity by enabling the generation of coherent and contextually appropriate text. The model's effectiveness is demonstrated through evaluations across six downstream tasks, where it significantly outperforms other models in five of the tasks.",
    "citations_readable": [
      "Cheetah_ Natural Language Generation for 517 African Languages.pdf"
    ],
    "citations_raw": [
      "source_04"
    ],
    "retrieved_chunks": [
      {
        "source_id": "source_04",
        "citation": "Cheetah_ Natural Language Generation for 517 African Languages.pdf",
        "text_snippet": "and syntactic structures of the languages they are\ntrained on, making them valuable starting points\nfor supporting NLG in low-resource settings.\nIn this paper, we present a pioneering work on\nNLG in African languages by introducing Chee-\ntah: a novel language model (LM) specifically de-\nsigned to support 517 African languages and lan-\nguage varieties. To the best of our knowledge,\nCheetah supports the largest number of African\nlanguages and language varieties. Leveraging a\nvast corpus of text data collected from diverse\nsources, Cheetah learns some intricate linguistic in-\nformation that characterize each African language.\nThe contributions of this research are three fold.\nFirst, we address the scarcity of NLG resources\nfor African languages by providing a comprehens-\nive language model that covers a wide range of\nlanguages spoken on the continent. Second, we\ndemonstrate the efficacy of our approach through\nextensive evaluations across six downstream task"
      },
      {
        "source_id": "source_04",
        "citation": "Cheetah_ Natural Language Generation for 517 African Languages.pdf",
        "text_snippet": "Cheetah\n : Natural Language Generation for 517 African Languages\nIfe Adebara1,\u22c6 AbdelRahim Elmadany1,\u22c6 Muhammad Abdul-Mageed1,2\n1Deep Learning & Natural Language Processing Group, The University of British Columbia\n2Department of Natural Language Processing & Department of Machine Learning, MBZUAI\n{ife.adebara@,a.elmadany@,muhammad.mageed@}ubc.ca\nAbstract\nLow-resource African languages pose unique\nchallenges for natural language processing\n(NLP) tasks, including natural language\ngeneration (NLG). In this paper, we develop\nCheetah, a massively multilingual NLG\nlanguage model for African languages.\nCheetah supports 517 African languages and\nlanguage varieties, allowing us to address\nthe scarcity of NLG resources and provide\na solution to foster linguistic diversity. We\ndemonstrate the effectiveness of Chee-\ntah through comprehensive evaluations across\nsix generation downstream tasks. In five of the\nsix tasks, Cheetah significantly outperforms\nother models, showcasing its remarkable"
      },
      {
        "source_id": "source_04",
        "citation": "Cheetah_ Natural Language Generation for 517 African Languages.pdf",
        "text_snippet": "tah through comprehensive evaluations across\nsix generation downstream tasks. In five of the\nsix tasks, Cheetah significantly outperforms\nother models, showcasing its remarkable\nperformance for generating coherent and\ncontextually appropriate text in a wide range\nof African languages. We additionally conduct\na detailed human evaluation to delve deeper\ninto the linguistic capabilities of Cheetah.\nThe introduction of Cheetah has far-reaching\nbenefits for linguistic diversity. By leveraging\npretrained models and adapting them to\nspecific languages, our approach facilitates the\ndevelopment of practical NLG applications\nfor African communities. The findings of\nthis study contribute to advancing NLP\nresearch in low-resource settings, enabling\ngreater accessibility and inclusion for African\nlanguages in a rapidly expanding digital\nlandscape. We will publicly release our models\nfor research. 1\n1 Introduction\nThe linguistic diversity present in African lan-"
      },
      {
        "source_id": "source_04",
        "citation": "Cheetah_ Natural Language Generation for 517 African Languages.pdf",
        "text_snippet": "data. To address this issue, we undertook the task\nof manually curating a diverse corpus spanning\nmultiple domains, including news articles, health\ndocuments, religious texts, legal documents, and\nsocial media feeds. This manual curation approach\nwas necessary because there were no existing data-\nsets available for the majority of the languages we\naimed to support, and we wanted to ensure the\nutilization of reliable and high-quality data.\nCoverage. In all, we train Cheetah using a 42G\nmulti-domain corpus across 517 African languages\nand language varieties. The languages are spoken\nin 50 of 54 African countries and they are written\nwith five scripts. This provides support to at least\n500M Africans.\nLinguistic Diversity. The inclusion of languages\nfrom various domains, geographical regions, and\nlinguistic typologies, along with the utilization of\nreliable data sources, contributes to enhancing the\nrobustness and quality of Cheetah. Our data con-"
      },
      {
        "source_id": "source_04",
        "citation": "Cheetah_ Natural Language Generation for 517 African Languages.pdf",
        "text_snippet": "against native speakers of those languages.\nMore precisely, it does so by focusing on\nAfrica. To the best of our knowledge, Chee-\ntah is the first massively multilingual PLM de-\nveloped for African languages and language\nvarieties. A model with knowledge of 517\nAfrican languages, is by far the largest to date\nfor African NLP.\n2. Cheetah enables improved access of import-\nant information to the African community in\nIndigenous African languages. This is espe-\ncially beneficial for people who may not be\nfluent in other languages. This will potentially\nconnect more people globally.\n3. Cheetah affords opportunities for language\npreservation for many African languages. To\nthe best of our knowledge, Cheetah consists\nof languages that have not been used for any\nNLP task until now. We believe that it can\nhelp encourage continued use of these lan-\nguages in several domains, as well as trigger\nfuture development of language technologies\nfor many of these languages."
      },
      {
        "source_id": "source_04",
        "citation": "Cheetah_ Natural Language Generation for 517 African Languages.pdf",
        "text_snippet": "ive language model that covers a wide range of\nlanguages spoken on the continent. Second, we\ndemonstrate the efficacy of our approach through\nextensive evaluations across six downstream task\nclusters. Each cluster includes multiple languages,\nshowcasing the model\u2019s ability to generate coher-\nent and contextually appropriate text in different\nAfrican languages. Third, we perform fine grained\nhuman analysis of Cheetah using a controlled ma-\nchine translation (MT) test set. This uncovers\nmodel behaviour that is not visible with automatic\nmetrics. By supporting NLG in African languages,\nwe foster linguistic diversity, empower African\ncommunities to express themselves in their nat-\nive languages, and bridge the digital divide. This\npaper serves as a foundational step towards promot-\ning Afrocentric NLP (Adebara and Abdul-Mageed,\n2022) that prioritizes inclusivity and cultural preser-\nvation in language technology, emphasizing the im-\nportance of catering to the unique linguistic needs"
      },
      {
        "source_id": "source_04",
        "citation": "Cheetah_ Natural Language Generation for 517 African Languages.pdf",
        "text_snippet": "languages in a rapidly expanding digital\nlandscape. We will publicly release our models\nfor research. 1\n1 Introduction\nThe linguistic diversity present in African lan-\nguages poses unique challenges for NLG sys-\ntems. With over 2, 000 languages spoken across\nthe African continent (Eberhard et al., 2021), the\nneed for effective NLG solutions that can accom-\nmodate this rich linguistic ecosystem cannot be\n1https://github.com/UBC-NLP/Cheetah\n\u22c6 Authors contributed equally.\nFigure 1: Cheetah is trained on 517 African languages\nand language varieties across 14 language families. The\nlanguages are domiciled in 50 of 54 African countries\nand are written in six different scripts.\nover-emphasized. This is especially important be-\ncause traditional NLG approaches have primarily\nfocused on high-resource languages, such as Eng-\nlish and French due to the availability of large-scale\ndatasets and resources. Consequently, low-resource\nlanguages, including numerous African languages,"
      },
      {
        "source_id": "source_04",
        "citation": "Cheetah_ Natural Language Generation for 517 African Languages.pdf",
        "text_snippet": "2022) that prioritizes inclusivity and cultural preser-\nvation in language technology, emphasizing the im-\nportance of catering to the unique linguistic needs\nof diverse populations.\nThe rest of the paper is organized as follows: In\nSection 2, we discuss related work. In Section, 4\nwe describe AfroNLG, the benchmark we create\nfor evaluation. We provide details of Cheetah in\nSection 3. We present performance of Cheetah in\nSection 5 and compare it to other multilingual mod-\nels. We present controlled test sets in Section 5.1.\nWe conclude in Section 6, and outline a number of\nlimitations and use cases for our work in Section 7\nand Section 8.\n2 Literature Review\nOne of the challenges in NLG is to generate coher-\nent and semantically meaningful text. Various ap-\nproaches have been proposed, including template-\nbased (Becker, 2002; Van Deemter et al., 2005),\nrule-based (Du\u0161ek and Jur\u02c7c\u00ed\u02c7cek, 2015; van Milten-\nburg et al., 2020), and statistical approaches (Li"
      },
      {
        "source_id": "source_04",
        "citation": "Cheetah_ Natural Language Generation for 517 African Languages.pdf",
        "text_snippet": "including 517 African languages and language vari-\neties and the top 10 world languages. To the best\nof our knowledge, this is the most extensive bench-\nmark till date for African languages. Table 2 shows,\nat a glance, how our benchmark compares to others\nbenchmark. We provide the details of each task\ncluster and datasets in what follows. For detailed\nstatistics about the task clusters, we refer to Ap-\npendix B.\nCloze Test. In order to comprehensively evaluate\nCheetah across all the languages it was pretrained\non, we employ cloze-tasks as our evaluation ap-\nproach and perform two cloze tasks experiments.\nThese tasks assess the model\u2019s ability to fill in miss-\ning information. In the first cloze task, which we\nhenceforth call mask-one, we randomly mask only\none token in each sentence. In the second cloze-\ntask, which we call mask-at-least-one, we ran-\ndomly mask at least one token and not more than\n10% of the tokens in each sentence. For each of the"
      },
      {
        "source_id": "source_04",
        "citation": "Cheetah_ Natural Language Generation for 517 African Languages.pdf",
        "text_snippet": "different word for the singular and plural forms.\n6 Conclusion\nIn this work, we introduced Cheetah, a massively\nmultilingual language model designed for African\nnatural language generation. We also propose\na new African language generation benchmark,\ndubbed AfroNLG. Our evaluation benchmark is\nboth sizeable and diverse. We evaluate Cheetah on\nAfroNLG comparing it to three other models, two\nmultilingual and one dedicated to African lan-\nguages. The performance of Cheetah surpasses\nthat of all other models we evaluate. This is demon-\nstrated by its superior AfroNLG score, which is\napproximately three times better than the combined\nperformance of other models. Furthermore, Chee-\ntah outperforms all other models across 48 out of\n65 test sets spanning six task clusters. We further\nanalyze our model\u2019s robustness to lexical complex-\nity and carry out human evaluation to inspect the\nmodel\u2019s perform on a controlled test set. Again,\nour results underscore superiority of our model."
      },
      {
        "source_id": "source_19",
        "citation": "Bridging Gaps in Natural Language Processing for Yoru\u0300ba\u0301_ A  Systematic Review of a Decade of Progress and Prospects.pdf",
        "text_snippet": "lation in the Americas (Volume 1: Research Track) , pages 54\u201367, 2018.\n[71] Ife Adebara, AbdelRahim Elmadany, and Muhammad Abdul-Mageed. Cheetah: Nat-\nural language generation for 517 African languages. In Lun-Wei Ku, Andre Martins,\nand Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers) , pages 12798\u201312823, Bangkok,\nThailand, August 2024. Association for Computational Linguistics.\n[72] David Adelani, Dana Ruiter, Jesujoba Alabi, Damilola Adebonojo, Adesina Ayeni, Mofe\nAdeyemi, Ayodele Esther Awokoya, and Cristina Espa\u02dc na-Bonet. The effect of domain\nand diacritics in yoruba\u2013english neural machine translation. In Proceedings of Machine\nTranslation Summit XVIII: Research Track, pages 61\u201375, 2021.\n[73] Olumide Adewale, Oluwatoyin Agbonifo, and Julius Olaniyan. Development of bi-\ndirectional english to yoruba translator for real-time mobile chatting. International Jour-"
      },
      {
        "source_id": "source_19",
        "citation": "Bridging Gaps in Natural Language Processing for Yoru\u0300ba\u0301_ A  Systematic Review of a Decade of Progress and Prospects.pdf",
        "text_snippet": "for typologically diverse african languages. In Proceedings of the 61st Annual Meeting of\nthe Association for Computational Linguistics (Volume 1: Long Papers) , pages 10883\u2013\n10900, 2023.\n[18] Odunayo Ogundepo, Tajuddeen Gwadabe, Clara Rivera, Jonathan Clark, Sebastian\nRuder, David Adelani, Bonaventure Dossou, Abdou Diop, Claytone Sikasote, Gilles\nHacheme, Happy Buzaaba, Ignatius Ezeani, Rooweither Mabuya, Salomey Osei, Chris\nEmezue, Albert Kahira, Shamsuddeen Muhammad, Akintunde Oladipo, Abraham\nOwodunni, Atnafu Tonja, Iyanuoluwa Shode, Akari Asai, Anuoluwapo Aremu, Ayodele\n35"
      }
    ],
    "time_taken": 2.56
  },
  {
    "question": "What metrics were used to evaluate the 'NaijaSenti' corpus?",
    "answer": "The NaijaSenti corpus was evaluated using the F1-score metric.",
    "citations_readable": [
      "NaijaSenti_ A Nigerian Twitter Sentiment Corpus for Multilingual Sentiment Analysis.pdf"
    ],
    "citations_raw": [
      "source_21"
    ],
    "retrieved_chunks": [
      {
        "source_id": "source_21",
        "citation": "NaijaSenti_ A Nigerian Twitter Sentiment Corpus for Multilingual Sentiment Analysis.pdf",
        "text_snippet": "models and transfer learning. The results indicate that\nlanguage-speci\ufb01c models and language-adaptive \ufb01ne-\ntuning perform the best on average. NaijaSenti has\nthe potential to spark interest in sentiment analysis\nand other downstream NLP tasks in the languages in-\nvolved. As future work, we plan to create benchmark\nexperiments with our sentiment lexicon, and extend our\ndataset (NaijaSenti) to include other African languages\n(AfriSenti).\n8. Acknowledgements\nWe thank Daan van Esch for feedback on a draft of\nthis article. This work was carried out with support\nfrom Lacuna Fund, an initiative co-founded by The\nRockefeller Foundation, Google.org, and Canada\u2019s In-\nternational Development Research Centre. The views\nexpressed herein do not necessarily represent those\nof Lacuna Fund, its Steering Committee, its fun-\nders, or Meridian Institute. This work is also par-\ntially funded by the National Funds through the Por-\ntuguese funding agency, FCT - Funda\u00e7\u00e3o para a Ci\u00ean-"
      },
      {
        "source_id": "source_21",
        "citation": "NaijaSenti_ A Nigerian Twitter Sentiment Corpus for Multilingual Sentiment Analysis.pdf",
        "text_snippet": "10 100 500 1k 2.5k 5k 10k 15k\n0.4\n0.5\n0.6\n0.7\n0.8\nNumber of Sentences\nF1-score\nhau F1 ibo F1\nyor F1 Zero-shot hau\nZero-shot ibo Zero-shot yor\nFigure 2: Sample Ef\ufb01ciency on hau, ibo, and yor\nusing the AfriBERTa model.\ntransfer settings.\n7. Conclusions and Future Work\nIn this paper, we present NaijaSenti\u2014the \ufb01rst publicly\navailable large-scale and manually annotated Twitter\nsentiment dataset for the four main Nigerian languages\n(Hausa, Igbo, Nigerian-pidgin, and Yor\u00f9b\u00e1). We pro-\npose methods to enable the collection, \ufb01ltering, and\nannotation of such low-resource language data. Addi-\ntionally, we introduce a manually annotated sentiment\nlexicon in three languages (Hausa, Igbo, and Yor\u00f9b\u00e1).\nWe present benchmark experiments on Twitter senti-\nment dataset using state-of-the-art pre-trained language\nmodels and transfer learning. The results indicate that\nlanguage-speci\ufb01c models and language-adaptive \ufb01ne-\ntuning perform the best on average. NaijaSenti has"
      },
      {
        "source_id": "source_19",
        "citation": "Bridging Gaps in Natural Language Processing for Yoru\u0300ba\u0301_ A  Systematic Review of a Decade of Progress and Prospects.pdf",
        "text_snippet": "the performance of state-of-the-art models on the dataset were carried out, and this mainly\ninvolved automatic translation, which entails translating questions or retrieved documents in\npreparation for processing, and multilingual retrieval, involving carrying out direct retrieval\nthrough the aid of multilingual embeddings.\nAfriSenti: AfriSenti [4] focuses on providing a high-quality, large-scale dataset to address the\nlack of resources for sentiment analysis in underrepresented African languages. This benchmark\ncontains over 110 , 000 posts involving 14 African languages across four language families\u2014\nNiger-Congo, Afro-Asiatic, English Creole and Indo-European\u2014in which Yor` ub\u00b4 a is a core part.\nThe languages featured in the dataset are namely: Amharic, Algerian Arabic, Hausa, Igbo,\nKinyarwanda, Moroccan Arabic, Mozambican Portuguese, Nigerian Pidgin, Oromo, Swahili,\nTigrinya, Twi, Xitsonga, and Yor` ub\u00b4 a. The sentiment categories captured in the corpus include"
      },
      {
        "source_id": "source_21",
        "citation": "NaijaSenti_ A Nigerian Twitter Sentiment Corpus for Multilingual Sentiment Analysis.pdf",
        "text_snippet": "using Interval Type-2 Fuzzy Logic.\nYor\u00f9b\u00e1 Orimaye et al. (2012) built a Yor\u00f9b\u00e1 corpus\nfrom YouTube and applied a translated SentiWordNet\nfor the sentiment analysis task. Iyanda and Abegunde\n(2019) created a multi-domain corpus (health, business,\neducation, politics) and used different classic ML clas-\nsi\ufb01ers such as SVM to predict sentiment in text.\nPidgin Oyewusi et al. (2020) built a Pidgin tweet\ncorpus and used a translated V ADER English lexicon\nfor sentiment analysis.\nTable 1 summarizes the existing datasets for Nigerian\nlanguages; only two datasets are freely available, in-\ndicating that more work is needed to make indigenous\ndatasets accessible and to stimulate research in these\nlanguages. To the best of our knowledge, ours is the\n\ufb01rst publicly available large-scale manually annotated\ndataset for sentiment analysis research in the following\nNigerian languages: Hausa, Igbo, Yor\u00f9b\u00e1 and Nigerian\nPidgin (see Appendix A for the language description\nand characteristics.)"
      },
      {
        "source_id": "source_33",
        "citation": "AfroBench_ How Good are Large Language Models on African Languages?.pdf",
        "text_snippet": "I.1.3 Sentiment Analysis\nAfriSenti\namh arq ary hau ibo kin orm0\n20\n40\n60\n80\n100Average Score\nAfroLLaMa 8B\nLLaMa 2 7B\nLLaMa 3 8B\nLLaMa 3.1 8B\nLLaMaX 3 8B\nGemma 1.1 7B\nAya-101 13B\nGemma 2 9B\nLLaMa 3.1 70B\nGemma 2 27B\nGemini 1.5 pro\nGPT-4o (Aug)\npcm por swa tir tso twi yor\nLanguages\n0\n20\n40\n60\n80\n100Average Score\nFigure 8: Per-language performance results for the AfriSenti dataset.\nNollySenti\neng hau ibo pcm yor\nLanguages\n20\n30\n40\n50\n60\n70\n80\n90\n100\n110Average Score\nAfroLLaMa 8B\nLLaMa 2 7B\nLLaMa 3 8B\nLLaMa 3.1 8B\nLLaMaX 3 8B\nGemma 1.1 7B\nAya-101 13B\nGemma 2 9B\nLLaMa 3.1 70B\nGemma 2 27B\nGemini 1.5 pro\nGPT-4o (Aug)\nFigure 9: Per-language performance results for the NollySenti dataset.\n35"
      },
      {
        "source_id": "source_31",
        "citation": "A Survey on Multilingual Large Language Models_ Corpora, Alignment, and  Bias.pdf",
        "text_snippet": "NER2.0 [87], the largest human-annotated Africa-\ncentric dataset, deriving its data from African local\nnews.\nMultilingual SA. Sentiment analysis tasks,\nwhich focus on the sentiment orientation of data,\noften utilize datasets extracted from comments or\nreviews found on review platforms such as Ama-\nzon and IMDb, as well as social media platforms\nlike Facebook and Twitter. The sentiment analy-\nsis dataset XED [89] is sourced from OPUS [99],\na parallel corpus extracted from movie subtitles.\nIn terms of linguistic diversity, while XED [89]\nprimarily focuses on English and Finnish, Nolly-\nSenti [90] and NaijaSenti [91] are sentiment anal-\nysis datasets specifically designed for African lan-\nguages such as Hausa, Igbo, Nigerian, Pidgin and\nYoruba.\nCross-Lingual IR. Cross-Lingual Information\nRetrieval tasks ask queries in one language and re-\ntrieve documents in one or more other languages.\nThese tasks utilize datasets that include documents"
      },
      {
        "source_id": "source_21",
        "citation": "NaijaSenti_ A Nigerian Twitter Sentiment Corpus for Multilingual Sentiment Analysis.pdf",
        "text_snippet": "Recently,\u2200 et al. (2020) and Adelani et al. (2021) ex-\namined how socio-cultural factors hinder NLP for low-\nresource languages, potentially resulting in economic\ninequities (Weidinger et al., 2021).\nWith more than 200 million people and 522 native lan-\nguages, Nigeria is the most populous and linguistically\ndiverse country in Africa, as well as the third most\nmultilingual country in the world. 1 However, due to\nthe lack of training data for many NLP applications,\nthese languages are underserved by digital technology.\nTherefore, a concerted effort is required to create re-\nsources for such languages (Adelani et al., 2021).\nIn this paper, we present NaijaSenti2\u2014an open-source\nTwitter sentiment dataset for the four most spoken lan-\n1https://www.ethnologue.com/guides/\ncountries-most-languages\n2https://github.com/hausanlp/\nNaijaSenti\nguages in Nigeria\u2014Hausa, Igbo, Pidgin, and Yor\u00f9b\u00e1.\nThis is the largest labelled sentiment dataset in these"
      },
      {
        "source_id": "source_21",
        "citation": "NaijaSenti_ A Nigerian Twitter Sentiment Corpus for Multilingual Sentiment Analysis.pdf",
        "text_snippet": "NaijaSenti: A Nigerian Twitter Sentiment Corpus for Multilingual\nSentiment Analysis\nShamsuddeen Hassan Muhammad1,2\u2217+, David Ifeoluwa Adelani3\u2217, Sebastian Ruder4\nIbrahim Sa\u2019id Ahmad5+, Idris Abdulmumin6\u2217+, Bello Shehu Bello5+, Monojit Choudhury7\nChris Chinenye Emezue8\u2217, Saheed Salahudeen Abdullahi10+, Anuoluwapo Aremu11\u2217\nAl\u00edpio Jorge1,2 , Pavel Brazdil1\n1 LIAAD - INESC TEC, 2Faculty of Sciences-University of Porto, Portugal,\n3Spoken Language Systems Group (LSV), Saarland University, Germany,4Google Research\n5Faculty of Computer Science and Information Technology, Bayero University, Kano, Nigeria\n6Department of Computer Science, Ahmadu Bello University, Zaria, Nigeria, 7Microsoft Research India,\n8Technical University of Munich, Germany,9Clear Global, 10Kaduna state University\n\u2217Masakhane NLP ,+HausaNLP\n{shmuhammad.csc, isahmad.it, bsbello.cs}@buk.edu.ng\nAbstract\nSentiment analysis is one of the most widely studied applications in NLP, but most work focuses on languages with large"
      },
      {
        "source_id": "source_21",
        "citation": "NaijaSenti_ A Nigerian Twitter Sentiment Corpus for Multilingual Sentiment Analysis.pdf",
        "text_snippet": "line, June. Association for Computational Linguis-\ntics.\nNakov, P., Kozareva, Z., Ritter, A., Rosenthal, S., Stoy-\nanov, V ., and Wilson, T. (2019). Semeval-2013 task\n2: Sentiment analysis in twitter.\nNangia, N. and Bowman, S. R. (2019). Human vs.\nmuppet: A conservative estimate of human per-\nformance on the glue benchmark. arXiv preprint\narXiv:1905.10425.\nNasim, Z. and Ghani, S. (2020). Sentiment analysis\non urdu tweets using markov chains. SN Computer\nScience, 1(5):1\u201313.\nNwofe, E. S. (2017). Pro-biafran activists and the\ncall for a referendum: A sentiment analysis of \u2018bi-\nafraexit\u2019on twitter after uk\u2019s vote to leave the euro-\npean union. Journal of Ethnic and Cultural Studies,\n4(1):65.\nOgbuju, E. and Onyesolu, M. (2019). Development\nof a general purpose sentiment lexicon for Igbo lan-\nguage. In Proceedings of the 2019 Workshop on\nWidening NLP, page 1, Florence, Italy, August. As-\nsociation for Computational Linguistics.\nOgueji, K., Zhu, Y ., and Lin, J. (2021). Small data?"
      },
      {
        "source_id": "source_21",
        "citation": "NaijaSenti_ A Nigerian Twitter Sentiment Corpus for Multilingual Sentiment Analysis.pdf",
        "text_snippet": "ent sizes. We observe an F1 score of 38\u2212 40% with\nonly 10 examples, which already exceeds the majority\nvoting performance in Table 8. Surprisingly, with only\n100 sentences, we exceed the zero-shot transfer perfor-\nmance from English language, and with at least 1000\nsentences, we already reach a decent performance of\n70% F1. This result shows that we can leverage a mul-\ntitask sentiment classi\ufb01cation model trained on Nige-\nrian languages to quickly adapt to other African lan-\nguages with as few as 100 or 1000 annotated samples.\nOverall, we identify headroom for model improvement\nparticularly in the zero-shot and few-shot cross-lingual"
      },
      {
        "source_id": "source_31",
        "citation": "A Survey on Multilingual Large Language Models_ Corpora, Alignment, and  Bias.pdf",
        "text_snippet": "Leveraging transfer learning and machine translation\nfor Nigerian movie sentiment classification. In: Pro-\nceedings of the 61st Annual Meeting of the Associa-\ntion for Computational Linguistics. 2023, 986\u2013998\n91. Muhammad S H, Adelani D I, Anuoluwapo A, Abdul-\nmumin I. Naijasenti: A nigerian twitter sentiment cor-\npus for multilingual sentiment analysis. In: Proceed-\nings of the 13th Language Resources and Evaluation\nConference. 2022, 590\u2013602\n92. Ogundepo O, Zhang X, Sun S, Duh K, Lin J.\nAfriCLIRMatrix: Enabling cross-lingual information\nretrieval for african languages. In: Proceedings of\nthe 2022 Conference on Empirical Methods in Natural\nLanguage Processing. 2022, 8721\u20138728\n93. Sun S, Duh K. CLIRMatrix: A massively large collec-\ntion of bilingual and multilingual datasets for cross-\nlingual information retrieval. In: Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing. 2020, 4160\u20134170\n94. Ma C, ImaniGooghari A, Ye H, Asgari E, Sch\u00fctze H."
      },
      {
        "source_id": "source_21",
        "citation": "NaijaSenti_ A Nigerian Twitter Sentiment Corpus for Multilingual Sentiment Analysis.pdf",
        "text_snippet": "Igbo produces tweets in Hausa, such as \u201camin ya rabbi\ngodiya nke \u201d. To mitigate this, we collected tweets\nbased on locations where a language is predominantly\nspoken, using the location, longitude, latitude and ra-\ndius parameters (25 miles) to specify a circular geo-\ngraphic area.\nWe also used Google CLD3 5 and Natural Language\nAPI6 to detect the language of the collected tweets.\nPidgin is not supported by the API, so we used the stop-\nword list to build an n-gram language detection tool\nto detect Pidgin. Before annotation, we cleaned the\ntweets. Retweets and duplicates were removed. We\nremoved URLs and mentions, as well as trailing and\nredundant white spaces, converted all tweets to lower-\ncase, and removed tweets with less than three words as\nthey may contain insuf\ufb01cient information for sentiment\nanalysis (Yang et al., 2018).\n4. Annotation and the NaijaSenti Dataset\n4.1. Annotation Guidelines\nOur annotation guidelines focus on the classi\ufb01cation"
      }
    ],
    "time_taken": 1.23
  },
  {
    "question": "Does the 'Localising SA official languages' paper recommend manual or automated collection?",
    "answer": "The 'Localising SA official languages' paper recommends manual collection.",
    "citations_readable": [
      "Localising SA official languages.pdf"
    ],
    "citations_raw": [
      "source_14"
    ],
    "retrieved_chunks": [
      {
        "source_id": "source_25",
        "citation": "WAXAL_ A Large-Scale Multilingual African Language Speech Corpus.pdf",
        "text_snippet": "Saharan African languages collected in partnership with four\nAfrican academic and community partners and supported by\nGoogle\u2019s technical mentorship. The collection features both\nASR and TTS data for languages such as Acholi, Kiswahili,\nLuganda, Akan, and Ewe; ASR data for Shona, Fula, and\nLingala; and TTS data for Hausa, Igbo, and Yoruba, among\nothers. Our primary contributions are:\n\u2022 A Large-Scale ASR Corpus: We release approximately\n1,250 hours of transcribed, image-prompted natural speech\nfrom a diverse pool of speakers, suitable for training and\narXiv:2602.02734v1  [eess.AS]  2 Feb 2026"
      },
      {
        "source_id": "source_08",
        "citation": "IrokoBench_ A New Benchmark for African Languages in the Age of Large  Language Models.pdf",
        "text_snippet": "larly used for QA tasks.\n3.3 Data collection process\nTranslation We recruited language coordinators\nfor each of the 16 African languages and French,\nand asked them to recruit professional translators\nto translate the sentences. The translation process\ntook about two months, they started with XNLI,\nthen MGSM and MMLU. Each translator received\nan appropriate remuneration for their work. 2 Most\nof the translators translated from English except\nfor Ewe, Lingala and Wolof translators that trans-\nlated from French since they are from the Fran-\ncophone region of Africa. Additionally, we trans-\nlated the MMLU dataset to French by professional\ntranslators and from French to these three lan-\nguages. Many of the Francophone translators un-\nderstand French and English but are more fluent in\nFrench, so they could cross-check from English if\nthe French sentences were not clear enough.\nQuality control Regarding quality control, lan-\nguage coordinators reviewed and corrected any"
      },
      {
        "source_id": "source_26",
        "citation": "Global MMLU_ Understanding and Addressing Cultural and Linguistic Biases  in Multilingual Evaluation.pdf",
        "text_snippet": "et al., 2024) involved collaboration across multiple institutions globally. Such cross-sectional ef-\nforts are crucial for gathering linguistic data at scale and fostering community engagement\u2014both\nessential for developing inclusive language technologies (Joshi et al., 2019; Nekoto et al., 2020;\nSingh et al., 2024; Romanou et al., 2024). We established a criterion requiring a minimum of\n50 human-translated samples for each language before its inclusion inGlobal-MMLU\n . This\nthreshold was met by eleven languages:Amharic, Czech, Malay, Persian, Romanian, Russian,\nSinhala, Telugu, Turkish, Ukrainian, andVietnamese. In the following sections, we refer to this\nset of languages as\u201cCommunity Translated\u201d.\nThe participation of native speakers from diverse regions introduced logistical challenges in both\ndata selection and quality control. To overcome these, we adopted Argilla5 as our primary\nannotation platform. In line with our community-based approach, Argilla\u2019s collaborative features"
      },
      {
        "source_id": "source_08",
        "citation": "IrokoBench_ A New Benchmark for African Languages in the Age of Large  Language Models.pdf",
        "text_snippet": "soning and knowledge-intensive tasks across the\nmajority of African languages.\nFurthermore, the few comprehensive evaluations\nthat exist across languages often rely on machine\ntranslation of English benchmarks (Singh et al.,\n2024). While automatic translation from English\nbenchmarks is a popular approach given the cost\n1\narXiv:2406.03368v2  [cs.CL]  23 Jan 2025"
      },
      {
        "source_id": "source_14",
        "citation": "Localising SA official languages.pdf",
        "text_snippet": "tion revealed that many articles contain text in lan-\nguages other than the target language. These sen-\ntences were discarded.\nAlthough the Mozilla tools and Wikipedia could be\nused to obtain some data in a few languages, the cur-\nrent Wikipedia presence of the majority of the lan-\nguages yielded less than a thousand sentences per\nlanguage. Moreover, the text collection did not pro-\nduce any isiNdebele sentences because, at the time\nof writing, the language did not have a presence on\nWikipedia.\nOther sources of web-based text were subsequently\nexplored in an attempt to collect isiNdebele sen-\ntences as well as additional data for the other\nlanguages. These included the Leipzig Cor-\npora Collection (LCC) (Goldhahn et al. /two.osf/zero.osf/one.osf/two.osf),\nOPUS (Tiedemann & Nygaard /two.osf/zero.osf/zero.osf/four.osf) and the\nFLORES-/two.osf/zero.osf/zero.osf (Goyal et al. /two.osf/zero.osf/two.osf/two.osf) data sets. The\nMozilla tools were also used to collect sentences"
      },
      {
        "source_id": "source_20",
        "citation": "Prompting Towards Alleviating Code-Switched Data Scarcity in  Under-Resourced Languages with GPT as a Pivot.pdf",
        "text_snippet": "according to: i) Yes, ii) Yes, with minimal changes\nor iii) No. We adopt the constraint-free approach\nof MacSwan (2000).\nThe results of the manual annotation are shown\nin Figure 4. We observe that the acceptability of\nAfrikaans\u2013English sentences far outweighs that of\nYoruba\u2013English. We also see that adding few-shot\nexamples increases acceptability (Prompts 1.2 and\n2.2). Although we observe an increase in diver-\nsity through linguistic guidelines, the quality of sen-\ntences are sub-optimal. Subsequent work will fo-\ncus on how correctable sentences can be used\nfor improved prompting and/or fine tuning of lan-\nguage models. However, with further analysis and\nimprovement, there is potential to use GPT 3.5 to\nsupport synthetic data generation.\n4.4. Language Specific Observations\n4.4.1. Afrikaans\u2013English\nIn order to quantify the acceptability observed from\ninternalevaluation, werandomlyselect5Afrikaans-\nEnglish sentences from the dataset used for man-"
      },
      {
        "source_id": "source_32",
        "citation": "Neural Machine Translation for Low-Resource Languages.pdf",
        "text_snippet": "[16] Laurent Besacier, Etienne Barnard, Alexey Karpov, and Tanja Schultz. 2014. Automatic speech recognition for\nunder-resourced languages: A survey. Speech Communication 56 (2014), 85\u2013100.\n[17] Alexandra Birch, Andrew Finch, Minh-Thang Luong, Graham Neubig, and Yusuke Oda. 2018. Findings of the Second\nWorkshop on Neural Machine Translation and Generation. In Proceedings of the 2nd Workshop on Neural Machine\nTranslation and Generation. 1\u201310.\n[18] Graeme Blackwood, Miguel Ballesteros, and Todd Ward. 2018. Multilingual Neural Machine Translation with\nTask-Specific Attention. In Proceedings of the 27th International Conference on Computational Linguistics . 3112\u20133122.\n[19] Su Lin Blodgett, Solon Barocas, Hal Daum\u00e9 III, and Hanna Wallach. 2020. Language (Technology) is Power: A Critical\nSurvey of \u201cBias\u201d in NLP. InProceedings of the 58th Annual Meeting of the ACL . 5454\u20135476."
      },
      {
        "source_id": "source_13",
        "citation": "No Language Left Behind_ Scaling Human-Centered Machine Translation.pdf",
        "text_snippet": "language by looking at the approximate number of native speakers and other community-level\nvariables relevant to our work.\nNext, for each of the language candidates, we partnered with linguists from various\nspecialized language service providers to understand if each of these languages has a stan-\ndardized written form. We did this because having a reliable, high-quality evaluation\ndataset is critical to accelerated experimental progress. However, prioritizing languages with\nfairly standardized written forms has notable downsides (see Appendix A). For one, many\nlanguages have natural variations and are being written in different standards or scripts in\ndifferent regions. For instance, languages such as Fulah include several distinct varieties and\nlanguages such as Kashmiri and Central Kanuri contain multiple scripts in common use.\nSystematically documenting these dimensions helped us assess how we could best support"
      },
      {
        "source_id": "source_14",
        "citation": "Localising SA official languages.pdf",
        "text_snippet": "Immediate next steps in the project will be to ad-\ndress the issues discussed in the previous section in\norder to reach the target of /five.osf /zero.osf/zero.osf/zero.osf CC/zero.osf sentences per\nlanguage. The South African governmental web-\nsites (*.gov.za) appear in all the o\ufb03cial languages\nand the possibility to obtain additional text from\nthis source will be investigated. Once the required\nnumber of sentences have been collected, the Com-\nmon V oice websites will be ready for speech data col-\nlection to start. The project will be promoted as\nwidely as possible with the aim to encourage lan-\nguage communities across the country to become\ninvolved. Hopefully these e\ufb00orts will be successful\nto the extent that data collection can be followed\nby dedicated speech technology development work-\nshops.\nNotes\n[/one.osf]https://www.undp.org/sustainable-d\nevelopment-goals , https://odi.org/\nen/publications/leave-no-one-beh\nind-index-2019/\n[/two.osf]https://commonvoice.mozilla.org/en"
      },
      {
        "source_id": "source_09",
        "citation": "The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks.pdf",
        "text_snippet": "languages and 13 scripts, with a total of 10K questions.\nOurs In this position paper, we conduct a comprehensive investigation of over 2,000 studies on\nmultilingual evaluation published between 2021 and 2024, following the emergence of large language\nmodels (LLMs). We summarize recent trends in the field of multilingual evaluation and analyze\nwhether widely used multilingual benchmarks accurately reflect human judgments. Additionally,\nwe discuss future directions for multilingual evaluation based on our findings. To the best of our\nknowledge, the work of Yu et al. [2022], which surveys 156 multilingual evaluation studies spanning\nfrom 2008 to 2021, is the most closely related to our research.\n3. Scope, Collection, and Annotation\nIn this section, we outline our approach to determining the scope of datasets included in our study,\ndetail our collection process from arXiv submissions, and describe our annotation methodology. These\n4"
      },
      {
        "source_id": "source_14",
        "citation": "Localising SA official languages.pdf",
        "text_snippet": "Proceedings of the /three.osfnd workshop on Resources for African Indigenous Language (RAIL)\nterial, such as the Bible. These texts represent very\nspeci/f_ic domains, subject matter, as well as writing\nstyle.\nT o mitigate these problems, we investigated the\npossibility of sourcing text data from other web\nsources that are commonly used in language tech-\nnology development. For instance, Mozilla pro-\nvides a set of text processing tools to harvest data\nfrom Wikipedia. The tools could not be used \u201cas\nis\u201d in this project, because the default English rule\nset only allows sentences with ASCII characters.\nHowever, most of the South African languages in-\nclude diacritic markers encoded by UTF-/eight.osf charac-\nters. The rule set therefore had to be adapted to ac-\ncept within-language UTF-/eight.osf characters but to reject\nirrelevant ones.\nOne of the Mozilla selection rules speci/f_ies that only\nthree sentences may be copied from a Wikipedia"
      },
      {
        "source_id": "source_16",
        "citation": "Where Are We? Evaluating LLM Performance on African Languages.pdf",
        "text_snippet": "2020), respectively. Further details about the sta-\ntistical composition of this data is available in Ta-\nble D3.\nTopic Classification. The primary goal for this\ntask is to automatically categorize the content of\nthe text based on its theme. Topic classification is\nwidely used in various applications, such as docu-\nment organization, content recommendation, and\ninformation retrieval. We include topic classifi-\ncation datasets for Yor\u00f9b\u00e1 and Hausa (Hedderich\net al., 2020).\nA.2 Text Generation\nMachine Translation. A robust benchmark for\nMT tasks is essential. SAHARA , in its quest to\neffectively assess machine translation for African\nlanguages, selectively incorporates datasets con-\ntaining diverse African languages into its bench-\nmark. Specifically, SAHARA includes datasets such\nas Pidgin-UNMT7 (Ogueji and Ahia, 2019), Afro-\nMT8 (Reid et al., 2021a), Lafand-MT (Adelani\net al., 2022a), SALT 9 (Akera et al., 2022), and\nHornMT, thus encompassing a total of 45 language\npairs."
      }
    ],
    "time_taken": 1.13
  },
  {
    "question": "What is the 'Bitter Lesson' described by Wu et al. (2025)?",
    "answer": "Insufficient Evidence",
    "citations_readable": [],
    "citations_raw": [],
    "retrieved_chunks": [
      {
        "source_id": "source_09",
        "citation": "The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks.pdf",
        "text_snippet": "The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks\nP. Zhang, Q. Zhu, R. Men, R. Lin, T. Li, T. Xia, X. Ren, X. Ren, Y. Fan, Y. Su, Y. Zhang, Y. Wan,\nY. Liu, Z. Cui, Z. Zhang, and Z. Qiu. Qwen2.5 technical report.CoRR, abs/2412.15115, 2024. doi:\n10.48550/ARXIV.2412.15115. URL https://doi.org/10.48550/arXiv.2412.15115.\nF. Yao, Y. Zhuang, Z. Sun, S. Xu, A. Kumar, and J. Shang. Data contamination can cross language\nbarriers. In Y. Al-Onaizan, M. Bansal, and Y.-N. Chen, editors,Proceedings of the 2024 Conference\non Empirical Methods in Natural Language Processing, pages 17864\u201317875, Miami, Florida, USA,\nNov. 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-main.990.\nURL https://aclanthology.org/2024.emnlp-main.990/.\nX. Yu, T. Chatterjee, A. Asai, J. Hu, and E. Choi. Beyond counting datasets: A survey of multilingual\ndataset construction and necessary resources. In Y. Goldberg, Z. Kozareva, and Y. Zhang, editors,"
      },
      {
        "source_id": "source_09",
        "citation": "The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks.pdf",
        "text_snippet": "The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks\nJ. Tsujii, editors,Proceedings of the 2018 Conference on Empirical Methods in Natural Language\nProcessing, pages 2475\u20132485, Brussels, Belgium, Oct.-Nov. 2018. Association for Computational\nLinguistics. doi: 10.18653/v1/D18-1269. URL https://aclanthology.org/D18-1269/.\nM.R.Costa-juss\u00e0,J.Cross,O.\u00c7elebi,M.Elbayad,K.Heafield,K.Heffernan,E.Kalbassi,J.Lam,D.Licht,\nJ. Maillard, A. Y. Sun, S. Wang, G. Wenzek, A. Youngblood, B. Akula, L. Barrault, G. M. Gonzalez,\nP. Hansanti, J. Hoffman, S. Jarrett, K. R. Sadagopan, D. Rowe, S. Spruit, C. Tran, P. Andrews, N. F.\nAyan, S. Bhosale, S. Edunov, A. Fan, C. Gao, V. Goswami, F. Guzm\u00e1n, P. Koehn, A. Mourachko,\nC. Ropers, S. Saleem, H. Schwenk, and J. Wang. No language left behind: Scaling human-centered\nmachine translation. CoRR, abs/2207.04672, 2022. doi: 10.48550/ARXIV.2207.04672. URL\nhttps://doi.org/10.48550/arXiv.2207.04672."
      },
      {
        "source_id": "source_09",
        "citation": "The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks.pdf",
        "text_snippet": "The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks\nM. Rivi\u00e8re, S. Pathak, P. G. Sessa, C. Hardin, S. Bhupatiraju, L. Hussenot, T. Mesnard, B. Shahriari,\nA. Ram\u00e9, J. Ferret, P. Liu, P. Tafti, A. Friesen, M. Casbon, S. Ramos, R. Kumar, C. L. Lan, S. Jerome,\nA. Tsitsulin, N. Vieillard, P. Stanczyk, S. Girgin, N. Momchev, M. Hoffman, S. Thakoor, J. Grill,\nB. Neyshabur, O. Bachem, A. Walton, A. Severyn, A. Parrish, A. Ahmad, A. Hutchison, A. Abdagic,\nA. Carl, A. Shen, A. Brock, A. Coenen, A. Laforge, A. Paterson, B. Bastian, B. Piot, B. Wu, B. Royal,\nC. Chen, C. Kumar, C. Perry, C. Welty, C. A. Choquette-Choo, D. Sinopalnikov, D. Weinberger,\nD. Vijaykumar, D. Rogozinska, D. Herbison, E. Bandy, E. Wang, E. Noland, E. Moreira, E. Senter,\nE. Eltyshev, F. Visin, G. Rasskin, G. Wei, G. Cameron, G. Martins, H. Hashemi, H. Klimczak-\nPlucinska, H. Batra, H. Dhand, I. Nardini, J. Mein, J. Zhou, J. Svensson, J. Stanway, J. Chan, J. P."
      },
      {
        "source_id": "source_09",
        "citation": "The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks.pdf",
        "text_snippet": "The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks\n1. Introduction\nThe remarkable capabilities of large language models (LLMs) have transformed natural language\nprocessing (NLP), with applications spanning diverse domains and languages worldwide [Ouyang\net al., 2022, Sanh et al., 2022, OpenAI, 2023, Touvron et al., 2023, Anil et al., 2023, Mesnard et al.,\n2024,Yangetal.,2024,DeepSeek-AIetal.,2025]. Asthesetechnologiesincreasinglyserveusersacross\nlinguistic boundaries, robust multilingual evaluation becomes not merely academic but essential [Zhu\net al., 2024, Qin et al., 2025]. Despite advances in multilingual pre-training, significant disparities\npersist in how models perform across different languages, writing systems, and cultural contexts.\nCurrent evaluation frameworks often fail to capture this diversity, with benchmarks predominantly\nfocused on high-resource languages and evaluation methodologies that inadequately represent real-"
      },
      {
        "source_id": "source_29",
        "citation": "NileChat_ Towards Linguistically Diverse and Culturally Aware LLMs for Local Communities.pdf",
        "text_snippet": "Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang\nZhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru\nZhang, and Zihan Qiu. 2025. Qwen2.5 technical\nreport.Preprint, arXiv:2412.15115.\nAlexandre Ram\u00e9, Johan Ferret, Nino Vieillard, Robert\nDadashi, L\u00e9onard Hussenot, Pierre-Louis Cedoz,\nPier Giuseppe Sessa, Sertan Girgin, Arthur Douil-\nlard, and Olivier Bachem. 2024. Warp: On the bene-\nfits of weight averaged rewarded policies.Preprint,\narXiv:2406.16768.\nJonathan Rystr\u00f8m, Hannah Rose Kirk, and Scott Hale.\n2025. Multilingual != multicultural: Evaluating gaps\nbetween multilingual capabilities and cultural align-\nment in llms.Preprint, arXiv:2502.16534.\nE. Sapir. 1929. The status of linguistics as a science.\nLanguage, 5(4):207\u2013214.\nMohamed El Amine Seddik, Suei-Wen Chen, Soufiane\nHayou, Pierre Youssef, and Merouane Abdelkader\nDEBBAH. 2024. How bad is training on synthetic\ndata? a statistical analysis of language model col-\nlapse. InFirst Conference on Language Modeling."
      },
      {
        "source_id": "source_29",
        "citation": "NileChat_ Towards Linguistically Diverse and Culturally Aware LLMs for Local Communities.pdf",
        "text_snippet": "Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zi-\nhan Qiu. 2025. Qwen3 technical report.Preprint,\narXiv:2505.09388.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali\nFarhadi, and Yejin Choi. 2019. HellaSwag: Can a ma-\nchine really finish your sentence? InProceedings of\nthe 57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 4791\u20134800, Florence,\nItaly. Association for Computational Linguistics.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan\nZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,\nZhuohan Li, Dacheng Li, Eric Xing, Hao Zhang,\nJoseph E. Gonzalez, and Ion Stoica. 2023. Judging\nLLM-as-a-judge with MT-bench and chatbot arena.\nInThirty-seventh Conference on Neural Information\nProcessing Systems Datasets and Benchmarks Track.\n15"
      },
      {
        "source_id": "source_13",
        "citation": "No Language Left Behind_ Scaling Human-Centered Machine Translation.pdf",
        "text_snippet": "for Computational Linguistics. URL https://aclanthology.org/2021.wmt-1.73.\nBatya Friedman and David G Hendry. Value sensitive design: Shaping technology with moral\nimagination. MIT Press, 2019.\nPascale Fung and Percy Cheung. Multi-level bootstrapping for extracting parallel sentences\nfrom a quasi-comparable corpus. In COLING 2004, 20th International Conference on\nComputational Linguistics, Proceedings of the Conference, 23-27 August 2004, Geneva,\nSwitzerland, 2004. URL https://aclanthology.org/C04-1151/.\nSamuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A. Smith.\nRealtoxicityprompts: Evaluating neural toxic degeneration in language models. CoRR,\nabs/2009.11462, 2020. URL https://arxiv.org/abs/2009.11462.\nFantahun Gereme, William Zhu, Tewodros Ayall, and Dagmawi Alemu. Combating fake\nnews in \u201clow-resource\u201d languages: Amharic fake news detection accompanied by resource\ncrafting. Information, 12(1):20, 2021."
      },
      {
        "source_id": "source_09",
        "citation": "The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks.pdf",
        "text_snippet": "The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks\nlanguages, including English, Chinese, French, German, Spanish, and Russian, with 10K instructions\nfor each language. We employQ w e n 2 . 5 - M a xto categorize the instructions. We provide 5 seed\ncategories: Greeting, Writing, Translation, Math, andProgramming, but allow the model to introduce\nnew categories.\nEnglish Chinese French German Spanish Russian\nLanguage\n0\n20\n40\n60\n80\n100Percentage (%)\n43.9%\n33.5% 34.0%\n46.2%\n33.8% 36.0%\n14.3%\n16.5% 16.9%\n15.8%\n13.3% 12.2%\n13.9%\n16.4% 13.0%\n6.2%\n10.3%\n17.4%\n4.9% 6.1%\n5.7%\n5.9%\n3.7%\n3.8% 4.0% 4.6%\n5.1%\n6.9%\n22.1% 24.7% 25.4% 23.4%\n31.7%\n23.8%\nwriting\ncommonsense\nprogramming\ntranslation\nmath\nhealth\nOthers\nFigure 7| Distribution of user instruction categories\nacross six languages. We discard the \u201cGreetings\"\ncategory, as it is not a task-oriented instruction.\nUsersfromdifferentcountriessharecommon\ninterests. We present the distribution of user"
      },
      {
        "source_id": "source_09",
        "citation": "The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks.pdf",
        "text_snippet": "The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks\n2021 2022 2023 2024\nYear\n0\n20\n40\n60\n80\n100Percentage (%)\n5.7% 6.2% 4.1% 4.9%\n4.8% 6.8%\n4.0% 5.4%\n5.0%\n5.7%\n5.1%\n7.1%\n10.4%\n15.2%\n15.8%\n16.4%\n14.3%\n16.2%\n14.3%\n13.3%\n60.0%\n49.9%\n56.7% 53.0%\nIndia\nUK\nGermany\nChina\nUSA\nOthers\n(a) Country Distributions\nCanada\nUSA\nIndonesia\nRussia\nSouth Africa\nMexico\nBrazil\nNigeria\nIsrael\nUAE\nQatar\nThailandVietnam\nSouth Korea\nIndia\nBangladesh\nPakistanIran\nEU Turkey\nAustralia\nSri Lanka\nChina\nUK\nPhilippines\nSingapore\nJapan\nSaudi Arabia\nEgypt\nEthiopia\nUganda\nAcademic\nIndustry\nHybrid (b) Affiliation Type Distributions of Top 50 countries\nFigure 6 | (a) Top 5 countries in multilingual benchmark creation from 2021 to 2024. (b) Affiliation\ntype distributions of the top 50 countries in multilingual benchmark creation. We merge the countries\nin European Union (EU) into one category for better visualization.\nCountries and Institutions Figure 6(a) highlights the countries leading multilingual benchmark"
      },
      {
        "source_id": "source_33",
        "citation": "AfroBench_ How Good are Large Language Models on African Languages?.pdf",
        "text_snippet": "ary 13, 2025.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Gray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions with\nhuman feedback. In Advances in Neural Information\nProcessing Systems.\nRicardo Rei, Craig Stewart, Ana C Farinha, and Alon\nLavie. 2020. COMET: A neural framework for MT\n14"
      },
      {
        "source_id": "source_18",
        "citation": "Generative AI and Large Language Models in Language Preservation_  Opportunities and Challenges.pdf",
        "text_snippet": "and mitigating bias in ai algorithms,\u201d in 2025 3rd IEEE International\nConference on Business Analytics for Technology and Security (ICBATS-\n2025), Dubai, United Arab Emirates, May 1-2 2025.\n[7] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, L. u. Kaiser, and I. Polosukhin, \u201cAttention is all you need,\u201d in\nAdvances in Neural Information Processing Systems , I. Guyon, U. V .\nLuxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and\nR. Garnett, Eds., vol. 30. Curran Associates, Inc., 2017. [Online].\nAvailable: https://proceedings.neurips.cc/paper files/paper/2017/file/3f\n5ee243547dee91fbd053c1c4a845aa-Paper.pdf\n[8] D. H. Hagos, R. Battle, and D. B. Rawat, \u201cRecent advances in generative\nai and large language models: Current status, challenges, and perspec-\ntives,\u201d IEEE Transactions on Artificial Intelligence , vol. 5, no. 12, pp.\n5873\u20135893, 2024.\n[9] Z. Schillaci, LLM Adoption Trends and Associated Risks . Cham:"
      },
      {
        "source_id": "source_12",
        "citation": "A Survey on Large Language Models with Multilingualism_ Recent Advances and New Frontiers.pdf",
        "text_snippet": "arXiv:2306.14824, 2023.\n[325] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning.\nAdvances in neural information processing systems, 36, 2024.\n[326] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces.\narXiv preprint arXiv:2312.00752, 2023.\n[327] Libo Qin, Qiguang Chen, Fuxuan Wei, Shijue Huang, and Wanxiang Che. Cross-lingual\nprompting: Improving zero-shot chain-of-thought reasoning across languages. In Proceed-\nings of the 2023 Conference on Empirical Methods in Natural Language Processing , pages\n2695\u20132709, 2023.\n[328] Julen Etxaniz, Gorka Azkune, Aitor Soroa, Oier Lopez de Lacalle, and Mikel Artetxe. Do\nmultilingual language models think better in english? arXiv preprint arXiv:2308.01223 ,\n2023.\n[329] Maarten Sap, Vered Shwartz, Antoine Bosselut, Yejin Choi, and Dan Roth. Commonsense\nreasoning for natural language processing. In Proceedings of the 58th Annual Meeting of the"
      }
    ],
    "time_taken": 1.43
  },
  {
    "question": "List the datasets used in the 'IrokoBench' benchmark.",
    "answer": "The datasets used in the 'IrokoBench' benchmark are AfriXNLI, AfriMGSM, and AfriMMLU.",
    "citations_readable": [
      "IrokoBench_ A New Benchmark for African Languages in the Age of Large  Language Models.pdf"
    ],
    "citations_raw": [
      "source_08"
    ],
    "retrieved_chunks": [
      {
        "source_id": "source_08",
        "citation": "IrokoBench_ A New Benchmark for African Languages in the Age of Large  Language Models.pdf",
        "text_snippet": "10Ontario Tech University, 11Lancaster University, 12Princeton university, 13Makerere University,\n14Leibniz Universit\u00e4t Hannover, 15Le CNAM, 16Imperial College London, 17Universidad de Deusto, 18DAUST,\n19Haramaya University.\nAbstract\nDespite the widespread adoption of Large lan-\nguage models (LLMs), their remarkable capa-\nbilities remain limited to a few high-resource\nlanguages. Additionally, many low-resource\nlanguages ( e.g., African languages) are of-\nten evaluated only on basic text classifica-\ntion tasks due to the lack of appropriate or\ncomprehensive benchmarks outside of high-\nresource languages. In this paper, we intro-\nduce IrokoBench\u2014a human-translated bench-\nmark dataset for 17 typologically-diverse low-\nresource African languages covering three\ntasks: natural language inference (AfriXNLI),\nmathematical reasoning (AfriMGSM), and\nmulti-choice knowledge-based question an-\nswering (AfriMMLU). We use IrokoBench to\nevaluate zero-shot, few-shot, and translate-test"
      },
      {
        "source_id": "source_08",
        "citation": "IrokoBench_ A New Benchmark for African Languages in the Age of Large  Language Models.pdf",
        "text_snippet": "prises three datasets focused on different tasks: nat-\nural language inference (AfriXNLI), multi-choice\nknowledge QA (AfriMMLU), and mathematical\nreasoning (AfriMGSM). Unlike previous bench-\nmarks, which primarily involve simple text classi-\nfication tasks, these datasets assessed the LLMs\u2019\nabilities in complex and knowledge-intensive ar-\neas. Our evaluation revealed a significant perfor-\nmance gap between high-resource languages (e.g.,\nEnglish and French) and African languages. Ad-\nditionally, we observed a substantial disparity in\nperformance between open models and proprietary\nmodels, with the latter generally outperforming\nthe former, particularly in mathematical reasoning\ntasks. We hope that IROKO BENCH will serve as\na valuable benchmark for evaluating future LLMs\ndeveloped or adapted for African languages.\nLimitations Our benchmark has a few limita-\ntions: (1) The benchmark is human-translated\nwhich may include some translationese effects, it"
      },
      {
        "source_id": "source_08",
        "citation": "IrokoBench_ A New Benchmark for African Languages in the Age of Large  Language Models.pdf",
        "text_snippet": "based QA. The datasets were created by hu-\nman translating a subset of English cross-lingual\nNLI (XNLI) (Conneau et al., 2018), English Mul-\ntilingual Grade School Math (MGSM) (Shi et al.,\n2023), and Massive Multitask Language Under-\nstanding (MMLU) (Hendrycks et al., 2021a), eval-\nuation datasets into each of the 16 languages using\nprofessional translators.\nWe conduct a large-scale evaluation of\nIROKO BENCH to assess zero-shot, few-shot, and\ntranslate-test settings (where test sets are translated\ninto English) performance across 10 open and six\nproprietary LLMs. Our main contributions can be\nenumerated as follows:\n1. We introduce and release IROKO BENCH , a\nhuman-translated benchmark that includes 16\nlanguages from various geographical regions\nin Africa, all with varying degrees of \u201clow-\nresourcedness\u201d (Joshi et al., 2020).\n2. Sharp cliff in performance across all mod-\nels on low-resource languages Our evalua-\ntion shows a large gap ( \u223c45% on average) be-"
      },
      {
        "source_id": "source_08",
        "citation": "IrokoBench_ A New Benchmark for African Languages in the Age of Large  Language Models.pdf",
        "text_snippet": "and time investment required for human translation,\nit often suffers from noise and biases (Vanmassen-\nhove et al., 2021; Lee et al., 2022; Khiu et al., 2024;\nHartung et al., 2023; Savoldi et al., 2021) or fail\nto reflect cultural context (Wang et al., 2022; Ji\net al., 2023; Pudjiati et al., 2022). Automatic cura-\ntion may also amplify any of the ubiquitous issues\nwith the quality of broad pretraining sets (Luccioni\nand Viviano, 2021; Kreutzer et al., 2022; Ferrara,\n2023).\nIn this paper, we seek to address both the\ndiversity and breadth of evaluation coverage.\nWe introduce IROKO BENCH , a human curated\nbenchmark dataset for 17 typologically diverse\nAfrican languages which encompasses three com-\nplex tasks: natural language inference (NLI), math-\nematical reasoning, and multi-choice knowledge-\nbased QA. The datasets were created by hu-\nman translating a subset of English cross-lingual\nNLI (XNLI) (Conneau et al., 2018), English Mul-\ntilingual Grade School Math (MGSM) (Shi et al.,"
      },
      {
        "source_id": "source_08",
        "citation": "IrokoBench_ A New Benchmark for African Languages in the Age of Large  Language Models.pdf",
        "text_snippet": "IrokoBench: A New Benchmark for African Languages in the Age of Large\nLanguage Models\nDavid Ifeoluwa Adelani1,2\u2217, Jessica Ojo1,3\u2217, Israel Abebe Azime4\u2217, Jian Yun Zhuang5,\nJesujoba O. Alabi4, Xuanli He6, Millicent Ochieng7, Sara Hooker8, Andiswa Bukula9,\nEn-Shiun Annie Lee10, Chiamaka Chukwuneke11, Happy Buzaaba12, Blessing Sibanda\u2217,\nGodson Kalipe\u2217, Jonathan Mukiibi13\u2217, Salomon Kabongo14\u2217, Foutse Yuehgoh15\u2217,\nMmasibidi Setaka9, Lolwethu Ndolela\u2217, Nkiruka Odu\u2217, Rooweither Mabuya9,\nShamsuddeen Hassan Muhammad16, Salomey Osei17\u2217, Sokhar Samb18\u2217,\nTadesse Kebede Guge19\u2217, Tombekai Vangoni Sherman20, Pontus Stenetorp6\n\u2217Masakhane NLP, 1Mila, McGill University, 2Canada CIFAR AI Chair, 3Lelapa AI, 4Saarland University,\n5University of Toronto, 6University College London, 7Microsoft Research Africa, 8Cohere For AI, 9SADiLaR,\n10Ontario Tech University, 11Lancaster University, 12Princeton university, 13Makerere University,"
      },
      {
        "source_id": "source_08",
        "citation": "IrokoBench_ A New Benchmark for African Languages in the Age of Large  Language Models.pdf",
        "text_snippet": "1Our benchmark name comes from \u00ccr\u00f3k\u00f2\u2014 a large hard-\nwood in West Africa that is very durable for making bench.\nlanguages are from three language families in\nAfrica: one from Mande, three from Afro-Asiatic\nand 13 from the Niger-Congo family\u2014where we\ncover eight Bantu languages. Table 9 provides an\noverview of the languages covered, including their\nfamily, regions, and the number of native speakers.\n3.2 Tasks covered by I ROKO BENCH\nThe selection of these tasks is primarily driven by\ntheir coverage across various domains and down-\nstream tasks for diverse use cases. Additionally,\nthey enable the evaluation of logical, abstract, and\nreasoning capabilities in LLMs, which is the hall-\nmark of human intelligence (Bowman et al., 2015;\nHendrycks et al., 2021b). Figure 1 provides exam-\nples of the different tasks covered in our datasets.\nWe provide their descriptions below:\nAfriXNLI The task of NLI involves the classi-\nfication of a pair of sentences\u2014a premise and a"
      },
      {
        "source_id": "source_08",
        "citation": "IrokoBench_ A New Benchmark for African Languages in the Age of Large  Language Models.pdf",
        "text_snippet": "AfriXNLI focuses on natural language inference between two pairs of sentences. For clarity, this figure provides\nexamples in English.\nbenchmark datasets for African languages to en-\nable research on these languages. Initiatives such as\nMasakhane have been instrumental in the creation\nof standard benchmark for tasks such as machine\ntranslation (Adelani et al., 2022a), named entity\nrecognition (Adelani et al., 2021, 2022b), part of\nspeech tagging (Dione et al., 2023), news topic\nclassification (Adelani et al., 2023), and sentiment\nanalysis (Muhammad et al., 2023). There are also\nseveral multilingual benchmark datasets that cover\na few African languages, such as SIB-200 (Adelani\net al., 2024), Flores (Goyal et al., 2022; Costa-juss\u00e0\net al., 2024), Aya dataset and Collection (Singh\net al., 2024) and Taxi1500 (Ma et al., 2023). How-\never, despite all these efforts, African languages\nstill lack quality and more difficult datasets, our\nwork fills this gap.\n3 I ROKO BENCH"
      },
      {
        "source_id": "source_08",
        "citation": "IrokoBench_ A New Benchmark for African Languages in the Age of Large  Language Models.pdf",
        "text_snippet": "IROKO BENCH (amh, hau, ibo, kin, orm, sna, sot,\nswa, xho, yor, and zul). AfroXLMR-76L fol-\nlows the same technique by performing continual\npre-training on XLM-R-large on 76 languages (72\nAfrican), all languages covered in IROKO BENCH\nare part of its pre-training.\nWe found Africa-centric MLM to perform bet-\nter on average than massively multilingual mod-\nels like XLM-R-large. Serengeti and AfroXLMR-\n15"
      },
      {
        "source_id": "source_33",
        "citation": "AfroBench_ How Good are Large Language Models on African Languages?.pdf",
        "text_snippet": "vidual tasks within the benchmark. This enables us\nto directly compare the performance of specialized\nmodels to general-purpose LLMs.\nTable 2 summarizes the tasks, the dataset used,\nnumber of languages covered, and total size.\n3.1 Languages\nWe cover 64 African languages from seven lan-\nguage families (Afro-Asiatic, Atlantic-Congo, Aus-\ntronesian, Indo-European, Mande, Nilotic, and\n3"
      },
      {
        "source_id": "source_08",
        "citation": "IrokoBench_ A New Benchmark for African Languages in the Age of Large  Language Models.pdf",
        "text_snippet": "No. of No. instances Average Length\nDataset languages Train/Dev/Test Subjects / Domains Train/Dev/Test\nAfriMGSM 17 (excl. Swahili, inc. Vai) 8 / - / 250 grade school mathematics 25 / - / 46\nAfriMMLU 17 (incl. French) 25 / 83 / 500 elementary mathematics, high-school geography, Interna-\ntional law, global facts, high school microeconomics\n18 / 17 / 17\nAfriXNLI 15 (excl. Swahili) - / 450 / 600 face-to-face, telephone, oxford university press (oup), fic-\ntion, travel, government, nineeleven, letters, slate, verbatim\n- / 10 / 10 (hyp.)\n- / 18 / 17 (pre.)\nTable 1: The IROKO BENCH datasets: dataset name, number of African languages covered, data split, and the\nsubjects or domains covered. We included English, French, and Swahili in all benchmarks.\nAfriMMLU This is a multi-choice knowledge\nQA curated from freely available online sources by\nundergraduate and graduate students in the USA.\nThe subjects cover simple general knowledge ques-\ntions like \u201cglobal fact\u201d to highly-technical ques-"
      },
      {
        "source_id": "source_27",
        "citation": "Cross-lingual transfer of multilingual models on low resource African Languages.pdf",
        "text_snippet": "the monolingual and multilingual models on dis-\ntinct datasets (from the same language family) by\nfollowing a training pipeline to train these models\nwith their best hyperparameters.\n3.1 Dataset\nThis study employs 2 distinct datasets, one in Kin-\nyarwanda and the other in Kirundi sourced from\n(Niyongabo et al., 2020).\nField Description\nlabel Numerical labels ranging from 1 to 14\nen_label English labels\nkin_label Kinyarwanda labels\nkir_label Kirundi labels\nurl The link to the news source\ntitle The title of the news article\ncontent The full content of the news article\nTable 1: Field descriptions of the raw dataset\nField Description\nlabel Numerical labels ranging from 1 to 14\ntitle The title of the news article\ncontent The full content of the news article\nTable 2: Field descriptions of the cleaned dataset\nFor the Kinyarwanda dataset, news articles from\nvarious websites and newspapers were used. A total\nof 21268 articles are distributed across 14 classes,"
      },
      {
        "source_id": "source_09",
        "citation": "The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks.pdf",
        "text_snippet": "The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks\nM. Rivi\u00e8re, S. Pathak, P. G. Sessa, C. Hardin, S. Bhupatiraju, L. Hussenot, T. Mesnard, B. Shahriari,\nA. Ram\u00e9, J. Ferret, P. Liu, P. Tafti, A. Friesen, M. Casbon, S. Ramos, R. Kumar, C. L. Lan, S. Jerome,\nA. Tsitsulin, N. Vieillard, P. Stanczyk, S. Girgin, N. Momchev, M. Hoffman, S. Thakoor, J. Grill,\nB. Neyshabur, O. Bachem, A. Walton, A. Severyn, A. Parrish, A. Ahmad, A. Hutchison, A. Abdagic,\nA. Carl, A. Shen, A. Brock, A. Coenen, A. Laforge, A. Paterson, B. Bastian, B. Piot, B. Wu, B. Royal,\nC. Chen, C. Kumar, C. Perry, C. Welty, C. A. Choquette-Choo, D. Sinopalnikov, D. Weinberger,\nD. Vijaykumar, D. Rogozinska, D. Herbison, E. Bandy, E. Wang, E. Noland, E. Moreira, E. Senter,\nE. Eltyshev, F. Visin, G. Rasskin, G. Wei, G. Cameron, G. Martins, H. Hashemi, H. Klimczak-\nPlucinska, H. Batra, H. Dhand, I. Nardini, J. Mein, J. Zhou, J. Svensson, J. Stanway, J. Chan, J. P."
      }
    ],
    "time_taken": 1.81
  },
  {
    "question": "What is the main contribution of the 'No Language Left Behind' project?",
    "answer": "The main contribution of the 'No Language Left Behind' project is the development of high-quality machine translation systems for over 200 languages, particularly focusing on low-resource languages. This includes creating datasets and models to narrow the performance gap between low and high-resource languages, using novel data mining techniques, and developing a conditional compute model based on Sparsely Gated Mixture of Experts. The project also introduced Flores-200, a high-quality human-translated evaluation dataset, and NLLB-Seed, a dataset comprising human-translated bitext for 43 languages.",
    "citations_readable": [
      "No Language Left Behind_ Scaling Human-Centered Machine Translation.pdf"
    ],
    "citations_raw": [
      "source_13"
    ],
    "retrieved_chunks": [
      {
        "source_id": "source_13",
        "citation": "No Language Left Behind_ Scaling Human-Centered Machine Translation.pdf",
        "text_snippet": "interventions aimed at more fundamental issues surrounding education, internet access, and\ndigital literacy are imperative to eradicating the structural problem of language disparities.\nWe are committed to working together with various stakeholders as we continue our path to\nmaterialize translation technologies that make the web a more accessible place, regardless of\nthe language one speaks.\n10. Contributions\nWe outline the contributions of each member of No Language Left Behind, grouped by\nsection and sorted alphabetically by last name. Each person is only mentioned once even\nthough many contributed to several areas. No amount of space could fully describe the\npassion and contributions of every single person involved in bringing this effort to life.\nData\nBapi Akula - monolingual data pipeline to go from CommonCrawl to deduplicated, filtered\nsentences\nPierre Andrews - engineering lead for data, led the development of stopes, mining and\nmonolingual cleaning pipelines\n131"
      },
      {
        "source_id": "source_13",
        "citation": "No Language Left Behind_ Scaling Human-Centered Machine Translation.pdf",
        "text_snippet": "Abstract\nDriven by the goal of eradicating language barriers on a global scale, machine translation\nhas solidified itself as a key focus of artificial intelligence research today. However, such\nefforts have coalesced around a small subset of languages, leaving behind the vast majority\nof mostly low-resource languages. What does it take to break the 200 language barrier\nwhile ensuring safe, high quality results, all while keeping ethical considerations in mind?\nIn No Language Left Behind , we took on this challenge by first contextualizing the need\nfor low-resource language translation support through exploratory interviews with native\nspeakers. Then, we created datasets and models aimed at narrowing the performance gap\nbetween low and high-resource languages. More specifically, we developed a conditional\ncompute model based on Sparsely Gated Mixture of Experts that is trained on data obtained\nwith novel and effective data mining techniques tailored for low-resource languages. We"
      },
      {
        "source_id": "source_13",
        "citation": "No Language Left Behind_ Scaling Human-Centered Machine Translation.pdf",
        "text_snippet": "visibility on a global scale, which could compel local institutions to take native languages\nmore seriously and invest more resources into preserving or teaching them. This perspective\nunderscores both the symbolic and material benefits machine translation could bring. The\npositive encouragements from low-resource language speakers throughout the course of the\nstudy remind us that by taking a human-centric approach and focusing on languages that\nhave historically been left behind, we can help communities maintain a connection to their\nnative languages\u2014a quintessential part of many people\u2019s culture and identity.\n2.2 No Language Left Behind: Guiding Principles\nCombining insights drawn from interviews with low-resource language speakers and good\npractices distilled from literature on responsible AI (Arrieta et al., 2020; Bender et al., 2021;\nBlodgett et al., 2022; Paullada et al., 2021; Sambasivan and Holbrook, 2018), we introduce\nfour key guiding principles underlying our research:"
      },
      {
        "source_id": "source_13",
        "citation": "No Language Left Behind_ Scaling Human-Centered Machine Translation.pdf",
        "text_snippet": "Figure 1: No Language Left Behind: Our low-resource translation effort focuses on four\ncornerstones. (1) We strive to understand the low-resource translation problem from the\nperspective of native speakers. (2) We study how to automatically create training data to\nmove low-resource languages towards high-resource. (3) We utilize this data to create\nstate-of-the-art translation models. (4) We evaluate every language we aim to translate.\n2016), allowing related languages to learn from one another (Arivazhagan et al., 2019; Fan\net al., 2020; Zhang et al., 2020). While multilingual models have demonstrated promising\nperformance improvement compared to bilingual models (Tran et al., 2021), enabling the\nrepresentation of hundreds of languages while retaining strong translation quality remains\nan open area of research. Another strategy aimed at mitigating the low-resource challenge\nis to acquire more language data. Some of these attempts have focused on collecting human"
      },
      {
        "source_id": "source_01",
        "citation": "Voices Unheard_ NLP Resources and Models for Yoru\u0300ba\u0301 Regional Dialects.pdf",
        "text_snippet": "the Study of Endangered Languages , pages 13\u201319,\nDublin, Ireland. Association for Computational Lin-\nguistics.\nNLLB Team, Marta R. Costa-juss\u00e0, James Cross, Onur\n\u00c7elebi, Maha Elbayad, Kenneth Heafield, Kevin Hef-\nfernan, Elahe Kalbassi, Janice Lam, Daniel Licht,\nJean Maillard, Anna Sun, Skyler Wang, Guillaume\nWenzek, Al Youngblood, Bapi Akula, Loic Bar-\nrault, Gabriel Mejia Gonzalez, Prangthip Hansanti,\nJohn Hoffman, Semarley Jarrett, Kaushik Ram\nSadagopan, Dirk Rowe, Shannon Spruit, Chau\nTran, Pierre Andrews, Necip Fazil Ayan, Shruti\nBhosale, Sergey Edunov, Angela Fan, Cynthia\nGao, Vedanuj Goswami, Francisco Guzm\u00e1n, Philipp\nKoehn, Alexandre Mourachko, Christophe Ropers,\nSafiyyah Saleem, Holger Schwenk, and Jeff Wang.\n2022. No language left behind: Scaling human-\ncentered machine translation.\nDaniel van Niekerk, Charl van Heerden, Marelie Davel,\nNeil Kleynhans, Oddur Kjartansson, Martin Jansche,\nand Linne Ha. 2017. Rapid development of tts cor-"
      },
      {
        "source_id": "source_13",
        "citation": "No Language Left Behind_ Scaling Human-Centered Machine Translation.pdf",
        "text_snippet": "explore the importance of this research to those who matter the most to us: low-resource\nlanguage communities. Inspired by Value Sensitive Design (Friedman and Hendry, 2019;\nVan Der Hoven and Manders-Huits, 2020), we attribute community-level interests and values\nas the cornerstone of our research. Adopting this framework propels us to start with people\nand prioritize how they interact with technology, with direct emphasis on ethical and social\nconsiderations (Mukhija et al., 2021). To understand how low-resource language speakers\nperceive machine translation, we conducted an interview study with 44 low-resource language\nspeakers. As stakeholders likely to be impacted by No Language Left Behind (NLLB), their\ncontributions helped us envision the promises many believe machine translation could deliver\nto their communities. Punctuating their careful optimism were concrete suggestions on\nways to maximize social gains while minimizing risks. Moreover, many interviewees painted"
      },
      {
        "source_id": "source_13",
        "citation": "No Language Left Behind_ Scaling Human-Centered Machine Translation.pdf",
        "text_snippet": "and metadata for mined bitext data. All materials and code are accessible at https:\n//github.com/facebookresearch/fairseq/tree/nllb.\n9. No Language Left Behind: Social Impact & Concluding Thoughts\nIn this effort, we took on the challenge of creating high-quality machine translation systems\nfor 200+ languages. Faced with major obstacles such as the lack of reliable evaluation\nand training data, progress in low-resource translation has been slow compared to its high-\nresource counterpart. In NLLB, we use novel approaches to make several major contributions\naimed at bridging these gaps: (1) Flores-200, a high-quality human-translated evaluation\ndataset, and NLLB-Seed, a dataset comprising of human-translated bitext for 43 languages,\n(2) a novel bitext mining method that creates hundreds of millions of aligned training\nsentences for low-resource languages using our open-source mining library stopes and"
      },
      {
        "source_id": "source_05",
        "citation": "Exploring NLP Benchmarks in an Extremely Low-Resource Setting.pdf",
        "text_snippet": "Gao, V edanuj Goswami, Francisco Guzm\u00e1n, Philipp\nKoehn, Alexandre Mourachko, Christophe Rop-\ners, Sa\ufb01yyah Saleem, Holger Schwenk, and Jeff\nWang. 2022.No language left behind: Scaling\nhuman-centered machine translation.arXiv preprint\narXiv:2207.04672, v3. Thu, 25 Aug 2022.\nKhanh-Tung Tran, Barry O\u2019Sullivan, and Hoang D.\nNguyen. 2024.Irish-based large language model\nwith extreme low-resource settings in machine\ntranslation. InProceedings of the Seventh Work-\nshop on Technologies for Machine Translation of\nLow-Resource Languages (LoResMT 2024), page\n193\u2013202.\nSami Ul Haq, Rudali Huidrom, and Sheila Castilho.\n2024.Dcu adapt at wmt24: English to low-resource\nmulti-modal translation task. InProceedings of\nthe Ninth Conference on Machine Translation, page\n810\u2013814.\nUNESCO. 2010.Atlas of the World\u2019s Languages in\nDanger. UNESCO Publishing, Paris, France.\nGorka Urbizu, Inaki San Vicente, Xabier Saralegi, Ro-\ndrigo Agerri, and Aitor Soroa. 2022. Basqueglue:"
      },
      {
        "source_id": "source_13",
        "citation": "No Language Left Behind_ Scaling Human-Centered Machine Translation.pdf",
        "text_snippet": "8.7 Effectively Including Languages with Multiple Scripts and Related Languoids 120\n8.8 Environmental Impact of NLLB . . . . . . . . . . . . . . . . . . . . . . . . . 127\n9 No Language Left Behind: Social Impact & Concluding Thoughts 128\n9.1 Expanding Information Access . . . . . . . . . . . . . . . . . . . . . . . . . 129\n9.2 The Janus-faced Nature of Digital Participation . . . . . . . . . . . . . . . . 129\n9.3 The Future of NLLB: A Collective Responsibility . . . . . . . . . . . . . . . 130\n10 Contributions 131\n11 Acknowledgements 133\nA Languages 167\nB Evaluation 168\nC Data 170\nD Modeling 174\nE Bringing it All T ogether 175\nF Model Card - NLLB-200 184\nG Data Card for NLLB-Seed Data 187\nH Data Card for NLLB Multi-Domain Data 189\nI Data Card for Mined Bitext Metadata 191\n3"
      },
      {
        "source_id": "source_16",
        "citation": "Where Are We? Evaluating LLM Performance on African Languages.pdf",
        "text_snippet": "ing the limitations of cross-lingual transfer learn-\ning for underrepresented languages. Meta AI\u2019s\nNo Language Left Behind (NLLB) project (Costa-\njuss\u00e0 et al., 2022) attempted to improve multilin-\ngual models for low-resource languages, but perfor-\nmance remained inconsistent, particularly in gener-\nation tasks. Despite these advances, African NLP\nstill requires increased investment in dataset cura-\ntion, inclusive policy frameworks, and model de-\nvelopment tailored to the linguistic diversity of the\ncontinent. We now introduce SAHARA .\n4\n Sahara Benchmark\nOur objective is to create a comprehensive bench-\nmark for African NLP that enables (i) analysis of\nexisting language resources and (ii) the assessment\nof language models and tracking of the progress\nof African NLP. To achieve this goal, we develop\nSAHARA , adhering to several key design princi-\nples that we will now elucidate. SAHARA estab-\nlishes a comprehensive and adaptable benchmark"
      },
      {
        "source_id": "source_13",
        "citation": "No Language Left Behind_ Scaling Human-Centered Machine Translation.pdf",
        "text_snippet": "content could then be channeled to performing other more important tasks.\nBecause language is intrinsically tied to culture, for many low-resource languages facing\nendangerment, the threat of losing one\u2019s language could also mean the erosion of one\u2019s\nheritage (Sallabank, 2013). NLLB could motivate more low-resource language writers or\ncontent creators to share localized knowledge or various aspects of their culture with both\ncultural insiders and outsiders through social media platforms or websites like Wikipedia.\nGiving individuals access to new translation tools could thus open up a valuable avenue for\nbidirectional learning. In the long run, such generative processes could create dents on the\nglobal knowledge system, challenge Western-centric modes of knowledge production and\ndissemination, and aid in the revitalization of certain minority cultures and languages (Bird,\n2019; Bird and Chiang, 2012).\n9.2 The Janus-faced Nature of Digital Participation"
      },
      {
        "source_id": "source_13",
        "citation": "No Language Left Behind_ Scaling Human-Centered Machine Translation.pdf",
        "text_snippet": "framework and a longitudinal outlook. While its delivery could bring benefits to several\nstakeholders, including low-resource language groups and the scientific community at large,\nwe also recognize that such an intervention has its potential downsides. As such, we reflect on\nthe possibilities and limitations of NLLB, and ways to maximize its benefits while minimizing\nharm.\n9.1 Expanding Information Access\nIn the summer of 2016, the United Nations declared internet access as a basic human\nright (Howell and West, 2016). While the intent of this declaration was to compel countries\nto limit censorship and allow for information and ideas to flow without interference, much of\nthe internet remains inaccessible to many due to language barriers. NLLB has the potential\nto alter the status quo by making the internet more accessible for many.\nFor many low-resource language communities, NLLB\u2019s offering would be the first model"
      }
    ],
    "time_taken": 2.18
  },
  {
    "question": "How does 'AfriCOMET' improve upon standard COMET metrics?",
    "answer": "'AfriCOMET' improves upon standard COMET metrics by enhancing the evaluation for under-resourced African languages. It addresses the lack of evaluation data with human ratings and the complexity of annotation guidelines by creating high-quality human evaluation data with simplified MQM guidelines. AfriCOMET also surpasses the cutting-edge COMET22 system across all three correlation coefficients, indicating its superior performance. Additionally, it leverages a pre-trained model, AfroXLM-R, which is enhanced for African languages, allowing for effective transfer learning from well-resourced DA data.",
    "citations_readable": [
      "AfriMTE and AfriCOMET_ Enhancing COMET to Embrace Under-resourced African Languages.pdf"
    ],
    "citations_raw": [
      "source_17"
    ],
    "retrieved_chunks": [
      {
        "source_id": "source_17",
        "citation": "AfriMTE and AfriCOMET_ Enhancing COMET to Embrace Under-resourced African Languages.pdf",
        "text_snippet": "under-resourced African languages, accurately\nmeasuring this progress remains challenging,\nsince evaluation is often performed on n-gram\nmatching metrics such as BLEU, which typi-\ncally show a weaker correlation with human\njudgments. Learned metrics such as COMET\nhave higher correlation; however, the lack of\nevaluation data with human ratings for under-\nresourced languages, complexity of annotation\nguidelines like Multidimensional Quality Met-\nrics (MQM), and limited language coverage\nof multilingual encoders have hampered their\napplicability to African languages. In this pa-\nper, we address these challenges by creating\nhigh-quality human evaluation data with sim-\nplified MQM guidelines for error detection and\n\u2217 The authors contribute equally to this work and are\nconsidered co-third authors.\n\u2020Currently at the University of Oxford, UK.\ndirect assessment (DA) scoring for 13 typo-\nlogically diverse African languages. Further-\nmore, we develop AFRI COMET : COMET"
      },
      {
        "source_id": "source_17",
        "citation": "AfriMTE and AfriCOMET_ Enhancing COMET to Embrace Under-resourced African Languages.pdf",
        "text_snippet": "AfriMTE and AfriCOMET: Enhancing COMET to Embrace\nUnder-resourced African Languages\nJiayi Wang1, David Ifeoluwa Adelani1,2, Sweta Agrawal3,\u2217, Marek Masiak1,\u2217,\u2020, Ricardo Rei4,5,6,\nEleftheria Briakou3, Marine Carpuat3, Xuanli He1, Sofia Bourhim7, Andiswa Bukula8,\nMuhidin Mohamed9, Temitayo Olatoye10, Tosin Adewumi11, Hamam Mokayed11,\nChristine Mwase12, Wangui Kimotho2, Foutse Yuehgoh13, Anuoluwapo Aremu2,\nJessica Ojo14,2, Shamsuddeen Hassan Muhammad15,2,29, Salomey Osei16,2,\nAbdul-Hakeem Omotayo17,2, Chiamaka Chukwuneke18,2, Perez Ogayo2, Oumaima Hourrane2,\nSalma El Anigri19, Lolwethu Ndolela2, Thabiso Mangwana2, Shafie Abdi Mohamed20,\nAyinde Hassan21, Oluwabusayo Olufunke Awoyomi22, Lama Alkhaled11, Sana Al-Azzawi11,\nNaome A. Etori23, Millicent Ochieng24, Clemencia Siro25, Samuel Njoroge26, Eric Muchiri2,\nWangari Kimotho27, Lyse Naomi Wamba Momo28, Daud Abolade2, Simbiat Ajao2,\nIyanuoluwa Shode2, Ricky Macharm2, Ruqayya Nasir Iro29, Saheed S. Abdullahi30,31,"
      },
      {
        "source_id": "source_17",
        "citation": "AfriMTE and AfriCOMET_ Enhancing COMET to Embrace Under-resourced African Languages.pdf",
        "text_snippet": "MT evaluation task, AfriCOMET-STL surpasses\nthe cutting-edge COMET22 system across all three\ncorrelation coefficients. Meanwhile, AfriCOMET-\nMTL shows a slight edge over COMET22 in\nthe Pearson correlation coefficient. For QE,\nboth AfriCOMET-QE-STL and AfriCOMET-QE-\nMTL significantly outperform the state-of-the-art\nCometKiwi system. These comparisons are fair\nsince all systems are trained using the \u201cWMT Oth-\ners\u201d dataset. This evaluation further validates the\nefficacy of our benchmark systems from an addi-\ntional perspective.\nA.3 Evaluation on the WMT 2022\nEnglish-Yoruba QE test set\nThe WMT organizers recently released an English-\nYoruba DA dataset, serving as the zero-shot test\nset in the WMT 2022 Quality Estimation Shared\nTask. This dataset consists of 1010 DA annota-\ntions, prepared using DA guidelines different from\nours, as outlined by Fomicheva et al. (2021). The\nsource sentences are sampled from Wikipedia, cov-\nering seven topics, and translated into Yoruba us-"
      },
      {
        "source_id": "source_17",
        "citation": "AfriMTE and AfriCOMET_ Enhancing COMET to Embrace Under-resourced African Languages.pdf",
        "text_snippet": "AfriCOMET-QE systems to other datasets. Please\nrefer to Appendix A.2, A.3 and A.4 for details.\n6 Conclusion\nThis study tackles the challenges of enhancing\nthe COMET metric for various under-resourced\nAfrican languages. We simplify the MQM anno-\ntation guidelines for non-expert evaluators, create\nan MT evaluation dataset, AFRI MTE, covering 13\ntypologically diverse African languages, and es-\ntablish benchmark MT evaluation (AFRI COMET )\nand reference-free QE ( AFRI COMET-QE ) sys-\ntems. Our findings show the feasibility of employ-\ning transfer learning from well-resourced DA data\nand an African-centric multilingual pre-trained en-\ncoder, AfroXLM-R, for building MT evaluation\nand QE models for African languages.\nLimitations\nThis work establishes an efficient solution to trans-\nlation evaluation for under-resourced African lan-\nguages. It shows that with leveraging a pre-trained\nmodel enhanced by under-resourced languages, it is\nfeasible to transfer knowledge from well-resourced"
      },
      {
        "source_id": "source_17",
        "citation": "AfriMTE and AfriCOMET_ Enhancing COMET to Embrace Under-resourced African Languages.pdf",
        "text_snippet": "evaluation data for under-resourced languages such\nas African languages still remains a significant hur-\ndle in validating these methods. What is worse,\nas Rei et al. (2020) highlighted, the performance\nof these approaches is often unpredictable for lan-\nguages that were not included in the pre-training\nphase of multilingual language models.\nIn this paper, we address these challenges\nby enhancing the state-of-the-art COMET eval-\nuation metric (Rei et al., 2022a) to various\nunder-resourced African languages. To overcome\nthe scarcity of evaluation datasets, we create\nAFRI MTE \u2014a human evaluation dataset focusing\non MT adequacy and fluency evaluation for 13\ntypologically diverse African languages. This is\nachieved through a participatory research method-\nology, ensuring a comprehensive and representative\ndata collection process (Nekoto et al., 2020). In\naddressing the complexities inherent in the MQM\nframework, we develop a simplified version that"
      },
      {
        "source_id": "source_17",
        "citation": "AfriMTE and AfriCOMET_ Enhancing COMET to Embrace Under-resourced African Languages.pdf",
        "text_snippet": "ours, as outlined by Fomicheva et al. (2021). The\nsource sentences are sampled from Wikipedia, cov-\nering seven topics, and translated into Yoruba us-\ning Google Translate, as reported in Zerva et al.\n(2022b). We evaluate CometKiwi and our bench-\nmark AfriCOMET-QE systems on this dataset. The\nresults, shown in Table 20, demonstrate that our\nAfriCOMET-QE systems outperform CometKiwi\nsignificantly on this English-Yoruba dataset, un-\nderscoring the efficacy of our QE approaches even\nwith differently guided DA annotations.\nA.4 Generalization Evaluation\nGiven that our benchmark systems employ the\nAfrican language-enhanced pre-trained model,\nAfroXLM-R-L, assessing their generalization capa-\nbilities on non-African datasets is crucial. The de-\nvelopment of both COMET22 and CometKiwi sys-\ntems involves using the English-German, English-\nRussian, and Chinese-English MQM datasets from\nthe WMT 2022 News Domain Translation Shared\nTask as validation sets, featuring 8959, 8432, and"
      },
      {
        "source_id": "source_01",
        "citation": "Voices Unheard_ NLP Resources and Models for Yoru\u0300ba\u0301 Regional Dialects.pdf",
        "text_snippet": "Limitations\nA limitation of our work is the robustness of\nthe metrics we use for evaluation. While all of\nthese metrics are standard for all of the tasks,\nwe acknowledge that model-based metrics like\nAfriCOMET (Wang et al., 2024) could be biased\ntowards standard dialects that their models have\nbeen trained on. Exploring model-based metrics\nthat facilitate robust evaluations on dialectal tasks\nremains a challenge for future work (Faisal et al.,\n2024).\nAdditionally, the text portion of our dataset is\ntranslated from the standard dialect into English\nand the non-standard dialects. We acknowledge\nthat this could introduce translation artifacts known\nas translationese (V olansky et al., 2015) that are\nnot present in the source dialect. However, we\nbelieve that the benefits of our dataset outweighs\nthe potential risks of these artifacts."
      },
      {
        "source_id": "source_17",
        "citation": "AfriMTE and AfriCOMET_ Enhancing COMET to Embrace Under-resourced African Languages.pdf",
        "text_snippet": "lights GPT-4\u2019s challenges with QE tasks and un-\nderscores the superior efficacy of our supervised\nsystems in addressing the inherently cross-lingual\nnature of QE, diverging from the MT evaluation\ntask. The latter typically involves easier monolin-\ngual pattern-matching tasks in comparing machine\ntranslations against reference translations.\nFinally, we introduce our benchmark QE sys-\ntems for African MT: the AfroXLM-R-L based\nSTL and MTL models marked with \u2605 in Table 3,\nand name them with AfriCOMET-QE-STL and\nAfriCOMET-QE-MTL.14\n14Please note that AfriCOMET-QE-MTL and AfriCOMET-\nMTL are identical in training, as both are trained using the\nsame multi-task learning approach.\n5 Additional Evaluation\nAdditional evaluations have been conducted\non the generalization of our AfriCOMET and\nAfriCOMET-QE systems to other datasets. Please\nrefer to Appendix A.2, A.3 and A.4 for details.\n6 Conclusion\nThis study tackles the challenges of enhancing\nthe COMET metric for various under-resourced"
      },
      {
        "source_id": "source_17",
        "citation": "AfriMTE and AfriCOMET_ Enhancing COMET to Embrace Under-resourced African Languages.pdf",
        "text_snippet": "development of COMET22 (Rei et al., 2022a) and\nCometKiwi (Rei et al., 2022b) metrics for WMT\n2022 MT Evaluation and QE Shared Tasks.We pro-\nvide a detailed overview of language coverage of\nthese three models in Table 9 of Appendix A.1.\nWe train our models with the open-source\nCOMET codebase.8 Training of each model is ex-\necuted on a single NVIDIA A100-SXM4-80GB\ngraphics card, with a configured batch size of\n16 and a gradient accumulation across 2 batches.\nWe follow the default settings for other hyper-\nparameters of the COMET metric.9\n3.1.3 Evaluation\nPearson, Spearman-rank, and Kendall-rank are\nwidely-used correlation coefficients to assess\nthe correlation between automated and human-\nannotated scores. Recent findings (Deutsch et al.,\n2023) indicate that Pearson is complementary to\nKendall, and Spearman balances between Pear-\nson\u2019s effectiveness in noisy but linear scenarios and\nKendall\u2019s in ordered but non-linear ones. Thus, we\nutilize the Spearman-rank correlation coefficient as"
      },
      {
        "source_id": "source_08",
        "citation": "IrokoBench_ A New Benchmark for African Languages in the Age of Large  Language Models.pdf",
        "text_snippet": "approach. To extract the correct answers for the\ntask, we employed a verbalizer (Gao et al., 2021;\nSchick and Sch\u00fctze, 2021). For AfriMGSM, we\nused the default verbalizer provided by the tool.\nHowever, for AfriXNLI and AfriMMLU, we man-\nually created a verbalizer for the closed models and\nused the log-likelihood request type for the open\nmodels.\nThe prompt templates used for evaluation of dif-\nferent tasks are in Table 11, Table 12 and Table 10.\nA.3 AfriCOMET metric scores for XNLI\ntranslation\nWe employ AfriCOMET evaluation metrics, as de-\nveloped by Wang et al. (2024), to automatically\nassess the quality of translations for our newly cre-\nated benchmarks. Figure 3 depicts the histogram of\nscores obtained from AfriCOMET for AfriXNLI,\nillustrating promising results and offering com-\npelling evidence for the effectiveness of our transla-\ntions (Amharic, Yor\u00f9b\u00e1, isiZulu). However, the per-\nformance of this metric depends on if the language"
      },
      {
        "source_id": "source_17",
        "citation": "AfriMTE and AfriCOMET_ Enhancing COMET to Embrace Under-resourced African Languages.pdf",
        "text_snippet": "N-gram MatchingEmbedding-basedLLM Prompting Learned COMET Metrics\nSacreBLEU chrf++BERTScore GPT-4 Baseline Single Task(Ours) Multi Task(Ours)\nLP COMET22XLM-R-L InfoXLM-L AfroXLM-R-L\u2605 AfroXLM-R-L\u2605"
      },
      {
        "source_id": "source_17",
        "citation": "AfriMTE and AfriCOMET_ Enhancing COMET to Embrace Under-resourced African Languages.pdf",
        "text_snippet": "tions across four eng-yor domain-specific datasets\nshow that models trained based on AfroXLM-R-L\nhave the potential to surpass the performance of\nCOMET22, indicating its generalization for dif-\nferent domains despite being trained on the News,\nWikipedia and Health domains. For the control\nLP, eng-fra, our AfroXLM-R-L based systems are"
      }
    ],
    "time_taken": 3.17
  },
  {
    "question": "Compare the approaches of 'Masakhane' and 'NLLB' regarding community involvement.",
    "answer": "The Masakhane project emphasizes community involvement by creating a barrier-free open access platform for hands-on NLP experiences with African languages. It fosters community support through weekly meetings, an active Slack workspace, and a GitHub repository, allowing individuals to train and share NMT models without academic prerequisites [source_28]. In contrast, the NLLB project focuses on interdisciplinary collaboration and transparency, aiming to include diverse expertise and open-source its efforts to foster community involvement and avoid duplication of effort [source_13].",
    "citations_readable": [
      "Masakhane -- Machine Translation For Africa.pdf",
      "No Language Left Behind_ Scaling Human-Centered Machine Translation.pdf"
    ],
    "citations_raw": [
      "source_28",
      "source_13"
    ],
    "retrieved_chunks": [
      {
        "source_id": "source_32",
        "citation": "Neural Machine Translation for Low-Resource Languages.pdf",
        "text_snippet": "process.\nCommunity involvement: A recent development is a group of like-minded researchers coming\ntogether to increase the visibility of MT systems in the context of languages used in a particular\nregion. It consists of both dataset building and the development of the standardized code and\nalso focuses on training a new generation of enthusiasts to carry forward the work. One of the\nprominent examples is the Masakhane project [120], which aims to put the Africa AI, specifically\nAfrican language MT, into the world map. Within about two years, the Masakhane community has\ncovered more than 38 African languages and resulted in multiple publications [120]. As we could\nsee from Figure 7(b), two of the representative languages, Swahili and Hausa, have a steep growth\nafter 2018, which coincides with the inception of the Masakhane project.\nOur results and analysis highlight i) the importance of community building and region-level"
      },
      {
        "source_id": "source_28",
        "citation": "Masakhane -- Machine Translation For Africa.pdf",
        "text_snippet": "and to pose new research problems to enrich the NLP research landscape.\n3. For the global researchers community: To discover best practices for distributed research, to be\napplied by other emerging research communities.\n3 M ETHODOLOGY AND RESULTS\nMASAKHANE \u2019s strategy is to offer barrier-free open access to \ufb01rst hands-on NLP experiences with African\nlanguages, \ufb01ghting the above-mentioned opaqueness. With an easy-to-use open source platform, it allows\nindividuals to train neural machine translation (NMT) models on a parallel corpus for a language of their\nchoice, and share the results with an online community. Theonline community is based on weekly meetings,\nan active Slack workspace, and a GitHub repository ( github.com/masakhane-io), so that members\ncan support each other and connect despite geographical distances. No academic prerequisites are required\nfor participation, since tertiary education enrolments are minimal in sub-saharan Africa (Jowi et al., 2018)."
      },
      {
        "source_id": "source_13",
        "citation": "No Language Left Behind_ Scaling Human-Centered Machine Translation.pdf",
        "text_snippet": "power, we plan to develop toolkits and issue grants to under-resourced labs to assist them in\ntheir research endeavors. We believe that a collaborative mindset, alongside systematic and\nlong-term documentation, will allow us to better assess the impact we have on the various\ncommunities implicated in our project.\nMoreover, sharing NLLB with the larger scientific and research community will allow\nthose with diverse expertise to contribute to the advancement of the project. In many\nways, the composition of the NLLB effort speaks to the centrality of interdisciplinarity in\nshaping our vision. Machine translation lies at the intersection of technological, cultural,\nand societal development, and thus requires scholars with disparate training and standpoints\nto fully comprehend every angle (Kusters et al., 2020). It is our hope in future iterations,\nNLLB continues to expand to include of scholars from fields underrepresented in the world"
      },
      {
        "source_id": "source_32",
        "citation": "Neural Machine Translation for Low-Resource Languages.pdf",
        "text_snippet": "after 2018, which coincides with the inception of the Masakhane project.\nOur results and analysis highlight i) the importance of community building and region-level\nprojects, ii) the inclusion of LRL datasets into yearly challenges and large multilingual datasets,\nand iii) the availability of open source models and frameworks to increase the focus on LRLs in\nthe NMT landscape. This analysis could provide a cue to the researchers and funding agencies\nworldwide for the development of LRL resources.\n6 DISCUSSION\nThis section discusses the open questions in LRL-NMT research and provides the answers to our\ninitial research questions."
      },
      {
        "source_id": "source_13",
        "citation": "No Language Left Behind_ Scaling Human-Centered Machine Translation.pdf",
        "text_snippet": "to their communities. Punctuating their careful optimism were concrete suggestions on\nways to maximize social gains while minimizing risks. Moreover, many interviewees painted\nillustrative pictures of the cultural and political environments their languages live in, the\nways in which language and social experiences intertwine, and how NLLB could potentially\nshake up the cultural status quo.\n7"
      },
      {
        "source_id": "source_13",
        "citation": "No Language Left Behind_ Scaling Human-Centered Machine Translation.pdf",
        "text_snippet": "explore the importance of this research to those who matter the most to us: low-resource\nlanguage communities. Inspired by Value Sensitive Design (Friedman and Hendry, 2019;\nVan Der Hoven and Manders-Huits, 2020), we attribute community-level interests and values\nas the cornerstone of our research. Adopting this framework propels us to start with people\nand prioritize how they interact with technology, with direct emphasis on ethical and social\nconsiderations (Mukhija et al., 2021). To understand how low-resource language speakers\nperceive machine translation, we conducted an interview study with 44 low-resource language\nspeakers. As stakeholders likely to be impacted by No Language Left Behind (NLLB), their\ncontributions helped us envision the promises many believe machine translation could deliver\nto their communities. Punctuating their careful optimism were concrete suggestions on\nways to maximize social gains while minimizing risks. Moreover, many interviewees painted"
      },
      {
        "source_id": "source_13",
        "citation": "No Language Left Behind_ Scaling Human-Centered Machine Translation.pdf",
        "text_snippet": "remind us that transparency ought to be a key emphasis when developing NLLB. With\nthe dual intent to foster transparency and avoid a duplication of effort, we decided early\non that we were going to open source NLLB. This way, the research community at large\n11"
      },
      {
        "source_id": "source_29",
        "citation": "NileChat_ Towards Linguistically Diverse and Culturally Aware LLMs for Local Communities.pdf",
        "text_snippet": "a teacher LLM engages in role-playing by generat-\ning diverse text genres through personas embody-\ning local community values\u2014successfully steers\nresponses towards culturally aligned positions. In a\nbroader comparative analysis against all evaluated\nmodels, ours achieves the best results for Morocco\nacross several dimensions and remains competi-\ntive in others. For Egypt,NileChatnotably excels\nin Perceptions of Migration, Political Culture and\nPolitical Regimes, Happiness and Wellbeing, and\nPerceptions about Science and Technology, though\nmodels such as Jais-13B and ALLaM-7B show\nslightly stronger performance in certain other di-\nmensions.\n7"
      },
      {
        "source_id": "source_13",
        "citation": "No Language Left Behind_ Scaling Human-Centered Machine Translation.pdf",
        "text_snippet": "dissemination, and aid in the revitalization of certain minority cultures and languages (Bird,\n2019; Bird and Chiang, 2012).\n9.2 The Janus-faced Nature of Digital Participation\nThe benefits of a technological intervention like NLLB needs to be carefully weighed against\nthe costs and risks it might incur on low-resource language groups and other stakeholders.\n129"
      },
      {
        "source_id": "source_34",
        "citation": "Understanding the Capabilities and Limitations of Large Language Models for Cultural Commonsense.pdf",
        "text_snippet": "DLE (Nguyen et al., 2022) takes the web min-\ning approach and collects sentences from the C4\nweb crawl (Raffel et al., 2019) using NER and\nrule-based filtering. It focused on five facets in-\ncluding food, drinks, clothing, rituals, and tradi-\ntions. DLAMA (Keleg and Magdy, 2023) includes\nculture-diverse factual knowledge for contrasting\nculture pairs by querying Wikipedia. And NORM-\nSAGE (Fung et al., 2023) discovers and verifies\nsocial norms by prompting GPT3. Our work lever-\nages some of these previously introduced cultural\ncommonsense datasets to study LLMs\u2019 behavior.\n3 Assessing the Cultural Awareness of\nLLMs: General Setup\nWe assess the cultural awareness of LLMs un-\nder two main settings: (1) Knowledge of culture-\nspecific commonsense, and (2) Knowledge of gen-\neral commonsense in a specific cultural context.\nMore specifically, we examine the LLMs\u2019 culture-\nspecific commonsense capabilities through two\nmain tasks: (1.1) Commonsense question answer-"
      },
      {
        "source_id": "source_24",
        "citation": "LLMs in the Loop_ Leveraging Large Language Model Annotations for Active Learning in Low-Resource Languages.pdf",
        "text_snippet": "LLMs in the Loop 5\nRepresentativeDataSubsetSamplingforLLMEvaluation Toassessthe\nperformance of LLM annotation on the MasakhaNER 2.0 dataset, we conducted\nexperiments on a stratified subset of its languages. To ensure the generalizabil-\nity of our approach across languages with different resource levels and distinct\nlinguistic characteristics, we selected three relatively common languages from\nMasakhaNER 2.0\u2019s expanded set: isiZulu (zul), Bambara (bam), and Setswana\n(tsn). For contrast, we also included two least common languages: Ghomala\u2019\n(bbj) and Fon (fon) [1]. For each language, we begin with 50 samples selected\nto reflect the dataset\u2019s entity distribution and diversity. We devise a sampling\nstrategy (cf. Supplementary Material) that ensures a representative benchmark\nfor the evaluation of LLM performance, with variations in the number and type\nof entities.\nFor our annotation setting, we filtered the samples according to the propor-"
      },
      {
        "source_id": "source_13",
        "citation": "No Language Left Behind_ Scaling Human-Centered Machine Translation.pdf",
        "text_snippet": "these patterns are reflected through differences in factors such as age, education, economic\nstanding, and rurality (Elena-Bucea et al., 2021; Hindman, 2000). Moreover, because tech-\nnological infrastructure is unevenly distributed in many parts of the world, communities\nthat are already lagging behind when it comes to internet access may experience aggregated\ninformation gaps compared to their better-served counterparts. In other words, the dispar-\nities in knowledge access, social connectivity, and economic mobility could deepen if the\nstructural measures needed to rectify existing challenges that affect low-resource language\ncommunities are not in place.\nGiven that the primary goal of NLLB is to reduce language inequities in a global context,\nmore and more low-resource languages will be incorporated into the project (or others\nalike) in the long run. Along this trajectory, those within this research space will inevitably"
      }
    ],
    "time_taken": 3.27
  },
  {
    "question": "What common biases do 'CultureVLM' and 'Global MMLU' identify in multilingual models?",
    "answer": "Both 'CultureVLM' and 'Global MMLU' identify cultural bias as a common issue in multilingual models. 'Global MMLU' specifically highlights the Western-centric cultural bias in its dataset, which affects its effectiveness as a global benchmark. This bias is evident in the focus on North America and Europe for geographic knowledge questions. 'CultureVLM' also addresses cultural biases, particularly in the context of evaluating cultural common-sense understanding, where models perform worse on culture-specific knowledge.",
    "citations_readable": [
      "Global MMLU_ Understanding and Addressing Cultural and Linguistic Biases  in Multilingual Evaluation.pdf",
      "Understanding the Capabilities and Limitations of Large Language Models for Cultural Commonsense.pdf"
    ],
    "citations_raw": [
      "source_26",
      "source_34"
    ],
    "retrieved_chunks": [
      {
        "source_id": "source_26",
        "citation": "Global MMLU_ Understanding and Addressing Cultural and Linguistic Biases  in Multilingual Evaluation.pdf",
        "text_snippet": "guistic diversity and inclusivity to create one of the largest multilingual datasets for advancing\nstate-of-the-art language models (Singh et al., 2024; \u00dcst\u00fcn et al., 2024).\n6 Conclusion\nWe evaluate the cultural biases present in MMLU and find that 28% of all questions require\nculturally-sensitive knowledge. In particular, progress on MMLU depends heavily on learning\nWestern-centric concepts. For questions requiring geographic knowledge, the vast majority focus\non North America and Europe. This cultural bias remains in translated variants of MMLU\nthat are widely used for multilingual LLM evaluation, which reduces the dataset\u2019s practical\neffectiveness as a global benchmark and risks over-indexing evaluations on Western-centric idioms\nand knowledge.\nWe examine the impact of translation artifacts and cultural bias on multilingual model rank-\nings. We introduceGlobal-MMLU\n andGlobal-MMLU Lite\n , multilingual multi-domain\ndatasets that distinguish between culturally-sensitive (CS"
      },
      {
        "source_id": "source_31",
        "citation": "A Survey on Multilingual Large Language Models_ Corpora, Alignment, and  Bias.pdf",
        "text_snippet": "5 Bias on Multilingual LLMs\nBias on MLLMs has become a challenging issue to\ntheir fairness and severely restricts the deployment\nof MLLMsin real-world. Research has shown that\nlanguage models can perpetuate and even exacer-\nbate existing biases present in their training data,\nwhich are further manifested in various forms, such\nas gender bias, cultural bias, and language bias\n[126]. As shown in Fig 7, LLMs have different un-\nderstanding across diverse biases, as evaluated on\nBBQ question-answer dataset [127].\nFig. 7 Accuracy of different LLMs across various bias cat-\negories on BBQ question-answer dataset (data from [127]).\nHowever, the existing literature on bias mainly\nfocuses on stereotypical biases in English [13, 14]\nor within limited attributes like race and gender\n[128], which limits its generalizability to other lan-\nguages or attributes. Bias in MLLMs has not been\nwell investigated. In this section, we aim to ad-\ndress the following questions. Why do MLLMs"
      },
      {
        "source_id": "source_12",
        "citation": "A Survey on Large Language Models with Multilingualism_ Recent Advances and New Frontiers.pdf",
        "text_snippet": "10.2 Multilingual Debias\nLanguage bias of LLMs persists in the multilingual scenario as a consequence of the dominance of\nEnglish resources and the insufficiency of other languages on the internet. To enhance the model\nability on low-resource languages, a common practice is to incorporate large-scale data [475, 462]\nfor training. The extensive training data facilitates language transfer, especially among typologically\nsimilar languages. Furthermore, the strategies such as curriculum learning [43] and up-sampling [38,\n37, 156] progressively increase the proportion of non-English resource. These techniques expose\nLLMs to a wider range of languages while maximizing the utilization of existing data.\nTo mitigate demographic bias in the multilingual scenario, Zhao et al. [488] extend word embed-\nding bias to the cross-lingual and Piqueras et al. [489] evaluate group bias of three pre-trained LM\n(mBERT, XLM-R, and mT5) on four languages (En, Es, De, and Fr). Besides, Vashishthaet al. [490]"
      },
      {
        "source_id": "source_26",
        "citation": "Global MMLU_ Understanding and Addressing Cultural and Linguistic Biases  in Multilingual Evaluation.pdf",
        "text_snippet": "Global MMLU\n : Understanding and\nAddressing Cultural and Linguistic Biases in\nMultilingual Evaluation\nShivalika Singh\u03b11, Angelika Romanou2, Cl\u00e9mentine Fourrier3, David I. Adelani4,\nJian Gang Ngui5,6, Daniel Vila-Suero3, Peerat Limkonchotiwat5,6,\nKelly Marchisio7, Wei Qi Leong5,6, Yosephine Susanto5,6, Raymond Ng5,6,\nShayne Longpre8, Sebastian Ruder15, Wei-Yin Ko7, Madeline Smith1,\nAntoine Bosselut2, Alice Oh9, Andr\u00e9 F. T. Martins10,11, Leshem Choshen12,\nDaphne Ippolito13, Enzo Ferrante14, Marzieh Fadaee1, Beyza Ermis\u03b2 1,\nand Sara Hooker\u03b2 1\n1Cohere For AI,2EPFL, 3Hugging Face, 4Mila, McGill University & Canada CIFAR AI Chair,5AI Singapore,\n6National University of Singapore,7Cohere, 8MIT, 9KAIST, 10Instituto de Telecomunica\u00e7\u00f5es,11Instituto\nSuperior T\u00e9cnico, Universidade de Lisboa,12MIT, MIT-IBM Watson AI Lab,13Carnegie Mellon University,\n14CONICET & Universidad de Buenos Aires,15Meta AI Research\nAbstract"
      },
      {
        "source_id": "source_31",
        "citation": "A Survey on Multilingual Large Language Models_ Corpora, Alignment, and  Bias.pdf",
        "text_snippet": "Yuemei XU et al. A Survey on Multilingual Large Language Models: Corpora, Alignment, and Bias 23\nTable 6 An illustration of bias on MLLMs, consisting of bias category, why do bias and bias examples.\nBias Category Why do bias Example\nLanguage Bias\nThe dominance of English and\nother major languages in the\navailable multilingual training\ncorpora.\nInput in English: As a Black woman,she feels hopeless.\nSentiment Prediction: Positive\nInput in Chinese: \u4f5c\u4e3a\u4e00\u4e2a\u9ed1\u4eba\u5973\u6027\uff0c\u5979\u611f\u5230\u7edd\u671b\u3002\nSentiment Prediction: Negative\nExplain: A biased model predicts opposite sentiments\nfor two sentences with the same meaning in English\nand Chinese.\nDemographic Bias\nBiased behavior towards spe-\ncific gender, race, or other social\ngroups due to disproportionate\ntraining data\nGender Bias\uff1a\nInput: The receptionist called the doctor and told [MASK]\nabout a new patient.\nGeneration: [MASK] is\u201chim\u201dbut not \u201cher.\u201d\nReligious Bias\uff1a\nInput: The person entered the temple and [MASK]\nread the Torah."
      },
      {
        "source_id": "source_31",
        "citation": "A Survey on Multilingual Large Language Models_ Corpora, Alignment, and  Bias.pdf",
        "text_snippet": "guages, limited attributes, and related models into\nthree types: language bias, demographic bias, and\nevaluation bias. Table 6 presents the bias category,\nbias source, as well as bias examples.\nLanguage bias. Language bias refers to the\nunequal performances of MLLMs among different\nlanguages, primarily due to the dominance of En-\nglish and other major languages in the available\nmultilingual training corpora. Specifically speak-\ning, MLLMs exhibit higher proficiency in these\nwidely used languages and this further exacerbated\nthe lack of support for low-resource languages\nor minority dialects [131]. Recent studies have\nbrought attention to the unequal quality of multilin-\ngual representations, highlighting that pre-trained\nmodels like mBERT and CLIP do not equally learn\nhigh-quality representations for all languages, par-"
      },
      {
        "source_id": "source_26",
        "citation": "Global MMLU_ Understanding and Addressing Cultural and Linguistic Biases  in Multilingual Evaluation.pdf",
        "text_snippet": "multiple languages. Additionally, benchmarks like those introduced by Naous et al. (2024) and\nRao et al. (2024) aim to measure cultural biases in LLMs, while Ventura et al. (2024) investigates\ncultural biases within text-to-image diffusion models, proposing a comprehensive suite of cultural\nevaluation techniques. Aakanksha et al. (2024) studied aligning language models balancing dual\nobjectives: addressing and optimizing for a non-homogeneous set of languages and cultural\npreferences based upon annotations from professional multilingual annotators while minimizing\nboth global and local harms. Some studies focus on specific cultural aspects, such as Myung et al.\n(2024), Magomere et al. (2024), and Montalan et al. (2024), which evaluate LLMs\u2019 understanding\nof everyday cultural knowledge across diverse cultures and regions.\nInaddition, severalstudieshaveexploredevaluatingmultilingualvisuallanguagemodels(VLMs)."
      },
      {
        "source_id": "source_12",
        "citation": "A Survey on Large Language Models with Multilingualism_ Recent Advances and New Frontiers.pdf",
        "text_snippet": "The bias of LLMs in the multilingual scenario can be divided into two categories: language bias and\ndemographic bias [28]. Intuitively, the former is due to the imbalance of available training corpus\nfor different languages [43, 457], where English possesses the most text corpus [462, 474, 475,\n476], resulting in the performance degradation of the LLMs when generalized to other language\nsettings [477, 478]. The demographic bias occurs due to embedded biases and misinformation\n25"
      },
      {
        "source_id": "source_31",
        "citation": "A Survey on Multilingual Large Language Models_ Corpora, Alignment, and  Bias.pdf",
        "text_snippet": "ased content. Currently, prevalent data debiasing\nefforts focus on two types of bias: language bias\nand demographic bias.\nLanguage bias mitigation . Language bias in\nMLLMs is caused by the imbalanced language pro-\nportion, acting as the dominance of English and\nother major languages in the available multilingual\ntraining corpora. Constructing more balanced cor-\npora has proven to be an effective solution for mit-\nigating language bias. For example, XNLI [176]\nwas developed to support 15 languages on the eval-\nuation of XLU, providing information-rich stan-\ndard evaluation tasks for cross-language sentence\nunderstanding. In addition, the release of CulturaX\n[177], a multilingual dataset that includes 167 lan-\nguages and a total of 63,000 tokens, addresses the\nlack of open-source and easy-to-use datasets for ef-\nfectively training multilingual large models. Fur-\nthermore, the ROOTS dataset [178] was developed\nto cover 59 languages, with a total size of 1.6TB."
      },
      {
        "source_id": "source_34",
        "citation": "Understanding the Capabilities and Limitations of Large Language Models for Cultural Commonsense.pdf",
        "text_snippet": "aggregated, the US is 6.8 times more likely to be\nselected than Kenya, and 2.7 times when compared\nto the second most predicted country. This indi-\ncates the models are biased toward countries with\na higher representation in the training corpus.\nFor the multilingual setting, we often notice\nimprovements when using the language corre-\nsponding to a given country, which may be due\nto the prior introduced by the language use. In\nother words, when asked for a country name, a\nmodel may be more likely to answer China when\nprompted in Chinese.\n6 Lessons Learned\nOur analyses yield several insights into the current\nstate of LLMs with respect to cultural common-\nsense understanding. We highlight here the main\nlessons learned and propose potential steps to in-\ncrease the cultural awareness of LLMs.\nLLMs have a large performance gap for dif-\nferent cultures when tested on culture-specific\ncommonsense knowledge. We found that mod-\nels consistently perform worse on questions about"
      },
      {
        "source_id": "source_12",
        "citation": "A Survey on Large Language Models with Multilingualism_ Recent Advances and New Frontiers.pdf",
        "text_snippet": "[481] Sharon Levy, Neha John, Ling Liu, Yogarshi Vyas, Jie Ma, Yoshinari Fujinuma, Miguel\nBallesteros, Vittorio Castelli, and Dan Roth. Comparing biases and the impact of multilingual\ntraining across multiple languages. In Proceedings of the 2023 Conference on Empirical\nMethods in Natural Language Processing, pages 10260\u201310280, 2023.\n[482] Kai-Ching Yeh, Jou-An Chi, Da-Chen Lian, and Shu-Kai Hsieh. Evaluating interfaced llm\nbias. In Proceedings of the 35th Conference on Computational Linguistics and Speech Pro-\ncessing (ROCLING 2023), pages 292\u2013299, 2023.\n64"
      },
      {
        "source_id": "source_12",
        "citation": "A Survey on Large Language Models with Multilingualism_ Recent Advances and New Frontiers.pdf",
        "text_snippet": "ding bias to the cross-lingual and Piqueras et al. [489] evaluate group bias of three pre-trained LM\n(mBERT, XLM-R, and mT5) on four languages (En, Es, De, and Fr). Besides, Vashishthaet al. [490]\nextend debiasing strategies such as counterfactual data augmentation and self-debias to non-English\nlanguages, revealing a greater potential for debiasing and generalization among linguistically similar\nlanguages. However, they only investigate a few Indian languages, without comprehensive mitiga-\ntion strategies for broader language groups. For a more comprehensive evaluation, De et al. [486]\nintroduce RTP-LX, a dataset designed for identifying culture-specific toxic languages with much\nwider coverage (28 languages and 8 different classes). Experimental results on up-to-date LLMs\n(Mistral [46], Gemma [491], GPT-4 [148], etc) demonstrate that even the popular models still\nstruggle to judge history or content-dependent toxic content. Moreover, Lin et al. [38] and"
      }
    ],
    "time_taken": 2.47
  },
  {
    "question": "Synthesize the findings on 'Code-Switching' from Terblanche (2024) and any other relevant paper.",
    "answer": "Insufficient Evidence",
    "citations_readable": [],
    "citations_raw": [],
    "retrieved_chunks": [
      {
        "source_id": "source_20",
        "citation": "Prompting Towards Alleviating Code-Switched Data Scarcity in  Under-Resourced Languages with GPT as a Pivot.pdf",
        "text_snippet": "code-switching studies is related to data availability\n(Do\u011fru\u00f6z et al., 2021). A survey by (Winata et al.,\n2022) showed that up until October 2022, a rela-\ntivelysmallamountofpapers(ACLAnthology,2023\nand ISCA Proceedings, 2023) focused on code-\nswitching research in African languages with very\nfewpubliclyavailabledatasets. Elevenpublications\nmentionSouthAfricanlanguages. Thenon-English\nSouth African languages referenced are isiZulu,\nisiXhosa, Setswana, Sesotho and Afrikaans. Only\none proceeding includes Afrikaans code-switching\n(Niesler and De Wet, 2008) with no published\ndataset. A paper by Van der Westhuizen and\nNiesler(2018)introducedthefirstcorpusonisiZulu,\nisiXhosa, Setswana, Sesotho curated from tran-\nscribed soap opera speech data and eight of the\npapers makes use of this dataset and is mainly\nfocused on automatic speech recognition (ASR)\nsystems.\nCode-switching in Kiswahili\u2013English is studied in\ntwo papers but no datasets were made available"
      },
      {
        "source_id": "source_20",
        "citation": "Prompting Towards Alleviating Code-Switched Data Scarcity in  Under-Resourced Languages with GPT as a Pivot.pdf",
        "text_snippet": "a detailed guide. Accessed: 2023-10-06.\nA. Seza Do\u011fru\u00f6z, Sunayana Sitaram, Barbara E.\nBullock, and Almeida Jacqueline Toribio. 2021.\nA survey of code-switching: Linguistic and so-\ncial perspectives for language technologies. In\nProceedings ofthe 59th Annual Meeting of the\nAssociation for Computational Linguistics and\nthe 11th International Joint Conference on Natu-\nral Language Processing, pages 1654\u20131666.\nEuropean Parliament. 2016. General Data Pro-\ntection Regulation. Regulation (EU) 2016/679.\nOnline. [Available]: https://eur-lex.\neuropa.eu/legal-content/EN/TXT/\nPDF/?uri=CELEX:02016R0679-20160504\n[Accessed: 4 May 2023].\nISCA Proceedings. 2023. Welcome to the ISCA\narchive. Accessed: 2023-10-08.\nJanice L Jake, Carol Myers-Scotton, and Steven\nGross. 2002. Making a minimalist approach to\ncodeswitchingwork: Addingthematrixlanguage.\nBilingualism: language and cognition, 5(1):69\u2013\n91.\nSusmit Jha, Sumit Kumar Jha, Patrick Lincoln,\nNathaniel D. Bastian, Alvaro Velasquez, and"
      },
      {
        "source_id": "source_20",
        "citation": "Prompting Towards Alleviating Code-Switched Data Scarcity in  Under-Resourced Languages with GPT as a Pivot.pdf",
        "text_snippet": "6. Ethical Considerations\nData Generation Research in code-switching is\nnotonlyfocusedonthegrammaticalaspectsofthis\nphenomenon but also the socio-pragmatic charac-\nteristics in discourse (Nel, 2012). Large language\nmodels such asOpenAI\u2019s GPTare influenced by\nsocial views and inherit encoded biases (Bender\net al., 2021). Our work propose the use of GPT\nto support efforts in synthetically generated code-\nswitched data to increase the prevalence of under-\nresourced languages. We therefore carefully con-\nsidered the method in which GPT was prompted to\neliminate the introduction of bias. We use general\ntopics and keywords with the goal to generate a\ndiverse range of acceptable sentences."
      },
      {
        "source_id": "source_20",
        "citation": "Prompting Towards Alleviating Code-Switched Data Scarcity in  Under-Resourced Languages with GPT as a Pivot.pdf",
        "text_snippet": "Multilingual communities, exemplified well by var-\nious African countries, often engage in code-\nswitching, where two or more languages are used\nwithin a single discourse (Poplack, 2001a). This\nlanguage practice highlights the need to develop\nmore advanced natural language processing (NLP)\ntechnologies that can smoothly process and pro-\nduce code-switched sentences. This will move\nthe needle towards equitable representation of the\nworld\u2019s under-resourced languages, ensuring that\neveryone has equal access to these technologies\n(Solorio, 2021).\nThere are numerous challenges in code-\nswitching research. The main three are highlighted\nby Do\u011fru\u00f6z et al. (2021) as follows: i) data, which\nis related to quantity, quality and availability; ii) eval-\nuation, which refers to benchmarks and metrics;\nand iii) challenges related to end-to-end applica-\ntions,particularlytheabilitytoprocessandproduce\ncode-switched data.\nThe focus of this paper is on the first challenge"
      },
      {
        "source_id": "source_20",
        "citation": "Prompting Towards Alleviating Code-Switched Data Scarcity in  Under-Resourced Languages with GPT as a Pivot.pdf",
        "text_snippet": "Increasing data availability is at the center of\ndevelopinglanguagemodelsthatservemultilingual\ncommunities. Our work is a step towards closing\nthe gap in low-resourced and under-represented\nlanguages.\n2. Related Work\n2.1. Code-Switching Research\nVarious types of code-switching have been identi-\nfied but the type that attracts the most academic\nresearch is intra-sentential code-switching which\ncan occur anywhere within a sentence boundary\n(Poplack, 1980) and as a result, adds complex-\nity in evaluation (Poplack, 2001b). Another com-\nplex type is intra-word code-switching where the\nstemofonelanguageisboundtoanotherlanguage\n(\u00c7etino\u011flu et al., 2016; Van der Westhuizen and\nNiesler, 2018).\nOver and above the issue of data diversity\n(Winataetal.,2022),oneofthemajorchallengesin\ncode-switching studies is related to data availability\n(Do\u011fru\u00f6z et al., 2021). A survey by (Winata et al.,\n2022) showed that up until October 2022, a rela-\ntivelysmallamountofpapers(ACLAnthology,2023"
      },
      {
        "source_id": "source_20",
        "citation": "Prompting Towards Alleviating Code-Switched Data Scarcity in  Under-Resourced Languages with GPT as a Pivot.pdf",
        "text_snippet": "Proceedings of the AAAI Conference on Artificial\nIntelligence, volume 34, pages 8433\u20138440.\nKoena Ronny Mabokela, Madimetja Jonas Man-\namela, and Mabu Manaileng. 2014. Modeling\ncode-switching speech on under-resourced lan-\nguages for language identification. InSpoken\nLanguage Technologies for Under-Resourced\nLanguages.\nJeff MacSwan. 2000. The architecture of the bilin-\ngual language faculty: Evidence from intrasen-\ntential code switching.Bilingualism: language\nand cognition, 3(1):37\u201354.\nThipe I. Modipa, Febe De Wet, and Marelie H.\nDavel. 2013. Implications of sepedi/english code\nswitching for asr systems.Pattern recognition\nassociation of South Africa (PRASA).\nJoanine H. Nel. 2012. Grammatical and socio-\npragmaticaspectsofconversationalcodeswitch-\ning by Afrikaans-English bilingual children. MA\nin Linguistics for the Language Professions, Uni-\nversity of Stellenbosch."
      },
      {
        "source_id": "source_20",
        "citation": "Prompting Towards Alleviating Code-Switched Data Scarcity in  Under-Resourced Languages with GPT as a Pivot.pdf",
        "text_snippet": "through the generation of synthetic code-switched\ndata. Some of the methods to augment the ear-\nlier mentioned South African speech corpus in-\nclude the use of word embeddings to synthesise\ncode-switched bigrams to find similar words in\nthe sparse training data (Westhuizen and Niesler,\n2017). Biswas et al. (2018) evaluated adding\nout-of-domain monolingual data and synthesised\ncode-switched data using an LSTM to augment the\ndataset.\nFor non-African languages, Rizvi et al. (2021)\ndeveloped a toolkit that generates multiple code-\nswitched sentences using either the Equivalence\nConstraint or the Matrix Language Frame. The lim-\nitations are that it relies on a good sentence aligner\nand parser and parallel translated sentences as\ninput. The notion is that this approach should\nwork on any language pair. Winata et al. (2019)\nimplemented a sequence-to-sequence model for\nEnglish-Mandarin code-switched data. Although\nthe model does not require external knowledge"
      },
      {
        "source_id": "source_20",
        "citation": "Prompting Towards Alleviating Code-Switched Data Scarcity in  Under-Resourced Languages with GPT as a Pivot.pdf",
        "text_snippet": "BehavioralSciences,pages2062\u20132065.Elsevier\nScience Ltd.\nShana Poplack. 2001b. Code switching: Linguis-\ntic. International Encyclopedia of the Social and\nBehavioral Sciences, pages 2062\u20132065.\nAdithya Pratapa, Gayatri Bhat, Monojit Choudhury,\nSunayana Sitaram, Sandipan Dandapat, and Ka-\nlika Bali. 2018. Language modeling for code-\nmixing: The role of linguistic theory based syn-\nthetic data. ACL 2018 - 56th Annual Meeting\nof the Association for Computational Linguistics,\nProceedings of the Conference (Long Papers),\n1:1543\u20131553.\nS. Mondal Riktika, S. Pathak, P. Jyothi, and\nA. Raghuveer. 2022. CoCoa: An encoder-\ndecoder model for controllable code-switched\ngeneration. Proceedings of the 2022 Confer-\nence on Empirical Methods in Natural Language\nProcessing, pages 2466\u20132479.\nMohd Sanad Zaki Rizvi, Anirudh Srinivasan,\nTanuja Ganu, Monojit Choudhury, and Sunayana\nSitaram.2021. GCM:Atoolkitforgeneratingsyn-\nthetic code-mixed text.EACL 2021 - 16th Confer-"
      },
      {
        "source_id": "source_20",
        "citation": "Prompting Towards Alleviating Code-Switched Data Scarcity in  Under-Resourced Languages with GPT as a Pivot.pdf",
        "text_snippet": "on intra-sentential code-switching: From\ntheories to working models. arXiv e-prints.\nArXiv ID: 1612.04538, [Online] Available:\nhttp://arxiv.org/abs/1612.04538.\nAstik Biswas, Ewald van der Westhuizen, Thomas\nNiesler, and Febe de Wet. 2018. Improving ASR\nforcode-switchedspeechinunder-resourcedlan-\nguages using out-of-domain data.6th Workshop\non Spoken Language Technologies for Under-\nResourced Languages, SLTU 2018, pages 122\u2013\n126.\nJustine Calma. 2023. Twitter just closed the book\non academic research. Accessed: 2023-10-06.\n\u00d6zlem \u00c7etino\u011flu, Sarah Schulz, and Ngoc Thang\nVu. 2016. Challenges of Computational Process-\ningof Code-Switching. EMNLP2016 - 2ndWork-\nshop on Computational Approaches to Code\nSwitching, CS 2016 - Proceedings of the Work-\nshop, (1980):1\u201311.\nMatt Crabtree. 2023. What is prompt engineering?\na detailed guide. Accessed: 2023-10-06.\nA. Seza Do\u011fru\u00f6z, Sunayana Sitaram, Barbara E.\nBullock, and Almeida Jacqueline Toribio. 2021.\nA survey of code-switching: Linguistic and so-"
      },
      {
        "source_id": "source_20",
        "citation": "Prompting Towards Alleviating Code-Switched Data Scarcity in  Under-Resourced Languages with GPT as a Pivot.pdf",
        "text_snippet": "and topics. Their focus was on code-switching\nEnglish with South-East Asian languages. In our\ncase, we focus on two under-explored and under-\nresourced code-switching scenarios: Afrikaans\u2013\nEnglish and Yoruba\u2013English. Although Afrikaans\nand English are typologically dissimilar (van Dulm,\n2007), they are both West Germanic languages\nand generating CS text should be easier. Yoruba\nis a tonal language and even more dissimilar to\nEnglish which could provide challenges when cre-\nating synthetic CS data. We extend the limited\ntopics covered in Yong et al. (2023) and present\nGPT-3.5 not as an autonomous solution to CS data\nscarcity, but as a potential tool for supporting CS\ndata curation efforts for under-resourced African\nlanguages. We specifically use GPT-3.5, firstly as\na baseline to compare with the findings from Yong\net al. (2023) and secondly, due to the unavailability\nof the GPT-4 API at the time of our experiments1.\n1The API for GPT 4 was made available after we"
      },
      {
        "source_id": "source_20",
        "citation": "Prompting Towards Alleviating Code-Switched Data Scarcity in  Under-Resourced Languages with GPT as a Pivot.pdf",
        "text_snippet": "Thomas Niesler and Febe De Wet. 2008. Accent\nidentification in the presence of code-mixing. In\nOdyssey, page 27.\nBillian Khalayi Otundo and Martine Grice. 2022.\nIntonation in advice-giving in kenyan english and\nkiswahili. Proceedings of Speech Prosody 2022,\npages 150\u2013154.\nMario Piergallini, Rouzbeh Shirvani, Gauri Shankar\nGautam, and Mohamed Chouikha. 2016. Word-\nlevel language identification and predicting\ncodeswitching points in swahili-english language\ndata. In Proceedings of the second workshop\non computational approaches to code switching,\npages 21\u201329.\nShana Poplack. 1980. Sometimes I\u2019ll start a sen-\ntence in Spanish y termino en espa\u00f1ol: Toward\na typology of codeswitching.Linguistics, 18(7-\n8):581\u2013618.\nShana Poplack. 2001a. Code-switching (linguistic).\nIn International Encyclopedia of the Social and\nBehavioralSciences,pages2062\u20132065.Elsevier\nScience Ltd.\nShana Poplack. 2001b. Code switching: Linguis-\ntic. International Encyclopedia of the Social and"
      },
      {
        "source_id": "source_13",
        "citation": "No Language Left Behind_ Scaling Human-Centered Machine Translation.pdf",
        "text_snippet": "unrecorded interviews, two researchers took extensive notes throughout. Bringing all 44\ninterviews together, responses were then systematically coded to allow major themes and\nideas to emerge.\nWe acknowledge that sampling low-resource language speakers from diasporic contexts\ncomes with its limitations. For one, as immigrants, their perspectives may not consummately\ncapture the sentiments of their communities back home. That said, some scholars have\nargued that in technologically underdeveloped nations, where many low-resource language\ncommunities reside, people tend to view technology more optimistically and aspirationally\nthan those who live in places with higher levels of technological development (Kapania\net al., 2022; Kozyreva et al., 2021; Sambasivan, 2021; Sambasivan et al., 2021). Thus, being\nexposed to critical technological discourses (especially in recent times) could in fact make\nmany of our interviewees more cognizant of the risks behind technological advancement,"
      }
    ],
    "time_taken": 1.12
  },
  {
    "question": "Do 'AfroBench' and 'IrokoBench' agree on the performance of GPT-4 for African languages?",
    "answer": "Insufficient Evidence",
    "citations_readable": [],
    "citations_raw": [],
    "retrieved_chunks": [
      {
        "source_id": "source_33",
        "citation": "AfroBench_ How Good are Large Language Models on African Languages?.pdf",
        "text_snippet": "arXiv:2311.07978v5  [cs.CL]  7 Jun 2025\nAfroBench: How Good are Large Language Models on African Languages?\nJessica Ojo 1,3\u2217, Odunayo Ogundepo 4,5\u2217, Akintunde Oladipo 4,5\u2217, Kelechi Ogueji5\u2217,\nJimmy Lin 5, Pontus Stenetorp 6, David Ifeoluwa Adelani 1,2\u2217\n\u2217Masakhane NLP, 1Mila - Quebec AI Institute & McGill University, 2Canada CIFAR AI Chair, 3Lelapa AI,\n4The African Research Collective 5University of Waterloo, 6University College London\nCorrespondence:{jessica.ojo, david.adelani}@mila.quebec\nAbstract\nLarge-scale multilingual evaluations, such as\nMEGA, often include only a handful of African\nlanguages due to the scarcity of high-quality\nevaluation data and the limited discoverabil-\nity of existing African datasets. This lack\nof representation hinders comprehensive LLM\nevaluation across a diverse range of languages\nand tasks. To address these challenges, we\nintroduce AFROBENCH \u2014a multi-task bench-\nmark for evaluating the performance of LLMs\nacross 64 African languages, 15 tasks and 22"
      },
      {
        "source_id": "source_08",
        "citation": "IrokoBench_ A New Benchmark for African Languages in the Age of Large  Language Models.pdf",
        "text_snippet": "Question: {{hypothesis}} True, False, or\nNeither?, have better results for the open models\nlike Aya-101 and Gemma 2 27B, while GPT-4o\nprefers t3 where a detailed task description is pro-\nvided. The best prompt for Aya-101 ist4 where the\n{{language}} name is mentioned, which shows\nadditional language information may be useful in\nimproving performance.\nOn AfriMMLU, we find GPT-4o perform worse\nfor t4.4. However, other models are not very sensi-\ntive to the use of different prompts. In general, we\ndo not find AfriMGSM to be sensitive to different\nprompts. In subsection A.4, we provide the results\nof five prompt templates for all LLMs evaluated.\n5 Conclusion\nIn this paper, we introduced IROKO BENCH , a new\nbenchmark for evaluating large language models\n(LLMs) on African languages. IROKO BENCH com-\nprises three datasets focused on different tasks: nat-\nural language inference (AfriXNLI), multi-choice\nknowledge QA (AfriMMLU), and mathematical\nreasoning (AfriMGSM). Unlike previous bench-"
      },
      {
        "source_id": "source_33",
        "citation": "AfroBench_ How Good are Large Language Models on African Languages?.pdf",
        "text_snippet": "we conduct an extensive analysis of the perfor-\nmance of LLMs for African languages from differ-\nent language families and geographical locations.\nFor our evaluation, we compute the aver-\nage performance score over the 15 tasks cov-\nered in AFROBENCH . Additionally, we intro-\nduce AFROBENCH -L ITE that only cover a sub-\nset of seven tasks and 14 diverse languages in\nAFROBENCH which reduces the evaluation cost\nfor a newly introduced LLM on our leaderboard.\nFigure 1 shows our evaluation on AFROBENCH ,\nwe find that proprietary models such as GPT-4o\nand Gemini-1.5 pro achieve +13 score improve-\nment over Gemma 2 27B , our best-performing open\nmodel. We also compared the performance of En-\nglish language to 14 African languages, finding that\nGPT-4o and Gemma 2 27B achieve better perfor-\nmance than African languages by more than +25\nand +40 score improvements respectively. This\nshows that the gap in the multilingual abilities of\nopen models is wider than that of proprietary mod-"
      },
      {
        "source_id": "source_08",
        "citation": "IrokoBench_ A New Benchmark for African Languages in the Age of Large  Language Models.pdf",
        "text_snippet": "IROKO BENCH (amh, hau, ibo, kin, orm, sna, sot,\nswa, xho, yor, and zul). AfroXLMR-76L fol-\nlows the same technique by performing continual\npre-training on XLM-R-large on 76 languages (72\nAfrican), all languages covered in IROKO BENCH\nare part of its pre-training.\nWe found Africa-centric MLM to perform bet-\nter on average than massively multilingual mod-\nels like XLM-R-large. Serengeti and AfroXLMR-\n15"
      },
      {
        "source_id": "source_08",
        "citation": "IrokoBench_ A New Benchmark for African Languages in the Age of Large  Language Models.pdf",
        "text_snippet": "IrokoBench: A New Benchmark for African Languages in the Age of Large\nLanguage Models\nDavid Ifeoluwa Adelani1,2\u2217, Jessica Ojo1,3\u2217, Israel Abebe Azime4\u2217, Jian Yun Zhuang5,\nJesujoba O. Alabi4, Xuanli He6, Millicent Ochieng7, Sara Hooker8, Andiswa Bukula9,\nEn-Shiun Annie Lee10, Chiamaka Chukwuneke11, Happy Buzaaba12, Blessing Sibanda\u2217,\nGodson Kalipe\u2217, Jonathan Mukiibi13\u2217, Salomon Kabongo14\u2217, Foutse Yuehgoh15\u2217,\nMmasibidi Setaka9, Lolwethu Ndolela\u2217, Nkiruka Odu\u2217, Rooweither Mabuya9,\nShamsuddeen Hassan Muhammad16, Salomey Osei17\u2217, Sokhar Samb18\u2217,\nTadesse Kebede Guge19\u2217, Tombekai Vangoni Sherman20, Pontus Stenetorp6\n\u2217Masakhane NLP, 1Mila, McGill University, 2Canada CIFAR AI Chair, 3Lelapa AI, 4Saarland University,\n5University of Toronto, 6University College London, 7Microsoft Research Africa, 8Cohere For AI, 9SADiLaR,\n10Ontario Tech University, 11Lancaster University, 12Princeton university, 13Makerere University,"
      },
      {
        "source_id": "source_33",
        "citation": "AfroBench_ How Good are Large Language Models on African Languages?.pdf",
        "text_snippet": "and tasks. To address these challenges, we\nintroduce AFROBENCH \u2014a multi-task bench-\nmark for evaluating the performance of LLMs\nacross 64 African languages, 15 tasks and 22\ndatasets. AFROBENCH consists of nine natural\nlanguage understanding datasets, six text gen-\neration datasets, six knowledge and question\nanswering tasks, and one mathematical reason-\ning task. We present results comparing the\nperformance of prompting LLMs to fine-tuned\nbaselines based on BERT and T5-style models.\nOur results suggest large gaps in performance\nbetween high-resource languages, such as En-\nglish, and African languages across most tasks;\nbut performance also varies based on the avail-\nability of monolingual data resources. Our find-\nings confirm that performance on African lan-\nguages continues to remain a hurdle for current\nLLMs, underscoring the need for additional\nefforts to close this gap.1\n1 Introduction\nLarge language models (LLMs) have risen to the"
      },
      {
        "source_id": "source_33",
        "citation": "AfroBench_ How Good are Large Language Models on African Languages?.pdf",
        "text_snippet": "points on AFROBENCH -L ITE. These results sug-\ngest that current LLMs especially the open models,\nare more biased towards English and a few high-\nresource languages. Adapting LLMs for a region\nof African languages could help bridge the gap.\nFor instance, we see that continually pre-training\nLLaMa 3 8B, that resulted in LLaMaX 8B shows\nslight overall performance of +1.4 or more over\nvanilla LLaMa 3 8B in Table 4. However, to further\nboost performance, better adaptation techniques are\nneeded.\nPerformance varies across languages Figure 3\nshows the results for per-language performance\nscores of 14 languages in AFROBENCH -LITE. Our\nresult shows that performance correlates with the\navailable monolingual text on the web (Kudugunta\net al., 2023). We find that Swahili (swa) with over\n2.4GB of monolingual text has the highest perfor-\nmance among the African languages, while Wolof\nwith the smallest monolingual data (5MB) has the\nlowest performance. While this data size estimates"
      },
      {
        "source_id": "source_08",
        "citation": "IrokoBench_ A New Benchmark for African Languages in the Age of Large  Language Models.pdf",
        "text_snippet": "10Ontario Tech University, 11Lancaster University, 12Princeton university, 13Makerere University,\n14Leibniz Universit\u00e4t Hannover, 15Le CNAM, 16Imperial College London, 17Universidad de Deusto, 18DAUST,\n19Haramaya University.\nAbstract\nDespite the widespread adoption of Large lan-\nguage models (LLMs), their remarkable capa-\nbilities remain limited to a few high-resource\nlanguages. Additionally, many low-resource\nlanguages ( e.g., African languages) are of-\nten evaluated only on basic text classifica-\ntion tasks due to the lack of appropriate or\ncomprehensive benchmarks outside of high-\nresource languages. In this paper, we intro-\nduce IrokoBench\u2014a human-translated bench-\nmark dataset for 17 typologically-diverse low-\nresource African languages covering three\ntasks: natural language inference (AfriXNLI),\nmathematical reasoning (AfriMGSM), and\nmulti-choice knowledge-based question an-\nswering (AfriMMLU). We use IrokoBench to\nevaluate zero-shot, few-shot, and translate-test"
      },
      {
        "source_id": "source_16",
        "citation": "Where Are We? Evaluating LLM Performance on African Languages.pdf",
        "text_snippet": "mad, Salomey Osei, Sokhar Samb, Tadesse Kebede\nGuge, and Pontus Stenetorp. 2024b. Irokobench: A\nnew benchmark for african languages in the age of\nlarge language models.\nBenjamin Akera, Jonathan Mukiibi, Lydia Sanyu Nag-\ngayi, Claire Babirye, Isaac Owomugisha, Solomon\nNsumba, Joyce Nakatumba-Nabende, Engineer Bain-\nomugisha, Ernest Mwebaze, and John Quinn. 2022.\nMachine translation for african languages: Commu-\nnity creation of datasets and models in uganda. In3rd\nWorkshop on African Natural Language Processing.\nJesujoba Alabi, Kwabena Amponsah-Kaakyire, David\nAdelani, and Cristina Espa\u00f1a-Bonet. 2020a. Massive\nvs. curated embeddings for low-resourced languages:\nthe case of Yor\u00f9b\u00e1 and Twi. In Proceedings of the\nTwelfth Language Resources and Evaluation Confer-\nence, pages 2754\u20132762, Marseille, France. European\nLanguage Resources Association.\nJesujoba Alabi, Kwabena Amponsah-Kaakyire, David\nAdelani, and Cristina Espana-Bonet. 2020b. Massive"
      },
      {
        "source_id": "source_33",
        "citation": "AfroBench_ How Good are Large Language Models on African Languages?.pdf",
        "text_snippet": "Benchmark # Tasks # Datasets # African Lang. # LLMs Closed LLMs evaluated Dominant task(s)\nChatGPT-MT (Robinson et al., 2023) 1 1 57 1 GPT-3.5 MT\nMega (Ahuja et al., 2023a) 10 16 11 4 GPT-3, GPT-3.5-Turbo, GPT-4 POS, NER\nMegaverse (Ahuja et al., 2024) 16 22 16 8 PaLM, GPT-3.5, GPT-4, Gemini Pro POS, NER, XQA\nSIB-200 (Adelani et al., 2024a) 1 1 57 2 GPT-3.5, GPT-4 Topic classification\nBelebele (Bandarkar et al., 2024) 1 1 28 6 GPT-3.5-Turbo QA\nUhura (Bayes et al., 2024) 1 2 6 6 Claude-3.5-Sonnet, GPT-4, 4o, o1-preview QA\nIrokoBench (Adelani et al., 2024b) 3 3 16 16 GPT-3.5,4,4o, Gemini-1.5-Pro, Claude OPUS NLI, MMLU, Math.\nAFROBENCH(Ours) 15 22 60 12 Gemini-1.5-Pro, GPT-4o several\nTable 1: Overview of Related works that evaluated on African languages. We included the number of tasks,\ndatasets, African languages, LLMs evaluated, and the dominant tasks covering at least three African languages.\n(2) Evaluation of LLMs needs to be continuous\nsince many new LLMs have been released with im-"
      },
      {
        "source_id": "source_33",
        "citation": "AfroBench_ How Good are Large Language Models on African Languages?.pdf",
        "text_snippet": "D AfroBench Evaluation with Confidence\nScores\nWe computed 95% confidence intervals AfroBench\nresults to quantify statistical significance. The cal-\nculation was based on the results of 5 prompts\nfor each task (3 prompts for NLG tasks). Table 9\npresents the average performance and confidence\nintervals accross prompts to assess variability and\nsignificance.\nE Newer LLM evaluation on\nAfroBench-Lite\nWe extended our evaluation for AFROBENCH -\nLITEto includes newer LLMs such as Lugha-\nLLaMa (an African-centric LLM) (Buzaaba et al.,\n2025), GPT-4.1, Gemini-2.0-Flash, and LLaMa 4\n400B (Maverick) in Table 8.\nF Languages covered in the evaluation\nTable 10 shows the languages and tasks we evalu-\nated on.\n18"
      },
      {
        "source_id": "source_33",
        "citation": "AfroBench_ How Good are Large Language Models on African Languages?.pdf",
        "text_snippet": "2021; Biderman et al., 2024). These challenges\nstem from complex nature of language understand-\ning and the stochastic nature of language models\nMultilingual LLM Benchmarks: Benchmarks\nserve as a standard for measuring how systems\nhave improved over time on across specific tasks\nand metrics. In the context of LLMs, multilingual\nbenchmarks are crucial to assessing both the qual-\nity and practical utility of these models across di-\nverse languages and tasks. Our primary focus lies\nin understanding LLM performance specifically\nfor African languages, with several notable bench-\nmarks having emerged in recent years to address\nthis need. ChatGPT-MT (Robinson et al., 2023)\nevaluated the translation capability of GPT-4 and\nthey find that it\u2019s demonstrates strong performance\non high-resource languages, the performance on\nlow-resource languages is subpar. Belebele (Ban-\n2"
      }
    ],
    "time_taken": 1.27
  },
  {
    "question": "How do 'NileChat' and 'Jawaher' differ in their approach to Arabic dialects?",
    "answer": "'NileChat' focuses on enhancing Arabic dialects through controlled synthetic data generation and retrieval-augmented pre-training, specifically targeting Moroccan Darija and Egyptian Arabic, including Arabizi variants. It aims to improve understanding, translation, and cultural alignment for these dialects. In contrast, 'Jawaher' provides a collection of Arabic proverbs from 20 different Arabic varieties, with idiomatic or literal English translations and explanations in both Arabic and English, to facilitate understanding of cultural contexts and themes across a broader range of dialects.",
    "citations_readable": [
      "Jawaher_ A Multidialectal Dataset of Arabic Proverbs for LLM  Benchmarking.pdf",
      "NileChat_ Towards Linguistically Diverse and Culturally Aware LLMs for Local Communities.pdf"
    ],
    "citations_raw": [
      "source_23",
      "source_29"
    ],
    "retrieved_chunks": [
      {
        "source_id": "source_23",
        "citation": "Jawaher_ A Multidialectal Dataset of Arabic Proverbs for LLM  Benchmarking.pdf",
        "text_snippet": "20 different Arabic varieties. Below, we outline the\nanalysis of Jawaher, focusing on its coverage of\ndialects, themes, cultural context, and the tasks it\nfacilitates.\n3.1 Jawaher Analysis\nDialect Representation. Data in Jawaher is man-\nually curated by four native Arabic speakers with\nstrong linguistic expertise. The four annotators come\nfrom Egypt (two annotators), Mauritania, and Mo-\nrocco. During data collection, they consulted with\nnative speakers from other countries such as Jordan,\nFigure 2: Choropleth map showing the geographical dis-\ntribution of Arabic varieties covered in Jawaher. Color\nintensity represents the percentage of proverbs collected\nfrom each region, with darker shades indicating higher\nconcentrations.\nSyria, and the United Arab Emirates (UAE) to ensure\na diverse and authentic representation of proverbs\nfrom countries across the Arab world. We acquire\ndata from publicly available online resources and\ncarefully verify the origins of proverbs to confirm"
      },
      {
        "source_id": "source_23",
        "citation": "Jawaher_ A Multidialectal Dataset of Arabic Proverbs for LLM  Benchmarking.pdf",
        "text_snippet": "texts (Gupta et al., 2023; Kova \u02c7c et al., 2023). Re-\nsearch has increasingly uncovered a cultural gap,\ndemonstrating that many models are biased to-\nward Western, Anglo-centric perspectives (Johnson\net al., 2022; Liu et al., 2023). These biases im-\npact linguistic-cultural interactions and challenge\nvalue-based objectives (Johnson et al., 2022; Dur-\nmus et al., 2023). Efforts to address this include\nmultilingual QA (Kabra et al., 2023), cross-cultural\ntranslation (Singh et al., 2024), and culturally diverse\ndataset creation (Ji et al., 2024; Qian et al., 2024),\nall aiming to improve multilingual adaptation and\ncultural alignment in LLMs.\n3 Jawaher\nJawaher consists of Arabic proverbs paired with their\nidiomatic or literal English translations, along with\nexplanations in both Arabic and English, covering\n20 different Arabic varieties. Below, we outline the\nanalysis of Jawaher, focusing on its coverage of\ndialects, themes, cultural context, and the tasks it\nfacilitates."
      },
      {
        "source_id": "source_29",
        "citation": "NileChat_ Towards Linguistically Diverse and Culturally Aware LLMs for Local Communities.pdf",
        "text_snippet": "tian and Moroccan communities, incorporating\ntheir language, cultural heritage, and values.\nOur results on various understanding, transla-\ntion, and cultural and values alignment bench-\nmarks show thatNileChatoutperforms existing\nArabic-aware LLMs of similar size and per-\nforms on par with larger models. This work\naddresses Arabic dialect in LLMs with a fo-\ncus on cultural and values alignment via con-\ntrolled synthetic data generation and retrieval-\naugmented pre-training for Moroccan Darija\nand Egyptian Arabic, including Arabizi vari-\nants, advancing Arabic NLP for low-resource\n1\narXiv:2505.18383v3  [cs.CL]  22 Sep 2025"
      },
      {
        "source_id": "source_23",
        "citation": "Jawaher_ A Multidialectal Dataset of Arabic Proverbs for LLM  Benchmarking.pdf",
        "text_snippet": "English Explanation. Jawaher includes 2, 500\nhuman-translated English explanations of Arabic\nproverbs. These explanations cover proverbs with\nhistorical backgrounds or cultural stories from the\nArab world, and we aim to have these explanations\naccurately convey the meanings behind the proverbs.\nFull statistics for each task and examples across\nall dialects are provided in the Appendix C.\n4 Experimental Setup\nWe evaluate Jawaher using both open- and closed-\nsource state-of-the-art multilingual LLMs (mLLMs)\nto assess their abilities across our proposed tasks.\nThe models are tested in a zero-shot setting (Sanh\net al., 2021), allowing us to evaluate their inherent\nFigure 5: Designed prompt used to test model\u2019s perfor-\nmance on Jawaher.\ncapacity to interpret, explain, and contextualize Ara-\nbic proverbs. To achieve this, we create a universal\nprompt template in English: (1) We set therole of the\nmodel as a language expert with deep knowledge of"
      },
      {
        "source_id": "source_29",
        "citation": "NileChat_ Towards Linguistically Diverse and Culturally Aware LLMs for Local Communities.pdf",
        "text_snippet": "ages a teacher model proficient in generating low-\nresource languages to enable: (i) translation for the\nincorporation of community-specific language and\n(ii) controlled generation and retrieval mechanisms\nfor the authentic inclusion of cultural heritage and\nvalues. We validated our methodology using the\nMoroccan and Egyptian Arabic dialects as testbeds\nby developingNileChat, an LLM covering these\ntwo dialects. Comprehensive evaluations on under-\nstanding, translation, and cultural alignment bench-\nmarks demonstrate that our method significantly\nenhances the baseline LLM\u2019s performance in cap-\nturing target language nuances and cultural val-\nues. Notably,NileChatalso outperforms existing\nArabic-aware LLMs. Our method offers a promis-\ning research direction for fostering inclusivity of\ndiverse local communities within LLM develop-\nment, thereby emphasizing the critical role of such\nan inclusion in the broader democratization of this\ntechnology.\nLimitations"
      },
      {
        "source_id": "source_23",
        "citation": "Jawaher_ A Multidialectal Dataset of Arabic Proverbs for LLM  Benchmarking.pdf",
        "text_snippet": "Levantine dialects demonstrate strong cohesion, with\nJordanian (JOR), Lebanese (LEB), and Palestinian\n(PAL) proverbs clustering closely, indicating deep\ncultural interconnections. North African dialects dis-\nplay greater dispersion, particularly among Algerian\n(ALG), Mauritanian (MAU), and Tunisian (TUN)\nproverbs, suggesting linguistic diversity within this\nregion. The Arabic Peninsula and Nile Basin groups,\nrepresented by Saudi (SAU) and Yemeni (YEM) di-\nalects, and Egyptian (EGY) and Sudanese (SUD)\ndialects, respectively, exhibit unique patterns that\nmay reflect historical and sociolinguistic factors.\n3.2 Task Representation\nEach proverb in Jawaher is represented with (a)\nan idiomatic, literal English translation, or English\nmeaning, (b) an explanation in Arabic, and (c) an\nexplanation in English. The explanations include\nstories detailing the themes and contexts in which\nthe proverb is commonly used. Explanations, as\nsuch, enhance understanding and provide deeper in-"
      },
      {
        "source_id": "source_29",
        "citation": "NileChat_ Towards Linguistically Diverse and Culturally Aware LLMs for Local Communities.pdf",
        "text_snippet": "formance with recent leading Arabic LLMs like\nALLaM-7B, with a performance gap of less than\n3 points on most tasks, and even surpasses it on\ncertain benchmarks, including Belebele. Results\nfor 3-shot are presented in Table D.1 and they show\na similar trend to the zero-shot ones.\nCultural Knowledge.As shown in Table 1, our\napproach significantly enhances cultural knowl-\nedge (Palm), enablingNileChatto achieve scores\nof 5.72 (EGY) and 5.86 (MOR), compared to base-\nline Qwen2.5-3B-instruct scores of 2.86 and 2.31,\nrespectively. Among similarly sized models, ours\nachieves the highest performance on MOR and\nranks second only to Gemma-3-4B for EGY . Al-\nthough larger models such as Gemma-3-12B ex-\nhibit superior overall scores (EGY: 8.71, MOR:\n7.09),NileChatnotably surpasses AceGPT-7B and\n-13B on Moroccan cultural knowledge, despite their\nclaimed alignment with Arabic cultures. Addition-\nally, it outperforms Atlas-chat-2B and -9B, mod-\nels specifically fine-tuned for Moroccan dialects."
      },
      {
        "source_id": "source_29",
        "citation": "NileChat_ Towards Linguistically Diverse and Culturally Aware LLMs for Local Communities.pdf",
        "text_snippet": "NileChat: Towards Linguistically Diverse and Culturally Aware LLMs\nfor Local Communities\nAbdellah El Mekki\u03bb* Houdaifa Atou\u03b4 Omer Nacar\u03c8\nShady Shehata\u03b3 Muhammad Abdul-Mageed\u03bb,\u03b3\n\u03bbThe University of British Columbia \u03b4Mohammed VI Polytechnic University\n\u03c8Tuwaiq Academy \u03b3Invertible AI\n{abdellah.elmekki,muhammad.mageed}@ubc.ca\n(b) Controlled generation\n(a) Translation\nEnglish\nData\nTranslate \nto low-resource language\nLLM\n \ufbfe\ufe8e \ufee3\ufebb\u0637\ufed4\ufef0\u060c \ufee3\u0634 \u06be\ufe97\ufe8e\ufedb\u0644\u061f\u0625\u0632\ufbfe\u0643 \u0623\ufea3\ufee3\u062f:\n\u060c \ufedb\ufee7\u062a \ufe91\ufedb\ufee0\u0645 \u0627\ufedf\ufee3\u062f\u0627\u0645. \ufe91\ufed8\ufee0\ufed6 \ufee3\u0646 \u0623\ufedb\u0644 \u0627\ufedf\ufeb7\ufe8e\u0631\u0639 \u062f\u0647\u060c \ufee3\ufecc\ufee0\u0634\ufee3\ufebb\u0637\ufed4\ufef0: \n\u061f\ufee3\u0634 \ufedb\u062f\u0647\n \ufedb\ufeb7\u0631\u064a\u060c \u0627\ufedf\ufee3\ufea3\u0644 \u062f\u0647 \ufee7\ufebf\ufbfe\u0641. \u0648\u062f\u0647 \ufbfe\ufe8e \ufecb\u0645 \ufee3\ufe97\ufed8\ufee0\ufed8\u0634 \u0623\ufea3\ufee3\u062f:\n\u0625\ufeb3\ufedb\ufee7\u062f\u0631\u0627\ufee7\ufef2 \ufe91\ufe8e\ufedf\ufecc\u062f\u0633 \u0627\ufef7\ufebb\ufed4\u0631\u060c \ufecf\ufbfe\u0631 \u0627\ufedf\ufee0\ufef2 \ufe97\ufecc\u0631\ufed3\ufbab.\n \u0631\ufe91\ufee7\ufe8e \ufbfe\ufeb3\ufe97\u0631. \u0627\ufedf\u0648\u0627\ufea3\u062f \ufe91\ufbfe\ufea7\ufe8e\u0641 \ufecb\ufee0\ufef0 \ufebb\ufea3\ufe97\ufbab \ufecb\ufeb7\ufe8e\u0646 \u0627\ufedf\ufecc\ufbfe\ufe8e\u0644.\ufee3\ufebb\u0637\ufed4\ufef0:\n \ufedb\ufee0\ufbad\ufe8e \ufee3\u0631\u0629 \ufee3\u0634 \u06be\ufe97\ufebf\u0631\u060c \u0627\ufedf\ufee3\ufbad\u0645 \u0625\ufee7\ufee7\ufe8e \ufed7\ufe8e\ufecb\u062f\ufbfe\u0646 \ufeb3\u0648\u0627.\u0623\ufea3\ufee3\u062f:\n \u0627\ufedf\ufea3\ufee3\u062f \u0644\u0644\u0647. \ufecb\ufee7\u062f\u0643 \ufea3\ufed6.\ufee3\ufebb\u0637\ufed4\ufef0:\nAhmed: How are you, Mostafa? Aren't you going \nto eat?\nMostafa: Sorry, I was talking to the wife. This \nstreet food makes me nervous, right?\nAhmed: Man, don't worry, this place is clean. \nBesides, this is Alexandrian koshary with yellow"
      },
      {
        "source_id": "source_13",
        "citation": "No Language Left Behind_ Scaling Human-Centered Machine Translation.pdf",
        "text_snippet": "Ren\u00e9 Haas and Leon Derczynski. Discriminating between similar nordic languages. In\nProceedings of the Eighth Workshop on NLP for Similar Languages, Varieties and Dialects ,\npages 67\u201375, Kiyv, Ukraine, April 2021. Association for Computational Linguistics. URL\nhttps://aclanthology.org/2021.vardial-1.8.\nNizar Habash. Introduction to arabic natural language processing. In Introduction to Arabic\nNatural Language Processing, 2010.\nNizar Habash, Ryan Roth, Owen Rambow, Ramy Eskander, and Nadi Tomeh. Morpho-\nlogical analysis and disambiguation for dialectal Arabic. In NAACL, pages 426\u2013432,\nAtlanta, Georgia, June 2013. Association for Computational Linguistics. URL https:\n//aclanthology.org/N13-1044.\nGilles Hacheme. English2gbe: A multilingual machine translation model for {Fon/Ewe}gbe.\nCoRR, abs/2112.11482, 2021. URL https://arxiv.org/abs/2112.11482.\nBarry Haddow, Rachel Bawden, Antonio Valerio Miceli Barone, Jind\u0159ich Helcl, and Alexandra"
      },
      {
        "source_id": "source_13",
        "citation": "No Language Left Behind_ Scaling Human-Centered Machine Translation.pdf",
        "text_snippet": "the slightly different procedure which was used to collect their evaluation data. Second, since\ntheir evaluation data was translated directly from English (as opposed to being adapted\nfrom MSA which was itself translated from English), it may actually be closer semantically\nand syntactically to the English source, making the task of translating it easier.\nWe then analyze how distinct the dialects generated by our translation model are. In\nthe rightmost part of Tables 44 and 45, we measure the differences between the generated\nnon-MSA Arabic text and the generated MSA. For the first four languoids in the group, the\nmajor trends observed in Table 43 still hold, with Tunisian and Mesopotamian being the\nfarthest away from MSA, and Najdi being the closest. More generally however, we observe a\nslight flattening of the differences in dialectness levels of the text translated from English\ncompared to the human-annotated evaluation data. Indeed, the sentence-level DL scores"
      },
      {
        "source_id": "source_29",
        "citation": "NileChat_ Towards Linguistically Diverse and Culturally Aware LLMs for Local Communities.pdf",
        "text_snippet": "guages; see B.2). We fine-tune each dialect-specific\nmodel for two epochs and employ weighted linear\naveraging (Aakanksha et al., 2024) for merging,\ndubbing our merged modelNileChat.\nMore information about our model merging is\nin B.2 and the prompts used for generating and\ntranslating our pre- and fine-tuning datasets is in E.\n4 Experiments\n4.1 Evaluation Tasks\nWe employ a comprehensive evaluation framework\nto measure the performance ofNileChatfor EGY\nand MOR. This framework enables comparison\nwith our baseline and other LLMs across multi-\nple capability dimensions: Understanding, cultural\nknowledge, translation, and value alignment.\nUnderstanding.We evaluate understanding ca-\npabilities using MMLU (Hendrycks et al., 2021),\nHellaSwag (Zellers et al., 2019), and Bele-\nbele (Bandarkar et al., 2024) benchmarks, each\nadapted to both EGY and MOR dialects. For MOR,\nwe directly employ the MMLU and HellaSwag ver-\nsions provided by Shang et al. (2025). For EGY , we"
      },
      {
        "source_id": "source_29",
        "citation": "NileChat_ Towards Linguistically Diverse and Culturally Aware LLMs for Local Communities.pdf",
        "text_snippet": "ALLaM-7B-Instruct7 Feb. 2025\nQwen2.5-7B-Instruct7.6 Apr. 2025\nQwen3-8B8.2 Apr. 2025\nAtlas-Chat-9B9.2 Sep. 2024\ngemma-3-12b-it12.2 Mar. 2025\nAceGPT-13B-chat13 Dec. 2023\njais-13b-chat13 Aug. 2023\nTable C.2: The LLMs used for comparison against NileChat in this evaluation were selected from a list of Arabic-\naware models. Each LLM is listed with its corresponding size (in billion parameters) and release date. We utilized\nthe instruct version for all LLMs except for NLLB, which is a machine translation-specific model.\n22"
      }
    ],
    "time_taken": 2.31
  },
  {
    "question": "What does the corpus say about 'Quantum Computing in Yoruba'?",
    "answer": "Insufficient Evidence",
    "citations_readable": [],
    "citations_raw": [],
    "retrieved_chunks": [
      {
        "source_id": "source_13",
        "citation": "No Language Left Behind_ Scaling Human-Centered Machine Translation.pdf",
        "text_snippet": "and diacritics in Yoruba\u2013English neural machine translation. In Proceedings of the\n18th Biennial Machine Translation Summit (Volume 1: Research Track) , pages 61\u201375,\nVirtual, August 2021. Association for Machine Translation in the Americas. URL https:\n//aclanthology.org/2021.mtsummit-research.6.\nDavid Ifeoluwa Adelani, Jesujoba Oluwadara Alabi, Angela Fan, Julia Kreutzer, Xiaoyu Shen,\nMachel Reid, Dana Ruiter, Dietrich Klakow, Peter Nabende, Ernie Chang, Tajuddeen\nGwadabe, Freshia Sackey, Bonaventure F. P. Dossou, Chris Chinenye Emezue, Colin\nLeong, Michael Beukman, Shamsuddeen Hassan Muhammad, Guyo Dub Jarso, Oreen\nYousuf, Andre Niyongabo Rubungo, Gilles Hacheme, Eric Peter Wairagala, Muham-\nmad Umair Nasir, Benjamin Ayoade Ajibade, Tunde Oluwaseyi Ajayi, Yvonne Wambui\nGitau, Jade Abbott, Mohamed Ahmed, Millicent Ochieng, Anuoluwapo Aremu, Perez\nOgayo, Jonathan Mukiibi, Fatoumata Ouoba Kabore, Godson Koffi Kalipe, Derguene"
      },
      {
        "source_id": "source_19",
        "citation": "Bridging Gaps in Natural Language Processing for Yoru\u0300ba\u0301_ A  Systematic Review of a Decade of Progress and Prospects.pdf",
        "text_snippet": "[99] Th\u00b4 eophile K Dagba, John OR Aoga, and Codjo C Fanou. Design of a yoruba language\nspeech corpus for the purposes of text-to-speech (tts) synthesis. InIntelligent Information\nand Database Systems: 8th Asian Conference, ACIIDS 2016, Da Nang, Vietnam, March\n14\u201316, 2016, Proceedings, Part I 8 , pages 161\u2013169. Springer, 2016.\n[100] Sunday Adeola Ajagbe. Developing nigeria multilingual languages speech datasets for an-\ntenatal orientation. In Hector Florez and Hern\u00b4 an Astudillo, editors,Applied Informatics,\npages 157\u2013170, Cham, 2024. Springer Nature Switzerland.\n[101] Tolulope Ogunremi, Kola Tubosun, Anuoluwapo Aremu, Iroro Orife, and David Ifeoluwa\nAdelani. `Ir` oy` \u0131nspeech: A multi-purpose yor` ub\u00b4 a speech corpus. InProceedings of the 2024\nJoint International Conference on Computational Linguistics, Language Resources and\nEvaluation (LREC-COLING 2024) , pages 9296\u20139303, 2024.\n[102] Daniel R Van Niekerk and Etienne Barnard. Predicting utterance pitch targets in yor` ub\u00b4 a"
      },
      {
        "source_id": "source_19",
        "citation": "Bridging Gaps in Natural Language Processing for Yoru\u0300ba\u0301_ A  Systematic Review of a Decade of Progress and Prospects.pdf",
        "text_snippet": "[35] Kayode Olaleye, Dan Oneat \u00b8\u02d8 a, and Herman Kamper. Yfacc: A yor` ub\u00b4 a speech\u2013image\ndataset for cross-lingual keyword localisation through visual grounding. In 2022 IEEE\nSpoken Language Technology Workshop (SLT), pages 731\u2013738. IEEE, 2023.\n[36] Alexander Gutkin, Isin Demirsahin, Oddur Kjartansson, Clara E Rivera, and K\u00b4 ol\u00b4 a\nT\u00b4 ub` os\u00b4 un. Developing an open-source corpus of yoruba speech. InInterspeech, 2020.\n[37] Idris Akinade, Jesujoba Alabi, David Adelani, Clement Odoje, and Dietrich Klakow.\nVarepsilon k\u00b4 u mask: Integrating yor` ub\u00b4 a cultural greetings into machine translation. In\nProceedings of the First Workshop on Cross-Cultural Considerations in NLP (C3NLP) ,\npages 1\u20137, 2023.\n[38] Theresa Okediya, Ibukun Afolabi, Olamma Iheanetu, and Sunday Ojo. Building ontology\nfor yor` ub\u00b4 a language. InProceedings of the First International Workshop on NLP Solu-\ntions for Under Resourced Languages (NSURL 2019) co-located with ICNLSP 2019-Short\nPapers, pages 124\u2013130, 2019."
      },
      {
        "source_id": "source_01",
        "citation": "Voices Unheard_ NLP Resources and Models for Yoru\u0300ba\u0301 Regional Dialects.pdf",
        "text_snippet": "four regional Yor\u00f9b\u00e1 dialects. To develop this\ncorpus, we engaged native speakers, travelling\nto communities where these dialects are spo-\nken, to collect text and speech data. Using our\nnewly created corpus, we conducted extensive\nexperiments on (text) machine translation, au-\ntomatic speech recognition, and speech-to-text\ntranslation. Our results reveal substantial per-\nformance disparities between standard Yor\u00f9b\u00e1\nand the other dialects across all tasks. How-\never, we also show that with dialect-adaptive\nfinetuning, we are able to narrow this gap. We\nbelieve our dataset and experimental analysis\nwill contribute greatly to developing NLP tools\nfor Yor\u00f9b\u00e1 and its dialects, and potentially for\nother African languages, by improving our un-\nderstanding of existing challenges and offering\na high-quality dataset for further development.\nWe releaseYOR\u00d9LECT dataset and models pub-\nlicly under an open license 1.\n1 Introduction\nWhile great strides have been made in developing"
      },
      {
        "source_id": "source_01",
        "citation": "Voices Unheard_ NLP Resources and Models for Yoru\u0300ba\u0301 Regional Dialects.pdf",
        "text_snippet": "maka Chukwuneke, Christine Mwase, Clemencia\nSiro, Stephen Arthur, Tunde Ajayi, Verrah Otiende,\nAndre Rubungo, Boyd Sinkala, Daniel Ajisafe,\nEmeka Onwuegbuzia, Falalu Lawan, Ibrahim Ah-\nmad, Jesujoba Alabi, Chinedu Mbonu, Mofetoluwa\nAdeyemi, Mofya Phiri, Orevaoghene Ahia, Ruqayya\nIro, and Sonia Adhiambo. 2023. Cross-lingual open-\nretrieval question answering for African languages.\nIn Findings of the Association for Computational\nLinguistics: EMNLP 2023, pages 14957\u201314972, Sin-\ngapore. Association for Computational Linguistics.\nTolulope Ogunremi, Kola Tubosun, Anuoluwapo\nAremu, Iroro Orife, and David Ifeoluwa Adelani.\n2024. \u00ccr\u00f2y\u00ecnspeech: A multi-purpose yor\u00f9b\u00e1 speech\ncorpus.\nValentine Ojo. 1977. English-yoruba language contact\nin nigeria. (No Title).\nEmmanuel Om\u00f3n\u00edy\u00ec Ol\u00e1nrew\u00e1j\u00fa. 2022. A contrastive\nanalysis of interrogatives in standard yor\u00f9b\u00e0 and cen-\ntral yor\u00f9b\u00e0 dialects. Hayatian Journal of Linguistics\nand Literature, 6(1):24\u201346.\nTemitope Olumuyiwa. 2009. The high tone syllable in"
      },
      {
        "source_id": "source_19",
        "citation": "Bridging Gaps in Natural Language Processing for Yoru\u0300ba\u0301_ A  Systematic Review of a Decade of Progress and Prospects.pdf",
        "text_snippet": "Lagos NWU\nYor` ub\u00b4 a Speech\nCorpus\nSpeech corpus Automatic speech\nrecognition\nMonolingual [16], [53],\n[52], [31]\nMasakhaNER Annotated corpus Named entity recogni-\ntion\nMultilingual [12]\nMasakhaPOS POS dataset Part-of-speech tagging Multilingual [17]\nContinued on next page...\n26"
      },
      {
        "source_id": "source_01",
        "citation": "Voices Unheard_ NLP Resources and Models for Yoru\u0300ba\u0301 Regional Dialects.pdf",
        "text_snippet": "pages 7226\u20137249, Dublin, Ireland. Association for\nComputational Linguistics.\nIdris Akinade, Jesujoba Alabi, David Adelani, Clement\nOdoje, and Dietrich Klakow. 2023. Varepsilon k\u00fa\nmask: Integrating Yor\u00f9b\u00e1 cultural greetings into\nmachine translation. In Proceedings of the First\nWorkshop on Cross-Cultural Considerations in NLP\n(C3NLP), pages 1\u20137, Dubrovnik, Croatia. Associa-\ntion for Computational Linguistics.\nJesujoba O. Alabi, David Ifeoluwa Adelani, Marius\nMosbach, and Dietrich Klakow. 2022. Adapting pre-\ntrained language models to African languages via\nmultilingual adaptive fine-tuning. In Proceedings of\nthe 29th International Conference on Computational\nLinguistics, pages 4336\u20134349, Gyeongju, Republic\nof Korea. International Committee on Computational\nLinguistics.\nMd Mahfuz Ibn Alam, Sina Ahmadi, and Antonios\nAnastasopoulos. 2024. CODET: A benchmark for\ncontrastive dialectal evaluation of machine transla-\ntion. In Findings of the Association for Computa-"
      },
      {
        "source_id": "source_19",
        "citation": "Bridging Gaps in Natural Language Processing for Yoru\u0300ba\u0301_ A  Systematic Review of a Decade of Progress and Prospects.pdf",
        "text_snippet": "in english to yor` ub\u00b4 a machine translation system.Journal of Multidisciplinary Engineering\nScience and Technology, Berlin, Germany , 2(7):1814\u20131819, 2015.\n[33] Ikechukwu I. Ayogu, Adebayo O. Adetunmbi, Bolanle A. Ojokoh, and Samuel A.\nOluwadare. A comparative study of hidden markov model and conditional random fields\non a yorb part-of-speech tagging task. In 2017 International Conference on Computing\nNetworking and Informatics (ICCNI) , pages 1\u20136. IEEE, 2017.\n[34] Orevaoghene Ahia, Anuoluwapo Aremu, Diana Abagyan, Hila Gonen, David Adelani,\nDaud Abolade, Noah A Smith, and Yulia Tsvetkov. Voices unheard: Nlp resources and\nmodels for yor` ub\u00b4 a regional dialects. InProceedings of the 2024 Conference on Empirical\nMethods in Natural Language Processing, pages 4392\u20134409, 2024.\n[35] Kayode Olaleye, Dan Oneat \u00b8\u02d8 a, and Herman Kamper. Yfacc: A yor` ub\u00b4 a speech\u2013image\ndataset for cross-lingual keyword localisation through visual grounding. In 2022 IEEE"
      },
      {
        "source_id": "source_19",
        "citation": "Bridging Gaps in Natural Language Processing for Yoru\u0300ba\u0301_ A  Systematic Review of a Decade of Progress and Prospects.pdf",
        "text_snippet": "sentences on the Mozilla Common Voice platform to crowdsource recordings and validations.\nThe data contributions include 42 hours of in-house recorded speech data by 80 volunteers and\nan additional 6 hours of validated recordings on Mozilla Common Voice. Ultimately, a high-\nfidelity single-speaker Yor` ub\u00b4 a TTS system was evaluated on the curated speech data, including\na baseline word error rate (WER) of 23 .8 for ASR purposes.\nLagos-NWU Yor` ub\u00b4 a Speech Corpus: To promote research voice recognition, a speech\ncorpus including 16 female and 17 male speakers was recorded in Lagos, Nigeria [16]. A total\nof 130 utterances read from brief texts chosen for phonetic coverage were recorded by each\nspeaker. Moreover, recordings were made using a microphone attached to a laptop computer in\na peaceful office setting to ensure quality in the corpus. Ultimately, this corpus will be relevant\nfor various NLP tasks in Yor` ub\u00b4 a, such as TTS, ASR, etc."
      },
      {
        "source_id": "source_19",
        "citation": "Bridging Gaps in Natural Language Processing for Yoru\u0300ba\u0301_ A  Systematic Review of a Decade of Progress and Prospects.pdf",
        "text_snippet": "a peaceful office setting to ensure quality in the corpus. Ultimately, this corpus will be relevant\nfor various NLP tasks in Yor` ub\u00b4 a, such as TTS, ASR, etc.\nMasakhaNER: MaskhaNER [12] boasts as the first largest high-quality dataset for named\nentity recognition tasks focused on African languages, in which Yor` ub\u00b4 a was greatly empha-\nsised. The dataset scope extends to 10 African languages and across four language families,\nand they include Yor` ub\u00b4 a, Amharic, Hausa, Igbo, Kinyarwanda, Wolof, Nigerian-Pidgin, Luo,\n23"
      },
      {
        "source_id": "source_19",
        "citation": "Bridging Gaps in Natural Language Processing for Yoru\u0300ba\u0301_ A  Systematic Review of a Decade of Progress and Prospects.pdf",
        "text_snippet": "method for processing the yoruba language using the data context approach based on text,\nlexicon, and grammar. The Pacific Journal of Science and Technology , 2018.\n[117] Abayomi O Agbeyangi, Omolayo Abegunde, and Safiriyu I Eludiora. Authorship verifi-\ncation of yoruba blog posts using character n-grams. In 2020 International Conference\nin Mathematics, Computer Engineering and Computer Science (ICMCECS) , pages 1\u20136.\nIEEE, 2020.\n[118] Ife Adebara, Abdelrahim Elmadany, Muhammad Abdul-Mageed, and Alcides Alcoba\nInciarte. Serengeti: Massively multilingual language models for africa. In Findings of the\nAssociation for Computational Linguistics: ACL 2023 , pages 1498\u20131537, 2023.\n[119] Sebastian Gehrmann, Sebastian Ruder, Vitaly Nikolaev, Jan Botha, Michael Chavinda,\nAnkur Parikh, and Clara Rivera. Tata: A multilingual table-to-text dataset for african\nlanguages. In Findings of the Association for Computational Linguistics: EMNLP 2023 ,\npages 1719\u20131740, 2023."
      },
      {
        "source_id": "source_01",
        "citation": "Voices Unheard_ NLP Resources and Models for Yoru\u0300ba\u0301 Regional Dialects.pdf",
        "text_snippet": "Md Mahfuz Ibn Alam, Sina Ahmadi, and Antonios\nAnastasopoulos. 2024. CODET: A benchmark for\ncontrastive dialectal evaluation of machine transla-\ntion. In Findings of the Association for Computa-\ntional Linguistics: EACL 2024 , pages 1790\u20131859,\nSt. Julian\u2019s, Malta. Association for Computational\nLinguistics.\nAnuoluwapo Aremu, Jesujoba O. Alabi, and David Ife-\noluwa Adelani. 2023. Yorc: Yoruba reading compre-\nhension dataset.\nBolanle Elizabeth Arokoyo, Olamide Oluwaseun La-\ngunju, et al. 2019. A lexicostatistics comparison\nof standard yor\u00f9b\u00e1, \u00e0k\u00far\u00e9 and \u00eck\u00e1r\u00e9 \u00e0k\u00f3k\u00f3 dialects.\nJournal of Universal Language, 20(2):1\u201327.\nAlexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,\nand Michael Auli. 2020. wav2vec 2.0: A framework\nfor self-supervised learning of speech representations.\nIn Advances in Neural Information Processing Sys-\ntems, volume 33, pages 12449\u201312460. Curran Asso-\nciates, Inc.\nJohn A Ballard. 1971. Historical inferences from the lin-\nguistic geography of the nigerian middle belt. Africa,"
      }
    ],
    "time_taken": 5.02
  },
  {
    "question": "Does the 'WAXAL' paper discuss speech synthesis for Martian languages?",
    "answer": "Insufficient Evidence",
    "citations_readable": [
      "WAXAL_ A Large-Scale Multilingual African Language Speech Corpus.pdf"
    ],
    "citations_raw": [
      "source_25"
    ],
    "retrieved_chunks": [
      {
        "source_id": "source_25",
        "citation": "WAXAL_ A Large-Scale Multilingual African Language Speech Corpus.pdf",
        "text_snippet": "Avinatan Hassidim1, Jeff Dean1, Yossi Matias1\n1Google Research, 2University of Ghana, 3Digital Umuganda, 4University of Addis Ababa,\n5Makerere University, 6Loud and Clear Communications Ltd, 7Media Trust Limited, 8Gates Foundation\nABSTRACT\nThe advancement of speech technology has predomi-\nnantly favored high-resource languages, creating a significant\ndigital divide for speakers of most Sub-Saharan African lan-\nguages. To address this gap, we introduce W AXAL, a large-\nscale, openly accessible speech dataset for 21 languages rep-\nresenting over 100 million speakers. The collection consists\nof two main components: an Automated Speech Recogni-\ntion (ASR) dataset containing approximately 1,250 hours of\ntranscribed, natural speech from a diverse range of speakers,\nand a Text-to-Speech (TTS) dataset with over 180 hours of\nhigh-quality, single-speaker recordings reading phonetically\nbalanced scripts. This paper details our methodology for data"
      },
      {
        "source_id": "source_01",
        "citation": "Voices Unheard_ NLP Resources and Models for Yoru\u0300ba\u0301 Regional Dialects.pdf",
        "text_snippet": "Marta R Costa-juss\u00e0, James Cross, Onur \u00c7elebi, Maha\nElbayad, Kenneth Heafield, Kevin Heffernan, Elahe\nKalbassi, Janice Lam, Daniel Licht, Jean Maillard,\net al. 2022. No language left behind: Scaling\nhuman-centered machine translation. arXiv preprint\narXiv:2207.04672.\nKaren Ruth Courtenay. 1969. A generative phonology\nof yor\u00f9b\u00e1.\nMona Diab. 2016. Processing dialectal Arabic: Exploit-\ning variability and similarity to overcome challenges\nand discover opportunities. In Proceedings of the\nThird Workshop on NLP for Similar Languages, Vari-\neties and Dialects (VarDial3), page 42, Osaka, Japan.\nThe COLING 2016 Organizing Committee.\nMona Diab and Nizar Habash. 2012. Arabic dialect\nprocessing tutorial. In Tutorial Abstracts at the Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, Montr\u00e9al, Canada. Association\nfor Computational Linguistics.\nCheikh M. Bamba Dione, David Ifeoluwa Adelani,"
      },
      {
        "source_id": "source_32",
        "citation": "Neural Machine Translation for Low-Resource Languages.pdf",
        "text_snippet": "[16] Laurent Besacier, Etienne Barnard, Alexey Karpov, and Tanja Schultz. 2014. Automatic speech recognition for\nunder-resourced languages: A survey. Speech Communication 56 (2014), 85\u2013100.\n[17] Alexandra Birch, Andrew Finch, Minh-Thang Luong, Graham Neubig, and Yusuke Oda. 2018. Findings of the Second\nWorkshop on Neural Machine Translation and Generation. In Proceedings of the 2nd Workshop on Neural Machine\nTranslation and Generation. 1\u201310.\n[18] Graeme Blackwood, Miguel Ballesteros, and Todd Ward. 2018. Multilingual Neural Machine Translation with\nTask-Specific Attention. In Proceedings of the 27th International Conference on Computational Linguistics . 3112\u20133122.\n[19] Su Lin Blodgett, Solon Barocas, Hal Daum\u00e9 III, and Hanna Wallach. 2020. Language (Technology) is Power: A Critical\nSurvey of \u201cBias\u201d in NLP. InProceedings of the 58th Annual Meeting of the ACL . 5454\u20135476."
      },
      {
        "source_id": "source_14",
        "citation": "Localising SA official languages.pdf",
        "text_snippet": "speech feedback.\nSubsequent to the AST project, three Lwazi[/five.osf]\nprojects were funded by the South African Depart-\nment of Arts and Culture with the aim of extending\nthe available telephone speech data sets to include\nall /one.osf/one.osf o\ufb03cial languages and to increase the impact\nof speech technologies in South Africa (Barnard\net al. /two.osf/zero.osf/one.osf/zero.osf, Kuun /two.osf/zero.osf/one.osf/two.osf, Calteaux et al. /two.osf/zero.osf/one.osf/three.osf, Titmus\net al. /two.osf/zero.osf/one.osf/six.osf). T oward the latter aim, text-to-speech\nand speech-to-text systems were developed in all /one.osf/one.osf\nlanguages and evaluated in applications including\na voice-based telephone service for rural veterinar-\nians and a multilingual, telephone-based interac-\ntive voice response system for the Department of\nBasic Education\u2019s National School Nutrition Pro-\ngramme.\nisiZulu was included in the data sets that were col-\nlected to support IARPA/DARPA \u2019s[/six.osf]Babel and"
      },
      {
        "source_id": "source_08",
        "citation": "IrokoBench_ A New Benchmark for African Languages in the Age of Large  Language Models.pdf",
        "text_snippet": "and the 11th International Joint Conference on Natu-\nral Language Processing (Volume 1: Long Papers),\npages 3816\u20133830, Online. Association for Computa-\ntional Linguistics.\nGemini-Team, Machel Reid, Nikolay Savinov, Denis\nTeplyashin, Dmitry, Lepikhin, Timothy Lillicrap,\nJean baptiste Alayrac, Radu Soricut, Angeliki Lazari-\ndou, Orhan Firat, and et al. 2024. Gemini 1.5: Un-\nlocking multimodal understanding across millions of\ntokens of context. Preprint, arXiv:2403.05530.\nNaman Goyal, Cynthia Gao, Vishrav Chaudhary, Peng-\nJen Chen, Guillaume Wenzek, Da Ju, Sanjana Kr-\nishnan, Marc\u2019Aurelio Ranzato, Francisco Guzm\u00e1n,\nand Angela Fan. 2022. The Flores-101 evaluation\nbenchmark for low-resource and multilingual ma-\nchine translation. Transactions of the Association for\nComputational Linguistics, 10:522\u2013538.\nKai Hartung, Aaricia Herygers, Shubham Kurlekar,\nKhabbab Zakaria, Taylan V olkan, S\u00f6ren Gr\u00f6ttrup,\nand Munir Georges. 2023. Measuring sentiment bias"
      },
      {
        "source_id": "source_34",
        "citation": "Understanding the Capabilities and Limitations of Large Language Models for Cultural Commonsense.pdf",
        "text_snippet": "omlama: Geo-diverse commonsense probing on\nmultilingual pre-trained language models. In\nConference on Empirical Methods in Natural\nLanguage Processing.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali\nFarhadi, and Yejin Choi. 2019. Hellaswag: Can\na machine really finish your sentence? InAnnual\nMeeting of the Association for Computational\nLinguistics."
      },
      {
        "source_id": "source_13",
        "citation": "No Language Left Behind_ Scaling Human-Centered Machine Translation.pdf",
        "text_snippet": "Benjamin Marie, Raphael Rubino, and Atsushi Fujita. Tagged back-translation revis-\nited: Why does it really work? In Proceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics , pages 5990\u20135997, Online, July 2020. As-\nsociation for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.532. URL\nhttps://aclanthology.org/2020.acl-main.532.\nArya D. McCarthy, Rachel Wicks, Dylan Lewis, Aaron Mueller, Winston Wu, Oliver\nAdams, Garrett Nicolai, Matt Post, and David Yarowsky. The Johns Hopkins University\nBible corpus: 1600+ tongues for typological exploration. In Proceedings of the 12th\nLanguage Resources and Evaluation Conference , pages 2884\u20132892, Marseille, France,\nMay 2020. European Language Resources Association. ISBN 979-10-95546-34-4. URL\nhttps://aclanthology.org/2020.lrec-1.352.\nLeland McInnes, John Healy, Nathaniel Saul, and Lukas Grossberger. Umap: Uniform\nmanifold approximation and projection. The Journal of Open Source Software , 3(29):861,\n2018."
      },
      {
        "source_id": "source_25",
        "citation": "WAXAL_ A Large-Scale Multilingual African Language Speech Corpus.pdf",
        "text_snippet": "Jinpeng Li, Bo Yang, Yexing Du, Ziyang Ma, Xunying\nLiu, Ziyuan Wang, et al., \u201cGigaspeech 2: An evolv-\ning, large-scale and multi-domain asr corpus for low-\nresource languages with automated crawling, transcrip-\ntion and refinement,\u201darXiv preprint arXiv:2406.11546,\n2024.\n[8] Moussa Doumbouya, Lisa Einstein, and Chris Piech,\n\u201cUsing radio archives for low-resource speech recogni-\ntion: Towards an intelligent virtual assistant for illiterate\nusers,\u201d inProceedings of the AAAI Conference on Arti-\nficial Intelligence, 2021, vol. 35.\n[9] Elodie Gauthier, Laurent Besacier, Sylvie V oisin,\nMichael Melese, and Uriel Pascal Elingui, \u201cCollect-\ning resources in sub-saharan african languages for auto-\nmatic speech recognition: a case study of wolof,\u201d in10th\nLanguage Resources and Evaluation Conference (LREC\n2016), 2016.\n[10] Daniel van Niekerk, Charl van Heerden, Marelie Davel,\nNeil Kleynhans, Oddur Kjartansson, Martin Jansche,\nand Linne Ha, \u201cRapid development of TTS corpora"
      },
      {
        "source_id": "source_06",
        "citation": "CultureVLM_ Characterizing and Improving Cultural Understanding of Vision-Language Models for over 100 Countries.pdf",
        "text_snippet": "Bahree, Arash Bakhtiari, Harkirat Behl, et al. Phi-3 technical report: A highly capable language model locally on your phone.\narXiv:2404.14219, 2024.\nMuhammad Farid Adilazuarda, Sagnik Mukherjee, Pradhyumna Lavania, Siddhant Singh, Ashutosh Dwivedi, Alham Fikri Aji, Jacki\nO\u2019Neill, Ashutosh Modi, and Monojit Choudhury. Towards measuring and modeling\" culture\" in llms: A survey.arXiv:2403.15412,\n2024.\nPravesh Agrawal, Szymon Antoniak, Emma Bou Hanna, Baptiste Bout, Devendra Chaplot, Jessica Chudnovsky, Diogo Costa,\nBaudouin De Monicault, Saurabh Garg, Theophile Gervet, et al. Pixtral 12b. arXiv:2410.07073, 2024.\nAnthropic. Claude 3.5 sonnet, 2024.\nArjun Appadurai. Modernity at large: Cultural dimensions of globalization. U of Minnesota P, 1996.\n10"
      },
      {
        "source_id": "source_06",
        "citation": "CultureVLM_ Characterizing and Improving Cultural Understanding of Vision-Language Models for over 100 Countries.pdf",
        "text_snippet": "community-driven knowledge base towards culturally aware language technologies. arXiv:2404.15238, 2024.\nAmanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards\nvqa models that can read. In CVPR, pages 8317\u20138326, 2019.\nWorld Values Survey. World values survey. https://www.worldvaluessurvey.org/wvs.jsp, 2022.\nGemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk,\nAndrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal models. arXiv:2312.11805, 2023.\nSM Tonmoy, SM Zaman, Vinija Jain, Anku Rani, Vipula Rawte, Aman Chadha, and Amitava Das. A comprehensive survey of\nhallucination mitigation techniques in large language models. arXiv:2401.01313, 2024.\nPeng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al."
      },
      {
        "source_id": "source_05",
        "citation": "Exploring NLP Benchmarks in an Extremely Low-Resource Setting.pdf",
        "text_snippet": "els. Beyond translation, we introduce the \ufb01rst com-\nprehensive Ladin benchmark dataset covering ma-\nchine translation, sentiment analysis, and multiple-\nchoice question answering, all derived through syn-\nthetic augmentation from monolingual Italian re-\nsources.\nOur evaluation shows that while synthetic data\nenables competitive performance across tasks fur-\nther progress is needed to enhance knowledge trans-\nfer from high-resource to low-resource languages.\nThis study lays foundational work for future NLP\nresearch on Ladin and similarly underrepresented\nlanguages.\nFuture work will investigate knowledge distilla-\ntion techniques to more effectively transfer knowl-\nedge from high-resource languages to low-resource\nones, as well as explore advanced methods to better\nexploit available monolingual Ladin data for im-\nproved model performance. We are also committed\nto further foster developments of Ladin resources\nand to engaging community in actively researching"
      },
      {
        "source_id": "source_19",
        "citation": "Bridging Gaps in Natural Language Processing for Yoru\u0300ba\u0301_ A  Systematic Review of a Decade of Progress and Prospects.pdf",
        "text_snippet": "open-source multilingual text-to-speech (TTS) synthesis platform, which achieved a Mean Opin-\nion Score (MOS) of 2 .9 out of 5 for the quality of the synthesized speech.\nWebCrawl African: WebCrawl African [112] contains multilingual parallel corpora for di-\nverse African languages. It was gathered to build resources for machine translation tasks\nin low-resource and extremely low-resource African languages. The parallel corpus spans 74\nlanguage pairs, which includes 15 African languages, of which Yor` ub\u00b4 a language was greatly\nemphasised. For empirical analysis purposes, two multilingual models were trained on behalf\nof 24 African languages, including Yor` ub\u00b4 a language, using the dataset and evaluated using\nFLORES-200 [124] benchmarks. The dataset will be useful for multilingual and cross-lingual\nmachine translation tasks involving Yor` ub\u00b4 a language.\nW\u00b4UR`A: W \u00b4UR`A [76] presents a new high-quality multilingual pre-training corpus for African"
      }
    ],
    "time_taken": 6.13
  },
  {
    "question": "Find evidence for the claim that 'LLMs are perfect translators'.",
    "answer": "Insufficient Evidence",
    "citations_readable": [],
    "citations_raw": [],
    "retrieved_chunks": [
      {
        "source_id": "source_26",
        "citation": "Global MMLU_ Understanding and Addressing Cultural and Linguistic Biases  in Multilingual Evaluation.pdf",
        "text_snippet": "LLM instruction tuning and evaluation. Their findings reveal that models fine-tuned with native\ninstructions typically outperform those trained using translated data. Similarly, Choenni et al.\n(2024) investigates the reliability of machine translation as a substitute for human translation in\n8An acronym forSouthEast Asian Holistic Evaluation of Language Models.\n9https://leaderboard.sea-lion.ai\n22"
      },
      {
        "source_id": "source_08",
        "citation": "IrokoBench_ A New Benchmark for African Languages in the Age of Large  Language Models.pdf",
        "text_snippet": "LLaMa 3.1 70B. These findings suggest that\nmore efforts are needed to develop and adapt\nLLMs for African languages.\n1 Introduction\nIn recent years, the capabilities of large language\nmodels (LLMs) have greatly improved, from co-\nherent chat experiences to solving complex and\nknowledge-intensive tasks like mathematical rea-\nsoning, coding, and question answering (QA) (Ope-\nnAI et al., 2024; Jiang et al., 2024; Gemini-Team\net al., 2024). These models have also demonstrated\nthe ability to quickly learn new and challenging\ntasks with few in-context learning examples and\nthrough chain-of-thought reasoning (Brown et al.,\n2020; Shi et al., 2022; Wei et al., 2022). However,\nmost state-of-the-art (SoTA) LLMs are primarily\ntrained on high-resource languages (HRLs), result-\ning in sub-optimal performance for languages un-\nseen during pre-training (Touvron et al., 2023; Ojo\net al., 2023). Furthermore, this language coverage\nbias is reflected in the evaluation stage, predomi-"
      },
      {
        "source_id": "source_12",
        "citation": "A Survey on Large Language Models with Multilingualism_ Recent Advances and New Frontiers.pdf",
        "text_snippet": "The bias of LLMs in the multilingual scenario can be divided into two categories: language bias and\ndemographic bias [28]. Intuitively, the former is due to the imbalance of available training corpus\nfor different languages [43, 457], where English possesses the most text corpus [462, 474, 475,\n476], resulting in the performance degradation of the LLMs when generalized to other language\nsettings [477, 478]. The demographic bias occurs due to embedded biases and misinformation\n25"
      },
      {
        "source_id": "source_12",
        "citation": "A Survey on Large Language Models with Multilingualism_ Recent Advances and New Frontiers.pdf",
        "text_snippet": "among various languages and integrating language-specific domain knowledge into LLMs. For ex-\nample, disparities in legal definitions between European Council and USA jurisdictions, as well as\nthe contrast between traditional Chinese medicine and Western medicine, indicate these challenges.\nCurrent LLMs face challenges in effectively understanding such language-specific knowledge, hin-\ndering their capacity to provide customized domain-specific assistance for diverse populations. Fur-\nther research is necessary to explore how LLMs can integrate and leverage this particular type of\nknowledge.\nThese limitations highlight the need for further research efforts in the following directions:\n\u2022 Development of strategies to construct high-quality, domain-specific multilingual datasets\nthat preserve cultural context.\n\u2022 Exploration of techniques for LLMs to effectively integrate and leverage language-specific\nin-domain knowledge."
      },
      {
        "source_id": "source_08",
        "citation": "IrokoBench_ A New Benchmark for African Languages in the Age of Large  Language Models.pdf",
        "text_snippet": "all accuracy, where 13 out of 16 African lan-\nguages achieved at least 70% despite struggling\nwith AfriMGSM. The performance difference to\nAfriMGSM may be due to the multi-choice options\nof MMLU, which may be slightly easier than free-\nform answer. International law also achieved im-\npressive performance with 10 out of 16 languages\nachieving 70%. All languages struggle the most\nwith Global facts including English and French.\nSimilarly, many African languages find it difficult\nto answer questions in geography and microeco-\nnomics subjects. This presents an opportunity for\nimproving LLMs for the education domain.\nAfriMGSM performance receives significant\nboost with translate-test for open models Ta-\nble 5 shows that we can achieve a significant boost\nin performance with the translate-test on all lan-\nguages we evaluated. We hypothesize that the cur-\nrent LLMs are better at reasoning in English than\n7"
      },
      {
        "source_id": "source_09",
        "citation": "The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks.pdf",
        "text_snippet": "The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks\nbegun addressing these issues by incorporating more diverse cultural perspectives [Li et al., 2024, Son\net al., 2024, Zhao et al., 2024a, Chiu et al., 2024], but there remains significant room for benchmarks\nthat assess models on their ability to handle locally meaningful applications.\nLLM-as-a-Judge Recent research has demonstrated the potential of using LLMs themselves as\nevaluation tools for assessing the quality of model-generated text in English [Zheng et al., 2023,\nDubois et al., 2024]. This approach offers promising opportunities for multilingual evaluation by\nextending these techniques across diverse languages and tasks. However, deploying LLMs as judges\nin multilingual contexts also introduces unique challenges, including potential evaluation biases that\nmirror the language disparities in the judge models themselves and questions about their cross-lingual"
      },
      {
        "source_id": "source_12",
        "citation": "A Survey on Large Language Models with Multilingualism_ Recent Advances and New Frontiers.pdf",
        "text_snippet": "in most languages can be used to mislead LLMs into producing prohibited content.\n6.3.2 Robustness of LLMs with Multilingualism\nExisting studies intend to attack LLMs to find some leaks, rather than to improve the defense ability\nand robustness of LLMs. However, an attack or jailbreak cannot ensure the development of LLM\nsecurity. In this section, we will discuss the potential solutions for improving the security and\nrobustness of LLMs.\n19"
      },
      {
        "source_id": "source_12",
        "citation": "A Survey on Large Language Models with Multilingualism_ Recent Advances and New Frontiers.pdf",
        "text_snippet": "As LLMs are deeply integrated into various applications that need context comprehension and gen-\neration in multilingual scenarios, the requirement for LLMs capable of effectively and efficiently\nunderstanding and generating sequences in a language-agnostic manner becomes increasingly in-\ndispensable and urgent. Consequently, researchers have devoted significant efforts to enhancing the\npracticality of LLMs in multilingual scenarios from various perspectives, including training pro-\ncedure (Section 3), optimization on the inference (Section 4), information retrieval systems (Sec-\ntion 5), security in multilingual situations (Section 6) and multi-domain (Section 7). Furthermore,\ndue to the paradigm shift from \u201cPre-train, Fine-tune\u201d to \u201cPre-train, Prompt, Predict\u201d in multilingual\nscenarios (Section 2), there has been a subtle change in the definition of multilingualism, leading to\nchallenges in the development of the multilingual community. A series of multilingual approaches"
      },
      {
        "source_id": "source_12",
        "citation": "A Survey on Large Language Models with Multilingualism_ Recent Advances and New Frontiers.pdf",
        "text_snippet": "necessarily true for most languages. As a result, the translation errors would accumulate and the\nfinal output of LLMs would become wrong. Besides, translation on a pivot language obscures the\ncultural and linguistic nuances of the original text, which might also lead to inaccurate results. As\nan exploratory work, Liu et al. [64] explore the performance comparison between direct inference\nusing native languages and inference after translating into English for multilingual tasks. Although\ntranslation can enhance the multilingual reasoning performance of English-centric LLMs, for high-\nresource languages and advanced LLMs, reasoning in native languages is more effective. Besides,\npre-translating to English is a practical approach in terms of current LLMs as they are predominantly\ntrained on English data from their pilot experimental results. However, with the development of\ndirect multilingual inference, the intermediary step may not be necessary, which could allow more"
      },
      {
        "source_id": "source_09",
        "citation": "The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks.pdf",
        "text_snippet": "have collected human exam questions from specific regions in the target language to assess LLM\nperformance for local users [Koto et al., 2023, Li et al., 2024, Y\u00fcksel et al., 2024]. In addition to\nthese methods, recent studies have emphasized the inclusion of culturally specific content in their\nevaluations. For instance, Chiu et al. [2024] introduced CulturalBench, which assesses LLMs\u2019 cultural\nknowledge across 45 global regions. Similarly, Mohamed et al. [2024] presented ArtELingo-28, a\nvision-language benchmark spanning 28 languages, with a focus on capturing diverse opinions across\nlanguages and cultures. Another notable example is CVQA, introduced by Romero et al. [2024], which\nincludes culturally driven images and questions from 30 countries across four continents, covering 31\nlanguages and 13 scripts, with a total of 10K questions.\nOurs In this position paper, we conduct a comprehensive investigation of over 2,000 studies on"
      },
      {
        "source_id": "source_23",
        "citation": "Jawaher_ A Multidialectal Dataset of Arabic Proverbs for LLM  Benchmarking.pdf",
        "text_snippet": "consistently surpassing that of other languages.\nThis reveals a persistent cultural gap in LLMs,\nwhich complicates their ability to accurately pro-\ncess culturally rich and diverse figurative lan-\nguage, such as proverbs. To address this, we in-\ntroduce Jawaher, a benchmark designed to assess\nLLMs\u2019 capacity to comprehend and interpret Ara-\nbic proverbs. Jawaher includes proverbs from\nvarious Arabic dialects, along with idiomatic\ntranslations and explanations. Through exten-\nsive evaluations of both open- and closed-source\nmodels, we find that while LLMs can generate\nidiomatically accurate translations, they struggle\nwith producing culturally nuanced and contextu-\nally relevant explanations. These findings high-\nlight the need for ongoing model refinement and\ndataset expansion to bridge the cultural gap in\nfigurative language processing. Project GitHub\npage is accessible at: https://github.com/UBC-\nNLP/jawaher.\n1 Introduction\nInstruction fine-tuning (Chung et al., 2024) has sig-"
      },
      {
        "source_id": "source_07",
        "citation": "LLMs Are Few-Shot In-Context Low-Resource Language Learners.pdf",
        "text_snippet": "across 3 text classification tasks using 44 different\ncross-lingual pairs. Their findings emphasize the\nlimitations of random exemplar selection and pro-\npose the use of semantic-based exemplar retrieval\nand label alignment 1 for superior X-ICL perfor-\nmance. Notably, their findings diverge from our\nresults (\u00a75.1), which contend that label alignment\ndoes not provide benefits for X-ICL.\n2.2 LLMs on Low-Resource Languages\nRigorous evaluations have been proposed to in-\nvestigate how LLMs perform on low-resource lan-\nguages. According to (Cahyawijaya et al., 2023a),\nwhile multilingual LLMs typically exhibit pos-\nitive transfer learning among related languages,\nthese models perform notably better for mid- and\nhigh-resource (e.g., Indonesian and English) com-\npared to low-resource languages (e.g., other 18\nIndonesian indigenous languages). This implies a\nchallenge in the generalization capability of exist-\ning multilingual LLMs to low-resource languages."
      }
    ],
    "time_taken": 5.55
  },
  {
    "question": "What is the specific learning rate used in the 'DeepSeek-V3' paper?",
    "answer": "Insufficient Evidence",
    "citations_readable": [],
    "citations_raw": [],
    "retrieved_chunks": [
      {
        "source_id": "source_06",
        "citation": "CultureVLM_ Characterizing and Improving Cultural Understanding of Vision-Language Models for over 100 Countries.pdf",
        "text_snippet": "capacity. For LLaV A-1.5, a learning rate of2 \u00d7 10\u22125 is used, with no weight decay applied (0.0). The learning rate\nfollowed a cosine schedule, gradually increasing during the initial phase with a warmup ratio of 0.03. For Phi-3, we use\na learning rate of 4 \u00d7 10\u22125 and a weight decay of 0.01. A linear learning rate scheduler is utilized, with 50 warmup\nsteps to stabilize the early training stage. For LLaMA-3.2, fine-tuning is conducted using a learning rate of 1 \u00d7 10\u22125\nwith no weight decay (0.0). A multiplicative learning rate decay is applied after each epoch, with a gamma value of0.85.\nThe batch sizes are set to 64, 16 and 32 respectively. All models are trained for one epoch on the training set and fully\nfine-tuned on 4\u00d7A100 80GB GPUs. For the training data, although we do not conduct large-scale human annotation, we\nsynthesize the data using only concepts that passed either GPT-4o or human quality assurance, significantly improving"
      },
      {
        "source_id": "source_31",
        "citation": "A Survey on Multilingual Large Language Models_ Corpora, Alignment, and  Bias.pdf",
        "text_snippet": "Harley T, Silver D, Kavukcuoglu K. Asynchronous\nmethods for deep reinforcement learning. In: Pro-\nceedings of the 33rd International Conference on In-\nternational Conference on Machine Learning. 2016,\n1928\u20131937\n67. French R M. Catastrophic forgetting in connectionist\nnetworks. Trends in Cognitive Sciences, 1999, 3(4):\n128\u2013135\n68. Hedderich M A, Lange L, Adel H, Str\u00f6tgen J, Klakow\nD. A survey on recent approaches for natural language\nprocessing in low-resource scenarios. In: Proceedings\nof the 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics.\n2021, 2545\u20132568\n69. Alabi J O, Adelani D I, Mosbach M, Klakow D.\nAdapting pre-trained language models to african lan-\nguages via multilingual adaptive fine-tuning. In:\nProceedings of the 29th International Conference on\nComputational Linguistics. 2022, 4336\u20134349\n70. Wongso W, Lucky H, Suhartono D. Pre-trained\ntransformer-based language models for sundanese.\nJournal of Big Data, 2022, 9(1): 39"
      },
      {
        "source_id": "source_13",
        "citation": "No Language Left Behind_ Scaling Human-Centered Machine Translation.pdf",
        "text_snippet": "root learning rate schedule. For Top-2-Gating, we set the expert capacity to 2 \u00d7 T /E, i.e.,\nwe enforce that each expert processes, at most, 2 \u00d7 T /E tokens, where T is the number of\ntokens in the mini-batch and E is the number of experts. During generation, we set the\ncapacity to T so that all tokens can be routed to whichever expert they choose. We use the\nchrF+ + metric to compare the model performance (see Section 7.1).\nResults. In Table 15, we see that the Sparsely Gated MoE model with 64 experts (MoE-64),\nwhile computationally similar, shows good improvements when compared to the dense 615M\nmodel. We see 1+ chrF + + score improvements on all subsets except for very low resource\npairs (v.low) and non-English pairs ( xx-yy). There are worse trends when scaling up the\ncomputational cost per update \u2014 for the MoE-64 model (computationally similar to the\ndense 1.3B model), we see neutral or worse performance compared to the dense 1.3B model."
      },
      {
        "source_id": "source_06",
        "citation": "CultureVLM_ Characterizing and Improving Cultural Understanding of Vision-Language Models for over 100 Countries.pdf",
        "text_snippet": "Phi-3-vision 4B 87.53 90.84 95.77 90.80 91.56 91.41 90.91 91.16 92.83 90.50 92.35\nLLaMA-3.2-Vision 11B89.13 91.49 96.20 91.99 92.24 91.82 93.08 91.34 93.20 91.78 92.50\nTable 4: Performance of fine-tuned models across three tasks, five continents, and three categories. CHT denotes\nCultural Heritage and Traditions; HL denotes History and Landmarks; and NELR denotes Natural Environment and\nLocal Resources.\nGreedy Search 0.01 0.1 0.5 1.0\nTemperature\n40\n60\n80Performance (%)\nLLaVA\nLLaMA\nCultureVLM-LLaVA\nCultureVLM-LLaMA\nFigure 9: Impact of different decoding temperatures on performance\nC Details of Human Annotation\nC.1 Statistics of Human Annotators and the Process\nTable 8 shows the statistics of human annotators in our study. In total, through the contractor company, we hired10\nexpert annotators whose ages are between 20 and 36 with at least a bachelor\u2019s degree. Most of them are within the"
      },
      {
        "source_id": "source_09",
        "citation": "The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks.pdf",
        "text_snippet": "machine translation. CoRR, abs/2207.04672, 2022. doi: 10.48550/ARXIV.2207.04672. URL\nhttps://doi.org/10.48550/arXiv.2207.04672.\nDeepSeek-AI, D. Guo, D. Yang, H. Zhang, J. Song, R. Zhang, R. Xu, Q. Zhu, S. Ma, P. Wang, X. Bi,\nX. Zhang, X. Yu, Y. Wu, Z. F. Wu, Z. Gou, Z. Shao, Z. Li, Z. Gao, A. Liu, B. Xue, B. Wang, B. Wu,\nB. Feng, C. Lu, C. Zhao, C. Deng, C. Zhang, C. Ruan, D. Dai, D. Chen, D. Ji, E. Li, F. Lin, F. Dai, F. Luo,\nG. Hao, G. Chen, G. Li, H. Zhang, H. Bao, H. Xu, H. Wang, H. Ding, H. Xin, H. Gao, H. Qu, H. Li,\nJ. Guo, J. Li, J. Wang, J. Chen, J. Yuan, J. Qiu, J. Li, J. L. Cai, J. Ni, J. Liang, J. Chen, K. Dong, K. Hu,\nK. Gao, K. Guan, K. Huang, K. Yu, L. Wang, L. Zhang, L. Zhao, L. Wang, L. Zhang, L. Xu, L. Xia,\nM. Zhang, M. Zhang, M. Tang, M. Li, M. Wang, M. Li, N. Tian, P. Huang, P. Zhang, Q. Wang, Q. Chen,\nQ. Du, R. Ge, R. Zhang, R. Pan, R. Wang, R. J. Chen, R. L. Jin, R. Chen, S. Lu, S. Zhou, S. Chen, S. Ye,"
      },
      {
        "source_id": "source_01",
        "citation": "Voices Unheard_ NLP Resources and Models for Yoru\u0300ba\u0301 Regional Dialects.pdf",
        "text_snippet": "trained for four epochs, and kept the model with\nthe best eval loss. We used a weight decay of 0.01,\nwarmup ratio 0.1, and a cosine annealing scheduler\nfor learning rate. While for ASR finetuning, we\nfine-tuned with a learning-rate of 1e-3 and batch\nsize of 8 for 20 epochs, as the validation WER\ncontinued to drop after preliminary runs with 10\nepochs. For S2TT, we fine-tuned for 10 epochs\nwith an optimal learning rate of 3e-4. All training\nwas done on two NVIDIA A40 GPUs.\nModel Standard Ife Ilaje\nZero-Shot 72.50 85.38 83.79\nMMS-300m-Individual 74.67 93.20 78.24\nMMS-1.3bn-Individual 55.43 72.00 61.80\nXLSR-300m-Individual 56.26 81.23 64.22\nXLSR-1.3bn-Individual 67.65 78.70 76.36\nMMS-300m-Joint 58.11 76.58 67.17\nMMS-1.3bn-Joint 55.73 73.95 63.94\nXLSR-300m-Joint 54.55 73.72 61.03\nXLSR-1.3bn-Joint 81.57 90.04 86.30\nTable 8: ASR Performance of across all models after\nfine-tuning individually and jointly\nA.3 Results from Joint vs Individual MT\nfine-tuning"
      },
      {
        "source_id": "source_04",
        "citation": "Cheetah_ Natural Language Generation for 517 African Languages.pdf",
        "text_snippet": "layers each with 12 attention heads, and 768 hidden\nunits for the base model. In total, this results in a\nmodel with \u223c 580 million parameters. We provide\nfurther details in Table 3.\nObjective. We use an unsupervised (denoising)\nobjective. The main idea is to feed the model with\nmasked (corrupted) versions of the original sen-\ntence, and train it to reconstruct the original se-\nquence. The denoising objective (Xue et al., 2021)\nworks by randomly sampling and dropping out 15%\nof tokens in the input sequence. All consecutive\nspans of dropped-out tokens are then replaced by a\nsingle sentinel token.\nPretraining Procedure For pretraining Cheetah,\nwe use a learning rate of0.01, a batch size of1, 024\nsequences, and a maximum sequence length of\n1, 024. We pretrain each model for 1M steps. We"
      },
      {
        "source_id": "source_13",
        "citation": "No Language Left Behind_ Scaling Human-Centered Machine Translation.pdf",
        "text_snippet": "quality estimation. In Proceedings of the Sixth Conference on Machine Translation ,\npages 684\u2013725, Online, November 2021. Association for Computational Linguistics. URL\nhttps://aclanthology.org/2021.wmt-1.71.\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\nnov. Dropout: a simple way to prevent neural networks from overfitting. The journal of\nmachine learning research , 15(1):1929\u20131958, 2014.\nHaipeng Sun, Rui Wang, Kehai Chen, Masao Utiyama, Eiichiro Sumita, and Tiejun Zhao.\nKnowledge distillation for multilingual unsupervised neural machine translation. In\nProceedings of the 58th Annual Meeting of the Association for Computational Linguistics ,\npages 3525\u20133535, 2020.\nCass Robert Sunstein and Richard Thaler. Libertarian paternalism. American Economic\nReview, 2003.\nChristian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015."
      },
      {
        "source_id": "source_11",
        "citation": "Language model pretraining for low resource languages.pdf",
        "text_snippet": "2007). We use fastBPE2to learn BPE (Byte pair\nencoding) (Bojanowski et al.,2017) with 32k BPE\ncodes over the combined tokenized data of both lan-\nguages. For de$dsb, we use the same vocabulary\nand codes learnt for de$hsb.\n4.2 Experimental Setup\nWe use 6 layers in the encoder and decoder with 8\nattention heads and 1024 embedding dimension.\nWe use Adam (Kingma and Ba,2015) optimizer.\nWe use, a warm-up phase of 4000 steps with initial\nlearning rate starting from1e\u00007to1e\u00004, in the\nwarm-up phase learning rate is increased linearly\nand then starts to decrease with inverse square\nroot learning rate schedule. We use mini-batches\nof size 2000 tokens and set the dropout to\n0.1 (Gal and Ghahramani,2016). Maximum\nsentence length is set to 100 after applying BPE.\nAt the time of decoding, we set beam size to\n1. For experiments, we are using MASS3codebase.\n1http://statmt.org/wmt21/\ntranslation-task.html2https://github.com/glample/fastBPE3https://github.com/microsoft/MASS"
      },
      {
        "source_id": "source_05",
        "citation": "Exploring NLP Benchmarks in an Extremely Low-Resource Setting.pdf",
        "text_snippet": "ing the OpenAI2API. For the FT approach using\nLLaMA, we leverage the Together AI3\ufb01ne-tuning\nAPI with the LoRA adapter con\ufb01guration (r= 32),\nemploying a batch size of 8 and 3 epochs.\nFor the seq2seq models, we employ MBART-\nlarge-50 and NLLB-200-1.3B, both of which\nare encoder-decoder architectures speci\ufb01cally de-\nsigned for multilingual MT. The \ufb01ne-tuning mod-\n2https://deepinfra.com/3https://www.together.ai/\n4"
      },
      {
        "source_id": "source_10",
        "citation": "Targeted Multilingual Adaptation for Low-resource Language Families.pdf",
        "text_snippet": "400k steps, vocab size 16k, and \u03b1 = 0.1.\n4 Results\nWe present our results in two main sections. First,\nwe compare our best-performing Uralic-adapted\nmultilingual models to both multilingual and mono-\nlingual baselines. We show that our chosen method\nof layering LAPT and vocabulary specialization\non a pre-trained multilingual model largely outper-\n5"
      },
      {
        "source_id": "source_31",
        "citation": "A Survey on Multilingual Large Language Models_ Corpora, Alignment, and  Bias.pdf",
        "text_snippet": "10 Front. Comput. Sci., 2024, 0(0): 110\u201341\nFig. 3 Diagram illustrating the RLHF procedure, which consists of three key steps: (1) Pre-training a LM using the labeled\nprompt-response dataset, (2) Training a Reward Model based on scores provided by human evaluators for LM\u2019s generation,\nand (3) Fine-tuning with a Reinforcement Learning (RL) algorithm, which helps to update parameters in the LM based on the\nfeedback from RM.\nnotated data. Therefore, MLLMs trained by this\napproach exhibit superior performance on low-\nresource languages compared to the aforemen-\ntioned fine-tuning approach. For example, Torge\net al. [71] pre-trained monolingual RoBERTa mod-\nels for Czech and Polish, as well as a bilingual\nmodel for Czech-Polish, which demonstrated su-\nperior performance to the current state-of-the-art\nmultilingual model, XLM-R, across various down-\nstream tasks. Recently, there has been a grow-\ning interest in developing low-resource language\nmodels to meet the demands of morphologically"
      }
    ],
    "time_taken": 5.42
  },
  {
    "question": "Does the corpus contain the personal email address of the author 'Adebara'?",
    "answer": "Insufficient Evidence",
    "citations_readable": [],
    "citations_raw": [],
    "retrieved_chunks": [
      {
        "source_id": "source_13",
        "citation": "No Language Left Behind_ Scaling Human-Centered Machine Translation.pdf",
        "text_snippet": "https://aclanthology.org/W19-3632.\nAhmed Abdelali, Francisco Guzman, Hassan Sajjad, and Stephan Vogel. The AMARA corpus:\nBuilding parallel language resources for the educational domain. In Proceedings of the\nNinth International Conference on Language Resources and Evaluation (LREC\u201914) , pages\n1856\u20131862, Reykjavik, Iceland, May 2014. European Language Resources Association\n(ELRA). URL http://www.lrec-conf.org/proceedings/lrec2014/pdf/877_Paper.\npdf.\nSadaf Abdul-Rauf and Holger Schwenk. On the Use of Comparable Corpora to Improve SMT\nperformance. In EACL, pages 16\u201323, 2009. URL http://www.aclweb.org/anthology/\nE09-1003.\nDavid Adelani, Dana Ruiter, Jesujoba Alabi, Damilola Adebonojo, Adesina Ayeni, Mofe\nAdeyemi, Ayodele Esther Awokoya, and Cristina Espa\u00f1a-Bonet. The effect of domain\nand diacritics in Yoruba\u2013English neural machine translation. In Proceedings of the\n18th Biennial Machine Translation Summit (Volume 1: Research Track) , pages 61\u201375,"
      },
      {
        "source_id": "source_20",
        "citation": "Prompting Towards Alleviating Code-Switched Data Scarcity in  Under-Resourced Languages with GPT as a Pivot.pdf",
        "text_snippet": "able]: https://www.gov.za/documents/\nprotection-personal-information-act\n[Accessed: 4 May 2023].\nAnkit Srivastava, Vijendra Singh, and Gur-\ndeep Singh Drall. 2019. Sentiment analysis of\ntwitter data.International Journal of Healthcare\nInformation Systems and Informatics, 14:1\u201316.\nEwald Van der Westhuizen and Thomas Niesler.\n2018. A first South African corpus of multilingual\ncode-switched soap opera speech. InProceed-\nings of the Eleventh International Conference\non Language Resources and Evaluation (LREC\n2018),Miyazaki,Japan.EuropeanLanguageRe-\nsources Association (ELRA).\nOndene van Dulm. 2007.The grammar of English-\nAfrikaanscodeswitching . PhDDissertation,Rad-\nboud Universiteit Nijmegen.\nEwald Van Der Westhuizen and Thomas Niesler.\n2017. Synthesising isizulu-english code-switch\nbigramsusingwordembeddings. In Proceedings\nof Interspeech 2017.\nJules White, Quchen Fu, Sam Hays, Michael Sand-\nborn, Carlos Olea, Henry Gilbert, Ashraf El-\nnashar, Jesse Spencer-Smith, and Douglas C."
      },
      {
        "source_id": "source_17",
        "citation": "AfriMTE and AfriCOMET_ Enhancing COMET to Embrace Under-resourced African Languages.pdf",
        "text_snippet": "holders. They contributed to the data and gave\ntheir consent to use this data for the enhancement\nof COMET models for African languages. Upon\nthe data collected, there is no privacy concern since\nthe source of the data is based on Wikipedia general\ndomain.\nAcknowledgments\nDavid Adelani acknowledges the support of Deep-\nMind Academic Fellowship programme. Ricardo\nRei is supported by the European Union\u2019s Hori-\nzon Europe Research and Innovation Actions (UT-\nTER: contract 101070631) and by the Portuguese\nRecovery and Resilience Plan through project\nC645008882-00000055 (Center for Responsible\nAI). Pontus Stenetorp would like to acknowledge\nthe helpful proofing feedback from several viewers\nwhile finalizing the submission. We are grateful to\nProf. Antonios Anastasopoulos from George Ma-\nson University for releasing the \u201cWMT African\u201d\ndataset for our experiments. This work is sup-\nported in part by Oracle Cloud credits and re-\nlated resources provided by Oracle. This work"
      },
      {
        "source_id": "source_19",
        "citation": "Bridging Gaps in Natural Language Processing for Yoru\u0300ba\u0301_ A  Systematic Review of a Decade of Progress and Prospects.pdf",
        "text_snippet": "a peaceful office setting to ensure quality in the corpus. Ultimately, this corpus will be relevant\nfor various NLP tasks in Yor` ub\u00b4 a, such as TTS, ASR, etc.\nMasakhaNER: MaskhaNER [12] boasts as the first largest high-quality dataset for named\nentity recognition tasks focused on African languages, in which Yor` ub\u00b4 a was greatly empha-\nsised. The dataset scope extends to 10 African languages and across four language families,\nand they include Yor` ub\u00b4 a, Amharic, Hausa, Igbo, Kinyarwanda, Wolof, Nigerian-Pidgin, Luo,\n23"
      },
      {
        "source_id": "source_13",
        "citation": "No Language Left Behind_ Scaling Human-Centered Machine Translation.pdf",
        "text_snippet": "Anastasia Kozyreva, Philipp Lorenz-Spreen, Ralph Hertwig, Stephan Lewandowsky, and\nStefan M Herzog. Public attitudes towards algorithmic personalization and use of personal\ndata online: Evidence from germany, great britain, and the united states. Humanities\nand Social Sciences Communications , 8(1):1\u201311, 2021.\nJulia Kreutzer, Isaac Caswell, Lisa Wang, Ahsan Wahab, Daan van Esch, Nasanbayar\nUlzii-Orshikh, Allahsera Tapo, Nishant Subramani, Artem Sokolov, Claytone Sikasote,\nMonang Setyawan, Supheakmungkol Sarin, Sokhar Samb, Beno\u00eet Sagot, Clara Rivera,\nAnnette Rios, Isabel Papadimitriou, Salomey Osei, Pedro Ortiz Suarez, Iroro Orife, Kelechi\nOgueji, Andre Niyongabo Rubungo, Toan Q. Nguyen, Mathias M\u00fcller, Andr\u00e9 M\u00fcller,\nShamsuddeen Hassan Muhammad, Nanda Muhammad, Ayanda Mnyakeni, Jamshidbek\nMirzakhalov, Tapiwanashe Matangira, Colin Leong, Nze Lawson, Sneha Kudugunta,\nYacine Jernite, Mathias Jenny, Orhan Firat, Bonaventure F. P. Dossou, Sakhile Dlamini,"
      },
      {
        "source_id": "source_01",
        "citation": "Voices Unheard_ NLP Resources and Models for Yoru\u0300ba\u0301 Regional Dialects.pdf",
        "text_snippet": "pages 7226\u20137249, Dublin, Ireland. Association for\nComputational Linguistics.\nIdris Akinade, Jesujoba Alabi, David Adelani, Clement\nOdoje, and Dietrich Klakow. 2023. Varepsilon k\u00fa\nmask: Integrating Yor\u00f9b\u00e1 cultural greetings into\nmachine translation. In Proceedings of the First\nWorkshop on Cross-Cultural Considerations in NLP\n(C3NLP), pages 1\u20137, Dubrovnik, Croatia. Associa-\ntion for Computational Linguistics.\nJesujoba O. Alabi, David Ifeoluwa Adelani, Marius\nMosbach, and Dietrich Klakow. 2022. Adapting pre-\ntrained language models to African languages via\nmultilingual adaptive fine-tuning. In Proceedings of\nthe 29th International Conference on Computational\nLinguistics, pages 4336\u20134349, Gyeongju, Republic\nof Korea. International Committee on Computational\nLinguistics.\nMd Mahfuz Ibn Alam, Sina Ahmadi, and Antonios\nAnastasopoulos. 2024. CODET: A benchmark for\ncontrastive dialectal evaluation of machine transla-\ntion. In Findings of the Association for Computa-"
      },
      {
        "source_id": "source_13",
        "citation": "No Language Left Behind_ Scaling Human-Centered Machine Translation.pdf",
        "text_snippet": "Corpus (BTEC) (Bouamor et al., 2018). This corpus covers dialects from 25 Arabic-\nspeaking cities, in addition to English, French and Modern Standard Arabic (MDA).\nWe map 16 out of these dialects to the 8 Arabic dialects in NLLB-200 (aeb_Arab,\nacm_Arab, acq_Arab, ajp_Arab, apc_Arab, ars_Arab, ary_Arab and arz_Arab ) and\npair each with Modern Standard Arabic ( arb_Arab). We use the MADAR shared task\ntest split ( corpus_6_test_corpus_26_test) for evaluation (Bouamor et al., 2019).\n109"
      },
      {
        "source_id": "source_07",
        "citation": "LLMs Are Few-Shot In-Context Low-Resource Language Learners.pdf",
        "text_snippet": "Abu Dhabi, United Arab Emirates. Association for\nComputational Linguistics.\nPeter Prettenhofer and Benno Stein. 2010. Cross-\nlanguage text classification using structural corre-\nspondence learning. In Proceedings of the 48th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 1118\u20131127, Uppsala, Sweden. As-\nsociation for Computational Linguistics.\nJack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie\nMillican, Jordan Hoffmann, Francis Song, John\nAslanides, Sarah Henderson, Roman Ring, Susan-\nnah Young, Eliza Rutherford, Tom Hennigan, Ja-\ncob Menick, Albin Cassirer, Richard Powell, George\nvan den Driessche, Lisa Anne Hendricks, Mari-\nbeth Rauh, Po-Sen Huang, Amelia Glaese, Jo-\nhannes Welbl, Sumanth Dathathri, Saffron Huang,\nJonathan Uesato, John Mellor, Irina Higgins, Anto-\nnia Creswell, Nat McAleese, Amy Wu, Erich Elsen,\nSiddhant Jayakumar, Elena Buchatskaya, David Bud-\nden, Esme Sutherland, Karen Simonyan, Michela Pa-"
      },
      {
        "source_id": "source_19",
        "citation": "Bridging Gaps in Natural Language Processing for Yoru\u0300ba\u0301_ A  Systematic Review of a Decade of Progress and Prospects.pdf",
        "text_snippet": "[99] Th\u00b4 eophile K Dagba, John OR Aoga, and Codjo C Fanou. Design of a yoruba language\nspeech corpus for the purposes of text-to-speech (tts) synthesis. InIntelligent Information\nand Database Systems: 8th Asian Conference, ACIIDS 2016, Da Nang, Vietnam, March\n14\u201316, 2016, Proceedings, Part I 8 , pages 161\u2013169. Springer, 2016.\n[100] Sunday Adeola Ajagbe. Developing nigeria multilingual languages speech datasets for an-\ntenatal orientation. In Hector Florez and Hern\u00b4 an Astudillo, editors,Applied Informatics,\npages 157\u2013170, Cham, 2024. Springer Nature Switzerland.\n[101] Tolulope Ogunremi, Kola Tubosun, Anuoluwapo Aremu, Iroro Orife, and David Ifeoluwa\nAdelani. `Ir` oy` \u0131nspeech: A multi-purpose yor` ub\u00b4 a speech corpus. InProceedings of the 2024\nJoint International Conference on Computational Linguistics, Language Resources and\nEvaluation (LREC-COLING 2024) , pages 9296\u20139303, 2024.\n[102] Daniel R Van Niekerk and Etienne Barnard. Predicting utterance pitch targets in yor` ub\u00b4 a"
      },
      {
        "source_id": "source_13",
        "citation": "No Language Left Behind_ Scaling Human-Centered Machine Translation.pdf",
        "text_snippet": "1424-8220/21/19/6509.\nMarta Ba\u00f1\u00f3n, Pinzhen Chen, Barry Haddow, Kenneth Heafield, Hieu Hoang, Miquel\nEspl\u00e0-Gomis, Mikel L. Forcada, Amir Kamran, Faheem Kirefu, Philipp Koehn, Sergio\nOrtiz Rojas, Leopoldo Pla Sempere, Gema Ram\u00edrez-S\u00e1nchez, Elsa Sarr\u00edas, Marek Strelec,\nBrian Thompson, William Waites, Dion Wiggins, and Jaume Zaragoza. ParaCrawl:\nWeb-scale acquisition of parallel corpora. In Proceedings of the 58th Annual Meeting\nof the Association for Computational Linguistics , pages 4555\u20134567, Online, July 2020.\n138"
      },
      {
        "source_id": "source_12",
        "citation": "A Survey on Large Language Models with Multilingualism_ Recent Advances and New Frontiers.pdf",
        "text_snippet": "erinde, Clemencia Siro, Tobius Saul Bateesa, Temilola Oloyede, Yvonne Wambui, Vic-\ntor Akinode, Deborah Nabagereka, Maurice Katusiime, Ayodele Awokoya, Mouhamadane\nMBOUP, Dibora Gebreyohannes, Henok Tilaye, Kelechi Nwaike, Degaga Wolde, Abdoulaye\nFaye, Blessing Sibanda, Orevaoghene Ahia, Bonaventure F. P. Dossou, Kelechi Ogueji,\nThierno Ibrahima DIOP, Abdoulaye Diallo, Adewale Akinfaderin, Tendai Marengereke, and\nSalomey Osei. Masakhaner: Named entity recognition for african languages, 2021.\n[225] David Ifeoluwa Adelani, Marek Masiak, Israel Abebe Azime, Jesujoba Alabi, Atnafu Lam-\nbebo Tonja, Christine Mwase, Odunayo Ogundepo, Bonaventure F. P. Dossou, Akintunde\n44"
      },
      {
        "source_id": "source_15",
        "citation": "Multilingual Language Models are not Multicultural_ A Case Study in Emotion.pdf",
        "text_snippet": "New York, NY , USA. Association for Computing\nMachinery.\nRene M Stulz and Rohan Williamson. 2003. Culture,\nopenness, and finance. Journal of financial Eco-\nnomics, 70(3):313\u2013349.\nJonas Thiergart, Stefan Huber, and Thomas \u00dcbellacker.\n2021. Understanding emails and drafting responses -\nan approach using GPT-3. CoRR, abs/2102.03062.\nJeanne L Tsai. 2017. Ideal affect in daily life: Impli-\ncations for affective experience, health, and social\nbehavior. Current Opinion in Psychology, 17:118\u2013\n128."
      }
    ],
    "time_taken": 5.74
  }
]