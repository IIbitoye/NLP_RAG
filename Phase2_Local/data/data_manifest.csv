source_id,filename,title,author,year,citation,Venue,Link /DOI,relevance_note
source_01,A Survey on Large Language Models with Multilingualism_ Recent Advances and New Frontiers.pdf,A Survey on Large Language Models with Multilingualism: Recent Advances and New Frontiers,Huang et al.,2025,"(Huang et al., 2025)",AphaXiv,arXiv:2405.10936v2," Broad overview of the current state of multilingual LLMs, covering training, inference, and security."
source_02,AfroBench_ How Good are Large Language Models on African Languages?.pdf,AfroBench: How Good are Large Language Models on African Languages?,Ojo et al.,2025,"(Ojo et al., 2025)",AphaXiv,https://arxiv.org/abs/2311.07978v5,"Establishes a multi-task benchmark for 64 African languages, quantifying the performance gap between English and low-resource languages."
source_03,"A Survey on Multilingual Large Language Models_ Corpora, Alignment, and Bias.pdf","A Survey on Multilingual Large Language Models: Corpora, Alignment, and Bias",Xu et al.,2024,"(Xu et al., 2024)",AphaXiv,https://doi.org/10.1007/s11704-024-40579-4,"Focuses specifically on the challenges of corpora, alignment, and bias in multilingual large language models."
source_04,Bridging Gaps in Natural Language Processing for Yorùbá_ A Systematic Review of a Decade of Progress and Prospects.pdf,Bridging Gaps in Natural Language Processing for Yorùbá: A Systematic Review,Jimoh et al.,2025,"(Jimoh et al., 2025)",AphaXiv,arXiv:2502.17364v1,A systematic review of a decade of NLP progress specifically for the Yorùbá language.
source_05,Cross-lingual transfer of multilingual models on low resource African Languages.pdf,Cross-lingual transfer of multilingual models on low resource African Languages,Thangaraj et al.,2024,"(Thangaraj et al., 2024)",AphaXiv,https://arxiv.org/abs/2409.10965,Benchmarks how well models like mBERT and AfriBERT transfer knowledge from high to low-resource Bantu languages.
source_06,AfriMTE and AfriCOMET_ Enhancing COMET to Embrace Under-resourced African Languages.pdf,AfriMTE and AfriCOMET: Enhancing COMET to Embrace Under-resourced African Languages,Wang et al.,2024,"(Wang et al., 2024)",AphaXiv,arXiv:2311.09828v3,"Introduces specific evaluation metrics (AfriCOMET) tailored for African languages, improving upon standard BLEU/COMET scores."
source_07,CROSSLINGUAL TRANSFER OF DEBIASING TECHNIQUES.pdf,Investigating Bias in Multilingual Language Models: Cross-Lingual Transfer of Debiasing Techniques,Reusens et al.,2023,"(Reusens et al., 2023)",AphaXiv,"https://doi.org/10.48550/arXiv.2310.10310
",Investigates if debiasing techniques developed for English can be successfully transferred to other languages in mBERT.
source_08,CultureVLM_ Characterizing and Improving Cultural Understanding of Vision-Language Models for over 100 Countries.pdf,CultureVLM: Characterizing and Improving Cultural Understanding of Vision-Language Models,Liu et al.,2025,"(Liu et al., 2025)",AphaXiv,arXiv:2501.01282v1,"Investigates how vision-language models fail to recognize non-Western cultural artifacts and proposes ""CultureVerse"" to fix it."
source_09,Generative AI and Large Language Models in Language Preservation_ Opportunities and Challenges.pdf,Generative AI and Large Language Models in Language Preservation: Opportunities and Challenges,Koc,2025,"(Koc, 2025)",AphaXiv,https://arxiv.org/abs/2501.11496,A framework for using GenAI to revitalize endangered languages while maintaining data sovereignty.
source_10,Global MMLU_ Understanding and Addressing Cultural and Linguistic Biases in Multilingual Evaluation.pdf,Global MMLU: Understanding and Addressing Cultural and Linguistic Biases in Multilingual Evaluation,Singha et al.,2025,"(Singha et al., 2025)",AphaXiv,https://arxiv.org/abs/2412.03304,Primary source for low-resource NLP research.
source_11,IrokoBench_ A New Benchmark for African Languages in the Age of Large Language Models.pdf,IrokoBench: A New Benchmark for African Languages in the Age of Large Language Models,Adelani et al.,2025,"(Adelani et al., 2025)",AphaXiv,https://arxiv.org/abs/2406.03368,"Introduces a comprehensive benchmark for African languages (AfriXNLI, AfriMGSM) to test LLM reasoning capabilities."
source_12,Jawaher_ A Multidialectal Dataset of Arabic Proverbs for LLM Benchmarking.pdf,Jawaher: A Multidialectal Dataset of Arabic Proverbs for LLM Benchmarking,Magdy et al.,2025,"(Magdy et al., 2025)",AphaXiv,https://arxiv.org/abs/2503.00231,"A benchmark for Arabic proverbs across dialects, essential for testing an LLM's grasp of figurative and cultural language."
source_13,Language model pretraining for low resource languages.pdf,Language Model Pretraining and Transfer Learning for Very Low Resource Languages,Khatri et al.,2021,"(Khatri et al., 2021)",Conference Precedings,https://aclanthology.org/2021.wmt-1.106/,Discusses strategies like MASS and iterative back-translation for scenarios with very little parallel data.
source_14,LLMs Are Few-Shot In-Context Low-Resource Language Learners.pdf,LLMs Are Few-Shot In-Context Low-Resource Language Learners,Cahyawijaya et al.,2024,"(Cahyawijaya et al., 2024)",AphaXiv,https://arxiv.org/abs/2403.16512,Demonstrates that In-Context Learning (ICL) can bridge the gap for low-resource languages without extensive fine-tuning.
source_15,LLMs in the Loop_ Leveraging Large Language Model Annotations for Active Learning in Low-Resource Languages.pdf,LLMs in the Loop: Leveraging Large Language Model Annotations for Active Learning,Kholodna et al.,2024,"(Kholodna et al., 2024)",AphaXiv,https://arxiv.org/abs/2404.02261,Proposes using LLMs (like GPT-4) to annotate data for low-resource languages to save costs (Active Learning).
source_16,Localising SA official languages.pdf,Localising the Mozilla Common Voice platform for South Africa's official languages,De Wet et al.,2023,"(De Wet et al., 2023)",Journal,https://doi.org/10.55492/dhasa.v4i01.4437,"Details the ""Mozilla Common Voice"" initiative in South Africa, recommending community-driven manual data collection."
source_17,Masakhane -- Machine Translation For Africa.pdf,Masakhane: Machine Translation For Africa,Orife et al.,2020,"(Orife et al., 2020)",AphaXiv,https://arxiv.org/abs/2003.11529,"Describes the Masakhane participatory research approach, a key reference for community-driven NLP in Africa."
source_18,Multilingual Language Models are not Multicultural_ A Case Study in Emotion.pdf,Multilingual Language Models are not Multicultural: A Case Study in Emotion,Havaldar et al.,2023,"(Havaldar et al., 2023)",AphaXiv,https://arxiv.org/abs/2307.01370,A case study demonstrating that multilingual models still impose Western emotional norms on non-Western languages.
source_19,NaijaSenti_ A Nigerian Twitter Sentiment Corpus for Multilingual Sentiment Analysis.pdf,NaijaSenti: A Nigerian Twitter Sentiment Corpus for Multilingual Sentiment Analysis,Muhammad et al.,2022,"(Muhammad et al., 2022)",AphaXiv,https://arxiv.org/abs/2201.08277,"Provides the first large-scale, human-annotated sentiment analysis corpus for Hausa, Igbo, Nigerian-Pidgin, and Yorùbá."
source_20,Neural Machine Translation for Low-Resource Languages.pdf,Neural Machine Translation for Low-Resource Languages: A Survey,Ranathunga et al.,2021,"(Ranathunga et al., 2021)",AphaXiv,https://arxiv.org/abs/2106.15115,A comprehensive overview of NMT techniques specifically optimized for low-resource settings.
source_21,NileChat: Towards Linguistically Diverse and Culturally Aware LLMs for Local Communities.pdf,NileChat: Towards Linguistically Diverse and Culturally Aware LLMs for Local Communities,El Mekki et al.,2025,"(El Mekki et al., 2025)",AphaXiv,https://arxiv.org/abs/2505.18383,Focuses on aligning LLMs with local values and dialects (Egyptian/Moroccan Arabic) rather than just standard translation.
source_22,No Language Left Behind: Scaling Human-Centered Machine Translation.pdf,No Language Left Behind: Scaling Human-Centered Machine Translation,NLLB Team et al.,2022,"(NLLB Team et al., 2022)",AphaXiv,https://arxiv.org/abs/2207.04672,The seminal NLLB paper describing techniques for scaling machine translation to 200+ languages using Mixture-of-Experts.
source_23,On The Origin of Cultural Biases in Language Models: From Pre-training Data to Linguistic Phenomena.pdf,On The Origin of Cultural Biases in Language Models: From Pre-training Data to Linguistic Phenomena,Naous & Xu,2025,"(Naous & Xu, 2025)",AphaXiv,https://arxiv.org/abs/2501.04662,"Traces cultural bias back to pre-training data and tokenization issues, specifically in Arabic script languages."
source_24,Preserving Cultural Identity with Context-Aware Translation Through Multi-Agent AI Systems.pdf,Preserving Cultural Identity with Context-Aware Translation Through Multi-Agent AI Systems,Anik et al.,2025,"(Anik et al., 2025)",AphaXiv,https://arxiv.org/abs/2503.04827,Proposes a multi-agent AI system to preserve cultural nuance and idiomatic meaning during translation.
source_25,Prompting Towards Alleviating Code-Switched Data Scarcity in Under-Resourced Languages.pdf,Prompting Towards Alleviating Code-Switched Data Scarcity in Under-Resourced Languages,Terblanche et al.,2024,"(Terblanche et al., 2024)",AphaXiv,https://arxiv.org/abs/2404.17216,Experiments with using GPT to generate synthetic code-switched data (Afrikaans/Yoruba) to augment training sets.
source_26,Targeted Multilingual Adaptation for Low-resource Language Families.pdf,Targeted Multilingual Adaptation for Low-resource Language Families,Downey et al.,2024,"(Downey et al., 2024)",AphaXiv,https://arxiv.org/abs/2405.12413,Shows how to adapt large models to specific language families (using Uralic as a test case) for better performance.
source_27,"The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks.pdf","The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks",Wu et al.,2025,"(Wu et al., 2025)",AphaXiv,https://arxiv.org/abs/2504.15521,"Analyzes over 2,000 benchmarks to reveal the systemic overrepresentation of English and the lack of native-language evaluation."
source_28,Understanding the Capabilities and Limitations of Large Language Models for Cultural Commonsense.pdf,Understanding the Capabilities and Limitations of Large Language Models for Cultural Commonsense,Shen et al.,2024,"(Shen et al., 2024)",AphaXiv,https://arxiv.org/abs/2405.04655,"Tests whether LLMs possess ""cultural commonsense"" or if they hallucinate based on Western default assumptions."
source_29,Unsupervised Cross-lingual Representation Learning at Scale (XLM-R).pdf,Unsupervised Cross-lingual Representation Learning at Scale (XLM-R),Conneau et al.,2020,"(Conneau et al., 2020)",AphaXiv,https://arxiv.org/abs/1911.02116,The original XLM-R paper; essential for understanding how cross-lingual transfer learning works (mBERT vs XLM-R).
source_30,Voices Unheard: NLP Resources and Models for Yorùbá Regional Dialects.pdf,Voices Unheard: NLP Resources and Models for Yorùbá Regional Dialects,Ahia et al.,2024,"(Ahia et al., 2024)",AphaXiv,https://arxiv.org/abs/2406.19564,"Introduces ""YorùLect,"" a corpus for Yorùbá regional dialects, addressing the gap between standard and spoken dialects."
source_31,WAXAL: A Large-Scale Multilingual African Language Speech Corpus.pdf,WAXAL: A Large-Scale Multilingual African Language Speech Corpus,Diack et al.,2026,"(El Mekki et al., 2025)",AphaXiv,https://www.arxiv.org/abs/2602.02734,"A massive speech corpus for African languages, crucial for multimodal or ASR-related research queries."
source_32,Where Are We? Evaluating LLM Performance on African Languages.pdf,Where Are We? Evaluating LLM Performance on African Languages,Adebara et al.,2025,"(Adebara et al., 2024)",AphaXiv,https://arxiv.org/abs/2502.19582,Empirically evaluates LLMs on African languages while analyzing how colonial language policies impact data availability.
source_33,Cheetah: Natural Language Generation for 517 African Languages.pdf,Cheetah: Natural Language Generation for 517 African Languages,Adebara et al.	,2024,"(Adebara et al., 2024)",Conference Precedings,https://aclanthology.org/2024.acl-long.691/,Proposes a massively multilingual model covering 517 African languages/varieties to solve NLG data scarcity.
source_34,Exploring NLP Benchmarks in an Extremely Low-Resource Setting.pdf	,Exploring NLP Benchmarks in an Extremely Low-Resource Setting,Nuha & Jatowt,2025,"(Nuha & Jatowt, 2025)",AphaXiv,https://arxiv.org/abs/2509.03962v1,"Uses the Ladin language to demonstrate how to build datasets (MCQA, Sentiment) in extremely low-resource settings."